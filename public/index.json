[{"content":"\rClimatic Data in Hydrological Analysis\rClimatic data such as temperature and precipitation are essential for any hydrological analysis. Usually, several years of data are required to do any type of hydrological analysis. The climatic data is obtained from either satellite-based observation or in situ observation.\nFor hydrological analysis historical precipitation and temperature data is essential. Usually these data can be obtained from Indian Meterolgocal Department (भारतीय मौसम विज्ञान विभाग:IMD) free of cost.\nIMD pune provide the gridded precipitation and Temperature data since 1901. Most of time I find many students are struggling to process the data that this either in NetCDF (ext: “.nc”), or Binary data (ext: “.grd”). The processign of both the data is a little different.\nData source, specifications, and metadata.\rTo process any data the most important step is metadata. Metadata for temperature and precipitation is shown:\nRainfall Data:\rThe rainfall data product is the resolution of daily gridded rainfall data (0.25 x 0.25 degrees). The unit of rainfall is in millimeters (mm). Data available for 122 years, 1901 to 2023. Data is arranged in 135x129 grid points. The first data in the record is at 6.5N \u0026amp; 66.5E, the second is at 6.5N \u0026amp; 66.75E, and so on. The last data record corresponds to 38.5N \u0026amp; 100.0E. The yearly data file consists of 365/366 records corresponding to nonleap/ leap years. You can download it from https://imdpune.gov.in/cmpg/Griddata/Rainfall_25_NetCDF.html\nTemperature data (min/max)\rIMD High resolution \\(1\\times1\\) degree gridded daily temperature data (1951-2018). This data is arranged in 31x31 grid points. Lat 7.5N, 8.5N … 36.5, 37.5 (31 Values). Long 67.5E, 68.5E … 96.5, 97.5 (31 Values). For leap years, data for 366 days are included. The unit of temperature is in Celsius.\rGridded data for the years 2008 and onwards are based on a relatively small number of stations (around 180) for which data were received operationally on real time basis.\nReference paper for more detail\rPai et al. (2014). Pai D.S., Latha Sridhar, Rajeevan M., Sreejith O.P., Satbhai N.S. and Mukhopadhyay B., 2014:\rDevelopment of a new high spatial resolution (0.25° X 0.25°)Long period (1901-2010) daily gridded rainfall data set over.\rIndia and its comparison with existing data sets over the region; MAUSAM, 65, 1(January 2014), pp1-18.\rProcessing ‘NetCDF’\rNetCDF (Network Common Data Form) is a widely used data format designed for storing and sharing scientific data in a structured, self-describing, and platform-independent manner. Developed by Unidata, NetCDF is primarily used in geosciences, such as atmospheric science, hydrology, oceanography, and climate modeling, but can be applied to various other fields requiring efficient storage of multidimensional data.\nStructure of a NetCDF File:\rAs an example, I am showing the metadata of the IMD precipitation file.\nFile ./RF25_ind2023_rfp25.nc (NC_FORMAT_CLASSIC):\r1 variables (excluding dimension variables):\rdouble RAINFALL[LONGITUDE,LATITUDE,TIME]\rmissing_value: -999\r_FillValue: -999\rlong_name: Rainfall\runits: mm\rhistory: From ind2023_rfp25.grd\r2 dimensions:\rLONGITUDE Size:135\runits: degrees_east\rpoint_spacing: even\raxis: X\rmodulo: 360\rstandard_name: longitude\rLATITUDE Size:129\runits: degrees_north\rpoint_spacing: even\raxis: Y\rstandard_name: latitude\rTIME Size:365 *** is unlimited ***\runits: days since 1900-12-31\raxis: T\rcalendar: GREGORIAN\rtime_origin: 31-DEC-1900\rstandard_name: time\r3 global attributes:\rhistory: FERRET V7.5 (optimized) 6-Feb-24\rConventions: CF-1.6\rBased on the aforementioned NetCDF metadata, it consists of three main components:\nDimensions: These define the shape of the data, such as time, latitude, longitude, or vertical levels. A dimension can be used to describe how many values a variable contains in a specific direction or category.\rExample: If you have daily temperature data at different locations, the dimensions could be time (number of days), latitude, and longitude.\rtime: Unlimited, indicating that more time steps can be added without rewriting the file.\rlatitude: 180 values, representing geographic locations from the southern to the northern hemisphere.\rlongitude: 360 values, representing locations from west to east around the globe.\rVariables: These store the actual data. Variables can be scalar or multi-dimensional arrays (e.g., temperature, rainfall, or elevation data). Each variable is defined along one or more dimensions and can have associated metadata (attributes).\rExample: A variable could represent temperature (temp), with dimensions time, latitude, and longitude.\rrainfall: A 3D variable that depends on time, latitude, and longitude. It stores daily rainfall values and includes metadata (units = mm, long_name = Daily Rainfall) to describe the data.\rlatitude and longitude: 1D variables with values representing geographic coordinates in degrees.\rtime: A 1D variable containing time values since a reference date (2000-01-01). The calendar attribute specifies the type of calendar system.\rAttributes: These provide additional information about the file or variables, such as units, missing data values, or descriptive text. They are used to describe global properties (applied to the whole dataset) or specific variables.\rExample: An attribute might define the units of temperature as “degrees Celsius” or mark missing values with a specific value like -9999.\rMetadata about the dataset, such as the title, institution, and data source, which provide context and provenance information.\rProcessing NetCDF file\rNetCDF file has 3 dimension lat, lon, and time (day)\nLoading ncdf4 library to process the NetCDF file. Geospatial analysis is performed by terra package.\nif (!require(ncdf4)) { install.packages(\u0026#39;ncdf4\u0026#39;); library(ncdf4)}\rif (!require(terra)) { install.packages(\u0026#39;terra\u0026#39;); library(terra)}\rif (!require(tidyverse)) { install.packages(\u0026#39;tidyverse\u0026#39;); library(tidyverse)}\rRead shapefile with multiple polygons in it.\nshp \u0026lt;- vect(\u0026quot;./Shapefile/Krishna_subbasins.shp\u0026quot;)\rshp \u0026lt;- project(x = shp, y = \u0026quot;+proj=longlat +datum=WGS84 +no_defs\u0026quot;)\rplot(shp, bg = \u0026quot;gray\u0026quot;)\rsbar(d = 1000, type = \u0026quot;bar\u0026quot;, divs = 4, below = \u0026quot;km\u0026quot;) # Scale\rnorth() # for north arrow.\rrotate_clockwise \u0026lt;- function(x) {t(apply(x, 2, rev))}\rrotate_counter_clockwise \u0026lt;- function(x) {apply(t(x),2, rev)}\rSelect the year line\ryr \u0026lt;- 2015:2023\rrainfall_mat \u0026lt;- timestamp \u0026lt;- c()\rfor (yr_i in yr) {\rnc_file \u0026lt;- nc_open(paste0(\u0026quot;./RF25_ind\u0026quot;,yr_i,\u0026quot;_rfp25.nc\u0026quot;))\r# print(nc_file) # get info about the file\rnames(nc_file$var) # Variable i.e. Rainfall\rnames(nc_file$dim) # Dimenstions (Lat, Lon, time)\rlatitude \u0026lt;- nc_file$dim[[1]]$vals\rlongitude \u0026lt;- nc_file$dim[[2]]$vals\rtime \u0026lt;- nc_file$dim[[3]]$vals\rtime_as_date \u0026lt;- as.Date(time, origin = \u0026quot;1900-12-31\u0026quot;)\rrainfall \u0026lt;- ncvar_get(nc_file, varid = \u0026quot;RAINFALL\u0026quot;)\rr_mat \u0026lt;- matrix(NA, nrow = length(time), ncol = length(shp$OBJECTID))\rfor(d_i in seq_along(time)){\rprecip_day \u0026lt;- rainfall[, , d_i] %\u0026gt;% rotate_counter_clockwise() %\u0026gt;% rast()\rext(precip_day) \u0026lt;- c(min(latitude), max(latitude), min(longitude), max(longitude))\rcrs(precip_day) \u0026lt;- \u0026quot;+proj=longlat +datum=WGS84 +no_defs\u0026quot;\rr_day \u0026lt;- terra::extract(x = precip_day, y = shp, fun = mean)\rr_mat[d_i, ] \u0026lt;- r_day$lyr.1\rprint(paste(d_i, yr_i))\r}\rrainfall_mat \u0026lt;- rbind(rainfall_mat, r_mat)\rtimestamp \u0026lt;- append(timestamp, time_as_date)\r}\rProcess the output\rrainfall_df \u0026lt;- as.data.frame(rainfall_mat)\rrainfall_df \u0026lt;- cbind(timestamp, rainfall_df)\rwrite_csv(rainfall_df, file = \u0026quot;Rainfall.csv\u0026quot;)\rProcessing Binary Data ‘GRD’ files\rThe grd file are provided by IMD and reading it a little bit different. The following code demostrate the working with .grd.\nReading files and metadata\r# Working with R blog.\rif (!require(terra)) { install.packages(\u0026#39;terra\u0026#39;); library(terra)}\rf \u0026lt;- \u0026quot;Maxtemp_MaxT_2022.GRD\u0026quot;\r# -------------------------------------------------\r# Latitude: 7.5:37.5\r# Longitude:67.5:97.5\r# Resolution : 1 degree\r# Temperature units : Celcius\r# Missing data : 99.9\r# --------------------------------------------------\r# https://imdpune.gov.in/cmpg/Griddata/Max_1_Bin.html#\r# Get the metadata of the file the previous link\rlat \u0026lt;- 31\rlon \u0026lt;- 31\rnspatial \u0026lt;- lat * lon\rSetting up for loop through the daily data\rleap_year \u0026lt;- function(year) {return(ifelse((year %%4 == 0 \u0026amp; year %%100 != 0) | year %%400 == 0, 366, 365)) }\rdays = leap_year(as.numeric(substr(f, 14, 17)))\r# read all the data\rm_data \u0026lt;- readBin(f, what = \u0026quot;numeric\u0026quot;, size = 4, n = lat * lon * days)\rtemp_list \u0026lt;- list()\rfor(day in 1:days){\rnday = m_data[1+(day*nspatial):(nspatial*(day+1) - 1)]\rm = matrix(nday, lat, lon, byrow=TRUE)\rm[m \u0026gt; 99]=NA\rm = m[nrow(m):1,] # Invert the matrix\rr = rast(m)\rext(r) = c(67.5, 97.5, 7.5, 37.5)\rcrs(r) = \u0026quot;+init=epsg:4326\u0026quot;\rtemp_list[[day]] \u0026lt;- r\rrm(r, m)\rprint(day)\r}\rtemp_raster \u0026lt;- rast(temp_list)\rplot(temp_raster[[122]])\rwriteRaster(x = temp_raster, filename = paste0(\u0026quot;IMD_Temp_\u0026quot;, substr(f, 14, 17), \u0026quot;.tif\u0026quot;))\rTest \u0026lt;- rast(\u0026quot;./IMD_Temp_2022.tif\u0026quot;)\rExtracting the daily raster stack data with extract\rWe use terra r-package to do geoprocessing.\nterra::extract(x, y, fun=NULL, method=\u0026quot;simple\u0026quot;, fun = FUN)\rwhere:\nx SpatRaster or SpatVector of polygons\ny SpatVector (points, lines, or polygons).\nYou can use any function to aggrigate the value, such as mean, count, sum, or custom.\nConcluding Remarks\rProcessing climatic data is essential for hydrological analysis. Many R packages and software are available for this. We find terra has all the functionality and is fast to process the .GRD and ‘.nc’ files. Some important tips:\r- Always plot to check the CRS of Raster and Vector data.\r- Plot a raster file to check the extent.\r- Check the null value carefully from the metadata (-99.9, -9999, etc.)\n","permalink":"https://ankitdeshmukh.com/post/2024-10-03-working-with-netcdf/","summary":"Climatic Data in Hydrological Analysis\rClimatic data such as temperature and precipitation are essential for any hydrological analysis. Usually, several years of data are required to do any type of hydrological analysis. The climatic data is obtained from either satellite-based observation or in situ observation.\nFor hydrological analysis historical precipitation and temperature data is essential. Usually these data can be obtained from Indian Meterolgocal Department (भारतीय मौसम विज्ञान विभाग:IMD) free of cost.","title":"Working with NetCDF and Binary climatic data in R"},{"content":"\rWhat is PostGIS and why PostGIS is benificial over tredtional analysis approaches.\rPostGIS is a spatial database extension for PostgreSQL that allows users to store and manipulate geospatial data. PostGIS has several advantages over the traditional approach of geospatial analysis, such as:\nPostGIS supports a wide range of spatial data types, functions, and operators, enabling complex spatial queries and operations.\rPostGIS integrates well with other GIS tools and frameworks, such as QGIS, GeoServer, and Leaflet, allowing users to visualize and analyze their data in different ways.\rPostGIS leverages the power and scalability of PostgreSQL, which is a robust, open-source, and widely used relational database management system.\rPostGIS enables spatial data analysis on large datasets, as it can handle millions of features and perform spatial joins and aggregations efficiently.\rPostGIS facilitates data sharing and collaboration, as it allows multiple users to access and modify the same spatial data concurrently.\rPostGIS for Professionals\rThe PostGIS roadmap is crucial for professionals in the field of GIS, Water resources, and Hydrology. To grow in the field of GIS expert you must to the certain taks on regular basis:\nMaster Core PostGIS Functions:\rEnsure a solid understanding of fundamental PostGIS functions for spatial data handling.\rProficiency in spatial queries, geometric operations, and indexing techniques is essential.\rDeepen Geospatial Database Skills:\rExpand your expertise in geospatial database management, including performance tuning and optimization strategies.\rFamiliarize yourself with advanced database concepts relevant to spatial data storage.\rIntegration with GIS Software:\rExplore integration possibilities with popular GIS software like QGIS and ArcGIS to enhance your interoperability skills.\rStay informed about evolving standards and best practices in the GIS industry.\rAdvanced Spatial Analytics:\rFocus on advanced spatial analytics using PostGIS, such as spatial regression analysis, network analysis, and 3D spatial operations.\rIncorporate machine learning algorithms into spatial analysis to address contemporary challenges in water resources and hydrology.\rOpen Source Contributions:\rConsider contributing to the PostGIS project or related open-source GIS projects. This enhances your visibility in the community and deepens your understanding of the system.\rNetworking and Professional Development:\rAttend conferences, workshops, and webinars focused on geospatial technologies and PostGIS.\rEngage with professionals in your field, both online and offline, to build a strong professional network.\rCertifications and Academic Collaborations:\rPursue relevant certifications in GIS and spatial databases to validate your expertise.\rCollaborate with academic institutions on research projects to stay at the forefront of advancements in hydrology, remote sensing, and GIS.\rBy systematically progressing through these steps, you’ll be a highly skilled professional in PostGIS, well-equipped to tackle complex challenges in water resources and hydrology, and ultimately increase your attractiveness for high-paying job opportunities.\nNow we will understand how to setup PostGIS with QGIS in a Windows machine.\nQuick installation\rInstall the PostgreSQL from the link 🔗\nInstall with default setting. Default port is 5432, change to something else if you have another version of PostgreSQL installed.\rYouTube link https://www.youtube.com/watch?v=IYHx0ovvxPs\rConnect a database with QGIS\rBasic of Structured Query Language (SQL)\rSQL stands for Structured Query Language and is a domain-specific language for managing data in relational databases. SQL was originally developed by IBM in the 1970s and later standardized by ANSI and ISO. SQL allows users to query, manipulate, and control data using keywords, clauses, expressions, and statements that resemble natural language.\nLearning the basics of SQL\rUse tutorial and data from: https://www.sqltutorial.org/sql-sample-database/\nCreating a SQL Sample Database\nThe following database diagram illustrates the HR sample database\rThis database has 7 tables, row numbers are shown in the table below\n| Table | Rows |\r----------------------\r| employees | 40 |\r| dependents | 30 |\r| departments | 11 |\r| jobs | 11 |\r| locations | 07 |\r| countries | 25 |\r| regions | 04 |\rThe following script creates the HR sample Database Structure in PostgreSQL\rCreating a Database:\rCREATE DATABASE test_db\rWITH\rOWNER = postgres\rENCODING = \u0026#39;UTF8\u0026#39;\rLOCALE_PROVIDER = \u0026#39;libc\u0026#39;\rCONNECTION LIMIT = -1\rIS_TEMPLATE = False;\rList all the database and select the one\rpostgres=# \\l\rpostgres=# \\c DATABASE_NAME\rRun the following to add PostGIS extension to Postgres\rCREATE EXTENSION postgis;\rCREATE EXTENSION postgis_raster;\rImport shapefile in PostGIS\rUse DB manager form QGIS\nImport raster in a database\rraster2pgsql -s [SRID] -I -M [raster data source] -F [schema.table_name] | psql -U [username] -d [database name] -p [port] -h [host]\rExample:\nraster2pgsql -s 4326 -I -M Your_file.tif -F | psql -U postgres -d post_gis_v1 -p 5432 -h localhost\rSELECT ST_X(ST_Centroid(geom)) AS long, ST_Y(ST_Centroid(geom)) AS lat FROM \u0026quot;ST_India-Dist\u0026quot;;\rSELECT ST_Centroid(geom) AS geom, gid, st_nm FROM \u0026quot;ST_India-Dist\u0026quot;;\r","permalink":"https://ankitdeshmukh.com/post/2024-01-08-postgis-with-qgis/","summary":"What is PostGIS and why PostGIS is benificial over tredtional analysis approaches.\rPostGIS is a spatial database extension for PostgreSQL that allows users to store and manipulate geospatial data. PostGIS has several advantages over the traditional approach of geospatial analysis, such as:\nPostGIS supports a wide range of spatial data types, functions, and operators, enabling complex spatial queries and operations.\rPostGIS integrates well with other GIS tools and frameworks, such as QGIS, GeoServer, and Leaflet, allowing users to visualize and analyze their data in different ways.","title":"PostGIS with QGIS"},{"content":"\rGetting started with gdal\rGDAL (Geospatial Data Abstraction Library) is a free and open source translator library for raster and vector geospatial data formats. It also comes with a variety of useful command line utilities for data translation and processing.\nGDAL is used by many GIS software packages, such as QGIS, ArcGIS, and GRASS GIS and R. It is also used in many scientific applications that require geospatial data processing, such as remote sensing, hydrology, and geology.\nIn R you can use GDAL with rgdal package. Several Packages are retiring for R See (https://r-spatial.org/r/2022/04/12/evolution.html)\ninstall.packages(\u0026quot;rgdal\u0026quot;, dependencies = TRUE)\rGDAL in QGIS\rThe GDAL library consists of a set of command line programs, each with a large list of options. Users comfortable with running commands from a terminal may prefer the command line, with access to the full set of options. The GDAL Tools plugin offers an easy interface to the tools, exposing only the most popular options.\nA summary of GDAL library capabilities\nExtensive Format Support: GDAL supports an extensive list of geospatial data formats, including GeoTIFF, NetCDF, shapefiles, and more. This diversity simplifies data access and manipulation.\nData Extraction: GDAL enables the extraction of specific data layers or subsets from geospatial datasets, which is particularly useful for researchers focused on specific regions or parameters within a dataset.\nReprojection and Transformation: Researchers often work with data in different projections. GDAL can reproject and transform data, ensuring it aligns spatially, which is vital in geospatial analysis.\nCommand-Line and GUI: GDAL offers both command-line and graphical user interfaces (such as QGIS), making it suitable for researchers with various preferences for interacting with geospatial data.\nPython and R Integration: GDAL provides bindings for popular programming languages like Python and R. This allows geospatial analysts to incorporate GDAL’s capabilities into their data analysis and modeling workflows, aligning with your preference for using R.\nGeospatial Image Processing: Beyond data conversion, GDAL provides capabilities for basic image processing, such as resampling, cropping, and filtering, which are essential for preparing data for further analysis.\nGDAL algorithm provider has major categories of operation in vector and raster analysis:\r1. Raster analysis\r2. Raster conversion\r3. Raster extraction\r4. Raster miscellaneous (merge, raster calculator, etc.)\r5. Raster projections\r6. Vector conversion\r7. Vector geoprocessing\r8. Vector miscellaneous (build virtual vector, SQL, etc.)\nWe will try to understand how we can use GDAL in R for most commonly used operation in geospatial analysis\nData Format Conversion: GDAL can convert data between various geospatial file formats. You can use the gdal_translate command to convert data from one format to another. For example, converting from GeoTIFF to Shapefile or vice versa.\nData Subset and Clipping: You can extract a specific region of interest from a larger dataset using the gdalwarp command. This is particularly useful for working with large raster datasets.\nResampling: GDAL allows you to change the resolution of raster data using the gdalwarp command. This can be helpful when merging or aligning datasets with different resolutions.\nMosaicking: You can merge multiple raster datasets into a single file using the gdal_merge.py utility. This is useful when you have data spread across multiple tiles or images.\nReprojection: Changing the coordinate system of a dataset is a common operation in geospatial analysis. GDAL’s gdalwarp allows you to reproject data to a different coordinate system.\nCreation of Virtual Raster (VRT): VRT files allow you to work with large datasets without actually copying or resampling the data. The gdalbuildvrt command can create VRT files, which are essentially virtual catalogs of your data.\nMetadata Extraction: You can extract metadata information from a geospatial dataset using the gdalinfo command. This includes information about the dataset’s spatial reference, geotransform, and more.\nRaster Calculator: The gdal_calc.py utility allows you to perform mathematical operations on raster datasets, enabling you to create derived products like difference maps or vegetation indices.\nData Warping and Reprojection: The gdalwarp utility can also be used to reproject and warp datasets, which is essential for ensuring that data aligns properly when conducting geospatial analyses.\nHistogram Analysis: GDAL provides tools for generating histograms of raster data, which can be valuable for understanding the distribution of values within a dataset. The gdal_hist utility can help with this.\nThese operations are fundamental to geospatial analysis and can be used in your research in the field of hydrology, remote sensing, and GIS. GDAL’s versatility and extensive command-line tools make it a valuable resource for working with geospatial data, especially when integrated with modern machine learning algorithms for drought analysis and geospatial data analysis using R.\n","permalink":"https://ankitdeshmukh.com/post/2023-10-24-gdal-an-introduction-rmd/","summary":"Getting started with gdal\rGDAL (Geospatial Data Abstraction Library) is a free and open source translator library for raster and vector geospatial data formats. It also comes with a variety of useful command line utilities for data translation and processing.\nGDAL is used by many GIS software packages, such as QGIS, ArcGIS, and GRASS GIS and R. It is also used in many scientific applications that require geospatial data processing, such as remote sensing, hydrology, and geology.","title":"Introduction of GDAL with R programming"},{"content":"\rHTML slides with Xaringan.\rXaringan is an R package for creating slideshows with remark.js through R Markdown.\n…from https://github.com/yihui/xaringan\nThe package name xaringan comes from Sharingan, a dōjutsu in Naruto with two abilities: the “Eye of Insight” and the “Eye of Hypnotism”. A presentation ninja should have these basic abilities, and I think remark.js may help you acquire these abilities, even if you are not a member of the Uchiha clan.\nInstalling Xaringan\rInstall the xaringan with CRAN or Github\ninstall.packages(\u0026#39;xaringan\u0026#39;)\r# or for latest version\rremotes::install_github(\u0026#39;yihui/xaringan\u0026#39;)\rSetting up the xaringan slides\r---\rtitle: \u0026quot;Title of my presentation\u0026quot;\rsubtitle: \u0026quot;Your subtitle\u0026quot;\rauthor: \u0026quot;**Dr. Ankit Deshmukh**\u0026quot;\rinstitute: \u0026quot;Affiliation\u0026quot;\rdate: \u0026quot;Week #: `r format(Sys.time(), \u0026#39;%d %B %Y\u0026#39;)`\u0026quot;\routput:\rxaringan::moon_reader:\rcss: [\u0026quot;css/default.css\u0026quot;, \u0026quot;css/metropolis.css\u0026quot;, \u0026quot;css/tachyons.min.css\u0026quot;]\rself_contained: false lib_dir: libs\rnature:\rhighlightStyle: solarized-light\rhighlightLines: true\rcountIncrementalSlides: false\rratio: 16:9\r---\rGlobal Setting for figures and code chunk\n```{r setup, include=FALSE}\roptions(htmltools.dir.version = FALSE)\rknitr::opts_chunk$set(\r#out.width = \u0026quot;100%\u0026quot;,\rcache = FALSE,\recho = TRUE,\rmessage = FALSE, warning = FALSE,\rfig.show = TRUE,\rhiline = TRUE,\rresults= \u0026quot;asis\u0026quot; # Useful to show bibliography as normal text.\r)\r```\rSetting up for Bibliography and Citation\r```{r setup, include=FALSE}\rlibrary(RefManageR)\rlibrary(bibtex)\rBibOptions(check.entries = FALSE, bib.style = \u0026quot;authoryear\u0026quot;, style = \u0026quot;text\u0026quot;, first.inits = FALSE)\rbib \u0026lt;- ReadBib(\u0026quot;~/adx/Bibliography.bib\u0026quot;) # A bibtex bibliography file. Use zotoro for this. ```\rUse xaringanExtra to enhance slide feature\ruse_logo for adding the logo in your slide.\ruse_progress_bar for progress bar.\ruse_extra_styles for code hover effect.\ruse_xaringan_extra(\"tile_view\") for see the preview of slides a once, it helps to jump on slides.\ruse_xaringan_extra(\"tachyons\") for an awesome miniature css with your slides. Find more here https://tachyons.io/#features\r```{r, echo=FALSE, include=TRUE}\rlibrary(xaringanExtra)\ruse_logo(image_url = \u0026quot;./css/Anix-Logo.png\u0026quot;, link_url = \u0026quot;https://www.ankitdeshmukh.com/\u0026quot;, width = \u0026quot;60px\u0026quot;, height = \u0026quot;60px\u0026quot;)\ruse_progress_bar(color = \u0026quot;#28282888\u0026quot;,location = \u0026quot;top\u0026quot;, height = \u0026quot;0.25em\u0026quot;)\ruse_extra_styles(hover_code_line = TRUE, mute_unhighlighted_code = FALSE)\ruse_xaringan_extra(c(\u0026quot;tile_view\u0026quot;, \u0026quot;tachyons\u0026quot;, \u0026quot;use_logo\u0026quot;, \u0026quot;use_progress_bar\u0026quot;))\r```\rOrganize all the image in images folder\ruse knitr of html+tachyons to add image\n```{r, include=TRUE, echo=FALSE, fig.align=\u0026#39;center\u0026#39;, out.width=\u0026quot;60%\u0026quot;}\rknitr::include_graphics(\u0026quot;images/YourImage.xyz\u0026quot;, error = FALSE)\r```\ror use html tags\r\u0026lt;img src=\u0026quot;images/YourImage.xyz\u0026quot; class=\u0026quot;w-60 br4 dib center\u0026quot;\u0026gt;\rCheckout the slides with Xaringan\rClick here to view in full screen\n","permalink":"https://ankitdeshmukh.com/post/2022-08-16-create-awesome-slides-with-xaringan/","summary":"HTML slides with Xaringan.\rXaringan is an R package for creating slideshows with remark.js through R Markdown.\n…from https://github.com/yihui/xaringan\nThe package name xaringan comes from Sharingan, a dōjutsu in Naruto with two abilities: the “Eye of Insight” and the “Eye of Hypnotism”. A presentation ninja should have these basic abilities, and I think remark.js may help you acquire these abilities, even if you are not a member of the Uchiha clan.","title":"Create awesome slides with Xaringan"},{"content":"\rThe main aim of this blog to show, how you can configure R, Python, and SQL in a single R-markdown file. Most of time we have to use data from databases and python code along with R functions, and having a setup that bring goodness of all the tool in one place comes really handy.\nSetup Python in Rstudio\rTo set up R Python And SQL in the Rstudio you have to first install miniconda. Miniconda helps to create python virtual environments and let you organize it.\rSuch as for, data cleaning you might use a data cleaning environment for web scraping you will use a web scraping environment and so on.\nCommands to manage conda environment inside RStudio:\rOnce you install miniconda You have to use R-reticulate in Rstudio, it is REPL (Read-eval-print loop) for Python in R, and helps to run Python code inside Rstudio. You can add miniconda into your windows terminal, but to use in Rstudio it is recommended to create and select environment from Rstudio itself.\nReticulate package provides all the necessary functionalities to run and manage python commands, such as\nload reticulate package with install.packages(reticulate) command in R console.\rcommand conda_ create() will create a new python environment.\ruse_condaenv(condaenv = \"ENVNAME\") to select a newly created environment or existing python environment\rto know the existing environments that you have created earlier, use conda_list() function.\ronce you select a environment now you are ready to install python packages with py_install(\"PackageName\").\rUse conda prompt\rI use conda prompt to create environment but some how those environment doesn’t work with my R-reticulate. But if you are using python only then you can configure conda with the following commands (run in system terminal):\nconda help for help.\rconda create --name your_environment to create a conda environment.\rconda env list to list all the environment in conda.\rconda activate your_environment to activate an specific conda environment.\rUse conda config to add other channels for package providers, but I tried pip and it worked just fine.\rconda config --add channels conda-forge (conda-forge is one of many channel)\rconda config --set channel_priority strict (set preference of channel)\rInstall a few packages: conda install pandas scikit-learn matplotlib\rHow to use SQL with R markdown\rUse dplyr, dbplyr, and RSQLite packages\nUse SQL block to view the query output in ln-line mode.\rA blogpot to learn R and SQL in Rstudio https://irene.rbind.io/post/using-sql-in-rstudio/\rExample of R, Python, and SQL with Rmarkdonw (*.Rmd)\rif(!require(dplyr)){install.packages(\u0026quot;dplyr\u0026quot;);library(dplyr)}\rif(!require(dbplyr)){install.packages(\u0026quot;dbplyr\u0026quot;);library(dbplyr)}\rif(!require(RSQLite)){install.packages(\u0026quot;RSQLite\u0026quot;);library(RSQLite)}\rconn \u0026lt;- src_memdb() # create a SQLite database in memory\r# Similar way you can add other database connections see `?dbplyr`\rcon_iso \u0026lt;- conn$con\rcopy_to(conn, storms, # this is a dataset built into dplyr\roverwrite = TRUE)\rtbl(conn, sql(\u0026quot;SELECT * FROM storms LIMIT 5\u0026quot;))\r## # Source: SQL [5 x 13]\r## # Database: sqlite 3.38.5 [:memory:]\r## name year month day hour lat long status category wind pressure\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 Amy 1975 6 27 0 27.5 -79 tropical de… -1 25 1013\r## 2 Amy 1975 6 27 6 28.5 -79 tropical de… -1 25 1013\r## 3 Amy 1975 6 27 12 29.5 -79 tropical de… -1 25 1013\r## 4 Amy 1975 6 27 18 30.5 -79 tropical de… -1 25 1013\r## 5 Amy 1975 6 28 0 31.5 -78.8 tropical de… -1 25 1012\r## # … with 2 more variables: tropicalstorm_force_diameter \u0026lt;int\u0026gt;,\r## # hurricane_force_diameter \u0026lt;int\u0026gt;\rNow create SLQ code chunk to directly run SQL queries from R.\n```{sql connection=con_iso}\rSELECT * FROM storms LIMIT 5;\r```\rThis will print 5 entire form database table storms\nTable 1: 5 records\rname\ryear\rmonth\rday\rhour\rlat\rlong\rstatus\rcategory\rwind\rpressure\rtropicalstorm_force_diameter\rhurricane_force_diameter\rAmy\r1975\r6\r27\r0\r27.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r27\r6\r28.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r27\r12\r29.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r27\r18\r30.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r28\r0\r31.5\r-78.8\rtropical depression\r-1\r25\r1012\rNA\rNA\rSQL code chunk can ouput the data as r-variable i.e. storm_preview\n```{sql connection=con_iso, output.var=\u0026quot;storm_preview\u0026quot;}\rSELECT * FROM storms LIMIT 5;\r```\r```{r}\rclass(storm_preview)\rstorm_preview ```\r## [1] \u0026quot;data.frame\u0026quot;\r## name year month day hour lat long status category wind\r## 1 Amy 1975 6 27 0 27.5 -79.0 tropical depression -1 25\r## 2 Amy 1975 6 27 6 28.5 -79.0 tropical depression -1 25\r## 3 Amy 1975 6 27 12 29.5 -79.0 tropical depression -1 25\r## 4 Amy 1975 6 27 18 30.5 -79.0 tropical depression -1 25\r## 5 Amy 1975 6 28 0 31.5 -78.8 tropical depression -1 25\r## pressure tropicalstorm_force_diameter hurricane_force_diameter\r## 1 1013 NA NA\r## 2 1013 NA NA\r## 3 1013 NA NA\r## 4 1013 NA NA\r## 5 1012 NA NA\rNow, load reticulate to run python codes\n```{r}\r# Below code will check if `reticulate` is installed or not, if not then it will install and load in the R-session. if(!require(reticulate)){install.packages(\u0026quot;reticulate\u0026quot;);library(reticulate)}\r```\rA example of Python code inside R markdown.\n```{python}\rfrom matplotlib import pyplot as plt # Importing Numpy Library import numpy as np plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) mu = 50 sigma = 7 x = np.random.normal(mu, sigma, size=200) fig, ax = plt.subplots() ax.hist(x, 20) ax.set_title(\u0026#39;Historgram\u0026#39;) ax.set_xlabel(\u0026#39;bin range\u0026#39;) ax.set_ylabel(\u0026#39;frequency\u0026#39;) fig.tight_layout() plt.show() # Comment out if you are using blogdown-sites to render the site. ```\r## (array([ 2., 1., 2., 8., 9., 12., 22., 25., 31., 24., 15., 15., 13.,\r## 9., 7., 2., 1., 0., 1., 1.]), array([31.4398407 , 33.56751485, 35.695189 , 37.82286315, 39.95053731,\r## 42.07821146, 44.20588561, 46.33355976, 48.46123392, 50.58890807,\r## 52.71658222, 54.84425638, 56.97193053, 59.09960468, 61.22727883,\r## 63.35495299, 65.48262714, 67.61030129, 69.73797544, 71.8656496 ,\r## 73.99332375]), \u0026lt;BarContainer object of 20 artists\u0026gt;)\rFigure 1: A figure python output of Histogram plot.\r","permalink":"https://ankitdeshmukh.com/post/2022-07-04-r-python-sql-in-rstudio/","summary":"The main aim of this blog to show, how you can configure R, Python, and SQL in a single R-markdown file. Most of time we have to use data from databases and python code along with R functions, and having a setup that bring goodness of all the tool in one place comes really handy.\nSetup Python in Rstudio\rTo set up R Python And SQL in the Rstudio you have to first install miniconda.","title":"Setting up R, Python, and SQL in RStudio"},{"content":"\rCourse I teach for graduate student and Ph.D.\nR For Hydrology and Water Resources\rtools available for Hydrology\rTauDEM\rQGIS\rGrass GIS\rR (Geo-spatial analysis)\rLand use/ Land cover classification\rSWAT\rbasic hydrologic concept and methods\rguidelines for the stream network analysis\rComputational Hydrology\rHydrology is the study of water across the earth system. I will tell you some interesting phenomenon of hydrology in this blog post(s). The main goal of writing post is to summarize the knowledge of Water resource and Hydrology.\nMostly I will refer two books of hydrology Applied Hydrology (Chow, Maidment, and Mays 1988) and Rainfall-Runoff Modelling: The Primer (Beven 2012)\nReference\rBeven, K. J. 2012. Rainfall-Runoff Modelling: The Primer. 2nd ed. Chichester, West Sussex ; Hoboken, NJ: Wiley-Blackwell.\rChow, Ven Te, David R. Maidment, and Larry W. Mays. 1988. Applied Hydrology. McGraw-Hill Series in Water Resources and Environmental Engineering. New York: McGraw-Hill.\r","permalink":"https://ankitdeshmukh.com/post/2022-07-02-computational-hydrology/","summary":"Course I teach for graduate student and Ph.D.\nR For Hydrology and Water Resources\rtools available for Hydrology\rTauDEM\rQGIS\rGrass GIS\rR (Geo-spatial analysis)\rLand use/ Land cover classification\rSWAT\rbasic hydrologic concept and methods\rguidelines for the stream network analysis\rComputational Hydrology\rHydrology is the study of water across the earth system. I will tell you some interesting phenomenon of hydrology in this blog post(s). The main goal of writing post is to summarize the knowledge of Water resource and Hydrology.","title":"Computational Hydrology"},{"content":"\rThe following packages are required for the random forest\nif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)}\rif(!require(janitor)){install.packages(\u0026quot;janitor\u0026quot;);library(janitor)} # for rename\rif(!require(randomForest)){install.packages(\u0026quot;randomForest\u0026quot;);library(randomForest)}\rif(!require(caret)){install.packages(\u0026quot;caret\u0026quot;);library(caret)} # for `confustionMatrix`\rA Random forest is made of Random Trees\rData \u0026lt;- read_csv(file = here::here(\u0026quot;content/post/2022-06-26-random-forest\u0026quot;, \u0026quot;german_credit.csv\u0026quot;))\rExploring the dataset\rData \u0026lt;- clean_names(Data)\rData$creditability \u0026lt;- as.factor(Data$creditability)\rglimpse(Data)\r## Rows: 1,000\r## Columns: 21\r## $ creditability \u0026lt;fct\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ account_balance \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 4, 2, 1, 1, …\r## $ duration_of_credit_month \u0026lt;dbl\u0026gt; 18, 9, 12, 12, 12, 10, 8, 6, 18, 24,…\r## $ payment_status_of_previous_credit \u0026lt;dbl\u0026gt; 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, …\r## $ purpose \u0026lt;dbl\u0026gt; 2, 0, 9, 0, 0, 0, 0, 0, 3, 3, 0, 1, …\r## $ credit_amount \u0026lt;dbl\u0026gt; 1049, 2799, 841, 2122, 2171, 2241, 3…\r## $ value_savings_stocks \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 2, …\r## $ length_of_current_employment \u0026lt;dbl\u0026gt; 2, 3, 4, 3, 3, 2, 4, 2, 1, 1, 3, 4, …\r## $ instalment_per_cent \u0026lt;dbl\u0026gt; 4, 2, 2, 3, 4, 1, 1, 2, 4, 1, 2, 1, …\r## $ sex_marital_status \u0026lt;dbl\u0026gt; 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 4, …\r## $ guarantors \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ duration_in_current_address \u0026lt;dbl\u0026gt; 4, 2, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, …\r## $ most_valuable_available_asset \u0026lt;dbl\u0026gt; 2, 1, 1, 1, 2, 1, 1, 1, 3, 4, 1, 3, …\r## $ age_years \u0026lt;dbl\u0026gt; 21, 36, 23, 39, 38, 48, 39, 40, 65, …\r## $ concurrent_credits \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, …\r## $ type_of_apartment \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, …\r## $ no_of_credits_at_this_bank \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, …\r## $ occupation \u0026lt;dbl\u0026gt; 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, …\r## $ no_of_dependents \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, …\r## $ telephone \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ foreign_worker \u0026lt;dbl\u0026gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, …\rAssess the creditabiliy with the help of other variables\r# code -------------------------------------------------------------------------\rggplot(data = Data, aes(x = age_years, color = creditability, fill = creditability)) +\rgeom_histogram(binwidth = 5, position = \u0026quot;identity\u0026quot;, alpha = 0.4) +\rscale_x_continuous(breaks = scales::pretty_breaks(n = 6)) +\rscale_y_continuous(breaks = scales::pretty_breaks(n = 6)) + theme_minimal()\r# Create a training and testing data\rset.seed(7791)\rpartitioning \u0026lt;- sample(2, nrow(Data), replace = TRUE, prob = c(0.8, 0.2))\rtable(partitioning)\r## partitioning\r## 1 2 ## 797 203\rtrain \u0026lt;- Data[partitioning == 1, ]\rtable(train$creditability)\r## ## 0 1 ## 244 553\rtest \u0026lt;- Data[partitioning == 2, ]\rtable(test$creditability)\r## ## 0 1 ## 56 147\rTrain the Random forest model\r# Generate random forest with train data\rrf_model \u0026lt;- randomForest(formula = creditability ~ ., data = train)\rpredict_train \u0026lt;- predict(rf_model, train)\rconfusionMatrix(predict_train, train$creditability)\r## Confusion Matrix and Statistics\r## ## Reference\r## Prediction 0 1\r## 0 244 0\r## 1 0 553\r## ## Accuracy : 1 ## 95% CI : (0.9954, 1)\r## No Information Rate : 0.6939 ## P-Value [Acc \u0026gt; NIR] : \u0026lt; 2.2e-16 ## ## Kappa : 1 ## ## Mcnemar\u0026#39;s Test P-Value : NA ## ## Sensitivity : 1.0000 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 1.0000 ## Prevalence : 0.3061 ## Detection Rate : 0.3061 ## Detection Prevalence : 0.3061 ## Balanced Accuracy : 1.0000 ## ## \u0026#39;Positive\u0026#39; Class : 0 ## Testing the model rf_model on test data\rpredict_test \u0026lt;- predict(rf_model, test)\rconfusionMatrix(predict_test, test$creditability)\r## Confusion Matrix and Statistics\r## ## Reference\r## Prediction 0 1\r## 0 28 12\r## 1 28 135\r## ## Accuracy : 0.803 ## 95% CI : (0.7415, 0.8553)\r## No Information Rate : 0.7241 ## P-Value [Acc \u0026gt; NIR] : 0.006157 ## ## Kappa : 0.459 ## ## Mcnemar\u0026#39;s Test P-Value : 0.017706 ## ## Sensitivity : 0.5000 ## Specificity : 0.9184 ## Pos Pred Value : 0.7000 ## Neg Pred Value : 0.8282 ## Prevalence : 0.2759 ## Detection Rate : 0.1379 ## Detection Prevalence : 0.1970 ## Balanced Accuracy : 0.7092 ## ## \u0026#39;Positive\u0026#39; Class : 0 ## # Reference\r# Prediction 0 1\r# 0 29 14\r# 1 27 133\rvarImpPlot(rf_model)\rOptimize the performance of randomforest.\rplot(rf_model) # black line is out of bag error.\roob_error \u0026lt;- double(20)\rfor (mtry in 1:20) {\rrf \u0026lt;- randomForest(formula = creditability ~ ., data = train, mtry = mtry, ntree = 166)\roob_error[mtry] \u0026lt;- rf$err.rate[166]\r}\rplot(1:20, oob_error, type = \u0026quot;b\u0026quot;, xlab = \u0026quot;Number of variable considered\u0026quot;, ylab = \u0026quot;Out of bag erro [-]\u0026quot;, xaxt = \u0026quot;n\u0026quot;)\raxis(1, at = 1:20, labels = 1:20, cex = 0.8)\ropti_num_var = which.min(oob_error)\rRe running the random forest with optimum number of variables\rrf_optim \u0026lt;- randomForest(formula = creditability ~ ., data = train, mtry = opti_num_var, ntree = 166)\r# Testing the model `rf_model` on test data\rconfusionMatrix(predict(rf_optim, test), test$creditability)\r## Confusion Matrix and Statistics\r## ## Reference\r## Prediction 0 1\r## 0 33 20\r## 1 23 127\r## ## Accuracy : 0.7882 ## 95% CI : (0.7255, 0.8423)\r## No Information Rate : 0.7241 ## P-Value [Acc \u0026gt; NIR] : 0.02266 ## ## Kappa : 0.4609 ## ## Mcnemar\u0026#39;s Test P-Value : 0.76037 ## ## Sensitivity : 0.5893 ## Specificity : 0.8639 ## Pos Pred Value : 0.6226 ## Neg Pred Value : 0.8467 ## Prevalence : 0.2759 ## Detection Rate : 0.1626 ## Detection Prevalence : 0.2611 ## Balanced Accuracy : 0.7266 ## ## \u0026#39;Positive\u0026#39; Class : 0 ## Exploring useful variables\rtrain \u0026lt;- as.data.frame(train)\rvarImpPlot(rf_model)\rimportance(rf_model)\r## MeanDecreaseGini\r## account_balance 36.543019\r## duration_of_credit_month 34.072986\r## payment_status_of_previous_credit 18.303982\r## purpose 20.837156\r## credit_amount 46.208519\r## value_savings_stocks 18.564843\r## length_of_current_employment 18.296281\r## instalment_per_cent 13.122638\r## sex_marital_status 12.781397\r## guarantors 7.055415\r## duration_in_current_address 13.666627\r## most_valuable_available_asset 14.340330\r## age_years 33.451374\r## concurrent_credits 8.064289\r## type_of_apartment 8.821601\r## no_of_credits_at_this_bank 7.306201\r## occupation 10.567355\r## no_of_dependents 4.325819\r## telephone 6.359517\r## foreign_worker 1.666973\r# How variable affect the chance of getting loan.\rpartialPlot(rf_model, train, account_balance, \u0026quot;1\u0026quot;)\rpartialPlot(rf_model, train, age_years, \u0026quot;1\u0026quot;)\rThis analysis for discrete variable of creditability.\n","permalink":"https://ankitdeshmukh.com/post/2022-06-26-random-forest/","summary":"The following packages are required for the random forest\nif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)}\rif(!require(janitor)){install.packages(\u0026quot;janitor\u0026quot;);library(janitor)} # for rename\rif(!require(randomForest)){install.packages(\u0026quot;randomForest\u0026quot;);library(randomForest)}\rif(!require(caret)){install.packages(\u0026quot;caret\u0026quot;);library(caret)} # for `confustionMatrix`\rA Random forest is made of Random Trees\rData \u0026lt;- read_csv(file = here::here(\u0026quot;content/post/2022-06-26-random-forest\u0026quot;, \u0026quot;german_credit.csv\u0026quot;))\rExploring the dataset\rData \u0026lt;- clean_names(Data)\rData$creditability \u0026lt;- as.factor(Data$creditability)\rglimpse(Data)\r## Rows: 1,000\r## Columns: 21\r## $ creditability \u0026lt;fct\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ account_balance \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 4, 2, 1, 1, …\r## $ duration_of_credit_month \u0026lt;dbl\u0026gt; 18, 9, 12, 12, 12, 10, 8, 6, 18, 24,…\r## $ payment_status_of_previous_credit \u0026lt;dbl\u0026gt; 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, …\r## $ purpose \u0026lt;dbl\u0026gt; 2, 0, 9, 0, 0, 0, 0, 0, 3, 3, 0, 1, …\r## $ credit_amount \u0026lt;dbl\u0026gt; 1049, 2799, 841, 2122, 2171, 2241, 3…\r## $ value_savings_stocks \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 2, …\r## $ length_of_current_employment \u0026lt;dbl\u0026gt; 2, 3, 4, 3, 3, 2, 4, 2, 1, 1, 3, 4, …\r## $ instalment_per_cent \u0026lt;dbl\u0026gt; 4, 2, 2, 3, 4, 1, 1, 2, 4, 1, 2, 1, …\r## $ sex_marital_status \u0026lt;dbl\u0026gt; 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 4, …\r## $ guarantors \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ duration_in_current_address \u0026lt;dbl\u0026gt; 4, 2, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, …\r## $ most_valuable_available_asset \u0026lt;dbl\u0026gt; 2, 1, 1, 1, 2, 1, 1, 1, 3, 4, 1, 3, …\r## $ age_years \u0026lt;dbl\u0026gt; 21, 36, 23, 39, 38, 48, 39, 40, 65, …\r## $ concurrent_credits \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, …\r## $ type_of_apartment \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, …\r## $ no_of_credits_at_this_bank \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, …\r## $ occupation \u0026lt;dbl\u0026gt; 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, …\r## $ no_of_dependents \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, …\r## $ telephone \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ foreign_worker \u0026lt;dbl\u0026gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, …\rAssess the creditabiliy with the help of other variables\r# code -------------------------------------------------------------------------\rggplot(data = Data, aes(x = age_years, color = creditability, fill = creditability)) +\rgeom_histogram(binwidth = 5, position = \u0026quot;identity\u0026quot;, alpha = 0.","title":"Random Forest with R-Programming"},{"content":"\rHow to make a better boxplot with custom fonts, let’s explore this in this post, it can be used for the standard template for boxplot with facet and user defined fonts.\nRequired R libraries:\rif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)} # for ggplot2 function\rif(!require(gapminder)){install.packages(\u0026quot;gapminder\u0026quot;);library(gapminder)} # for sample data\rif(!require(showtext)){install.packages(\u0026quot;showtext\u0026quot;);library(showtext)} # to import fonts\rAdd fonts in R session\rfont_add_google(\u0026quot;Karla\u0026quot;, \u0026quot;Karla\u0026quot;) # adding local font\rfont_add(family = \u0026quot;Helvetica\u0026quot;, regular = \u0026quot;C:/Windows/Fonts/Helvetica 400.ttf\u0026quot;)\r# Adding from Google fonts\rfont_add_google(\u0026quot;Roboto Slab\u0026quot;, \u0026quot;Roboto Slab\u0026quot;) # adding font from the web/google font\rfont_families()\r## [1] \u0026quot;sans\u0026quot; \u0026quot;serif\u0026quot; \u0026quot;mono\u0026quot; \u0026quot;wqy-microhei\u0026quot; \u0026quot;Karla\u0026quot; ## [6] \u0026quot;Helvetica\u0026quot; \u0026quot;Roboto Slab\u0026quot;\rDefine theme for Boxplot and fonts\rthemeBox \u0026lt;- function(base_family = \u0026quot;sans\u0026quot;, exFont, ...){\rtheme_bw(base_family = base_family, ...) +\rtheme(\rpanel.grid = element_blank(),\rplot.title = element_text(size = 8),\raxis.ticks.length = unit(-0.05, \u0026quot;in\u0026quot;),\raxis.text.y = element_text(margin=unit(c(0.3,0.3,0.3,0.3), \u0026quot;cm\u0026quot;)),\raxis.text.x = element_text(margin=unit(c(0.3,0.3,0.3,0.3), \u0026quot;cm\u0026quot;)),\raxis.ticks.x = element_blank(),\raspect.ratio = 1,\rlegend.background = element_rect(color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;),\rtext = element_text(family=exFont)\r)\r}\rPlot the boxplots of Average Life Expectancy\rggplot(gapminder, aes(x = continent, y = lifeExp, fill = continent)) +\rfacet_wrap(~year) +\rgeom_boxplot(linetype = \u0026quot;dashed\u0026quot;) +\rstat_boxplot(aes(ymin = ..lower.., ymax = ..upper..), outlier.shape = 1) +\rstat_boxplot(geom = \u0026quot;errorbar\u0026quot;, aes(ymin = ..ymax..)) +\rstat_boxplot(geom = \u0026quot;errorbar\u0026quot;, aes(ymax = ..ymin..)) +\rscale_y_continuous(name = \u0026quot;Average Life Expectancy\u0026quot;) +\rscale_x_discrete(labels = abbreviate, name = \u0026quot;Continent\u0026quot;) + themeBox(exFont = \u0026quot;Karla\u0026quot;)\rLinks:\rBoxplot customization\r","permalink":"https://ankitdeshmukh.com/post/2022-01-21-pretty-boxplot-with-focet/","summary":"How to make a better boxplot with custom fonts, let’s explore this in this post, it can be used for the standard template for boxplot with facet and user defined fonts.\nRequired R libraries:\rif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)} # for ggplot2 function\rif(!require(gapminder)){install.packages(\u0026quot;gapminder\u0026quot;);library(gapminder)} # for sample data\rif(!require(showtext)){install.packages(\u0026quot;showtext\u0026quot;);library(showtext)} # to import fonts\rAdd fonts in R session\rfont_add_google(\u0026quot;Karla\u0026quot;, \u0026quot;Karla\u0026quot;) # adding local font\rfont_add(family = \u0026quot;Helvetica\u0026quot;, regular = \u0026quot;C:/Windows/Fonts/Helvetica 400.ttf\u0026quot;)\r# Adding from Google fonts\rfont_add_google(\u0026quot;Roboto Slab\u0026quot;, \u0026quot;Roboto Slab\u0026quot;) # adding font from the web/google font\rfont_families()\r## [1] \u0026quot;sans\u0026quot; \u0026quot;serif\u0026quot; \u0026quot;mono\u0026quot; \u0026quot;wqy-microhei\u0026quot; \u0026quot;Karla\u0026quot; ## [6] \u0026quot;Helvetica\u0026quot; \u0026quot;Roboto Slab\u0026quot;\rDefine theme for Boxplot and fonts\rthemeBox \u0026lt;- function(base_family = \u0026quot;sans\u0026quot;, exFont, .","title":"How to create a pretty facet-boxplot with custom fonts"},{"content":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nA few useful syntax are shown below in the post:\n1. You can embed an R code chunk like this:\rsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r2. Including Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\rFigure 1: A fancy pie chart.\rReferencing a image/figure.\r# Define the tag after lagnuage notation such as `pie`\r{r pie, fig.cap=\u0026#39;A fancy pie chart.\u0026#39;, tidy=FALSE}\rUse \\@ref(fig:pie) to refrence an image. 3. Inline R Code\rThere were `r nrow(cars)` cars studied\rThere were 50 cars studied.\n4. Use links in Rmarkdown\rUse a plain http address or add a link to a phrase:\nhttp://example.com\rlinked phrase\ryou can also use use a snippet predefined for Rmd file link (Shfit + Tab) to create a link.\n5. Use local images or image URLs.\r![Caption](http://example.com/logo.png)\r![optional caption text](figures/img.png)\r6. Tables in R markdown\rInsert table use knitr::kable() function. Applicable for any 2D rectangular data(Data Frame, Matrix, etc.).\rknitr::kable(head(iris[, 1:3]), \u0026quot;pipe\u0026quot;)\rSepal.Length\rSepal.Width\rPetal.Length\r5.1\r3.5\r1.4\r4.9\r3.0\r1.4\r4.7\r3.2\r1.3\r4.6\r3.1\r1.5\r5.0\r3.6\r1.4\r5.4\r3.9\r1.7\rOr create table with traditional markdown way.\rmpg\rcyl\rdisp\rhp\rMazda RX4\r21.0\r6\r160\r110\rMazda RX4 Wag\r21.0\r6\r160\r110\rDatsun 710\r22.8\r4\r108\r93\rHornet 4 Drive\r21.4\r6\r258\r110\rHornet Sportabout\r18.7\r8\r360\r175\rValiant\r18.1\r6\r225\r105\r7. LaTeX Equations\rInline equation:\r$equation$\r\\(\\int\\limits_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx = \\sqrt{\\pi}\\)\nDisplay equation:\r$$equation$$\r\\[\\int\\limits_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx = \\sqrt{\\pi}\\]\n8. Sub/Super scripts and others\rsuperscript^2^ subscript~2~ ~~strikethrough~~\r\u0026lt;mark\u0026gt;This is a highlighted text\u0026lt;/mark\u0026gt;\rsuperscript2\nsubscript2\nstrikethrough\nThis is a highlighted text\n9. Use Bibliography\rCitation in sentence @R-base or after the sentence [@casella2002statistical] and one more [-@king1974nonoperative]\rCitation in sentence Brutsaert (2005) or after the sentence (Brath and Jonker 2015) and one more (2022)\nThis will automatically add the bibliography at the end of the documents. For more information see this\n10. Links and footnotes\rA sample phrase^[This is a footnote; bottom of the page]\rA sample phrase1\nReference\rBrath, Richard, and David Jonker. 2015. Graph Analysis and Visualization: Discovering Business Opportunity in Linked Data. Indianapolis, Ind: Wiley.\rBrutsaert, Wilfried. 2005. Hydrology: An Introduction. Cambridge ; New York: Cambridge University Press.\rChen, Chen, Jiange Jiang, Zhan Liao, Yang Zhou, Hao Wang, and Qingqi Pei. 2022. “A Short-Term Flood Prediction Based on Spatial Deep Learning Network: A Case Study for Xi County, China.” Journal of Hydrology 607 (April): 127535. https://doi.org/10.1016/j.jhydrol.2022.127535.\rThis is a footnote text; see in the bottom of the page.↩︎\n","permalink":"https://ankitdeshmukh.com/post/2021-12-01-r-markdown/","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nA few useful syntax are shown below in the post:\n1. You can embed an R code chunk like this:\rsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.","title":"Hello R Markdown!"},{"content":"\rR is a very versatile statistical tool and programming language. It’s my all-time good-to-go data analysis tool. It is fast, reliable and nifty. It provides great flexibility for my daily work and analysis tasks. If one uses R, they could consider RStudio as a more sophisticated GUI than the Base R once.\n1. Installing R\rStep 01: Install R-Binaries from R-Project\nStep 02: Install RStudio IDE from RStudio\n2. Customizing RStudio\rA lot of ways you can make RStudio more useful for your personal use. Like updating your ‘.RProfile’, adding an awesome theme, and fonts that support ligatures.\nUsing Rstudio as R-IDE: R studio has multiple windows but the most important are Code editor, console, Environment variable pane, and plot output pane.\rFigure 1: R-Studio IDE have many pans.\rCode editor: You will write your code in this window. R used # as the comment character. To assign a variable to a value we use ← (lowercase followed by a dash).\r2.1 Customize R startup with “.Rprofile”\rIf you are using a Windows machine you can find the location of your ‘.Rprofile’ at ‘C:/Users/UserName/Documents’. The code block is shown below. This is what my ‘.Rprofile’ looks like. Several costume functions can be added here and they will load with R startup every time.\ncat(\u0026quot;\\014\u0026quot;)\rcat(\u0026quot;Hi Ankit! What are we doing today?\\n\u0026quot;)\rFigure 2: A RStudio window\r2.2 Adding a theme\rI personally like the dark theme for my R studio. I particularly like to use the ‘Gruvbox’ theme that is not available in RStudio but can be downloaded from here. Download the file and paste it to 'C:\\Users\\UserName\\AppData\\Roaming\\RStudio\\themes'. To apply the theme in R studio go to Tools \u0026gt; Global Option \u0026gt; Appearance and select editor theme as gruvbox.\nFigure 3: RSudio configuration windows\r2.3 Adding fonts that support ligature\rThis helps to read and understand code faster and efficiently, mostly the merged common occurring 2 characters to one for easy reading but this is just a font rendering feature it means the underlying code remains ASCII-compatible [Source].\nFigure 4: Font Ligature makes code more asthetic pleasing and readable\rFira Code is a free monospaced font containing ligatures for common programming multi-character combinations.\nFigure 5: Fira Font with different themes\rDownload and FiraCode font from Here, and Install on your machine for all users. To apply the theme in R studio go to Tools \u0026gt; Global Option \u0026gt; Appearance and select editor font as Fira Code.\n3. Customize R with code snippets\rThe snippet is a re-usable piece of code or text. Ordinarily, these are formally defined operative units to incorporate into larger programming modules. To repeat a few operations and formats you can use snippets in R. To edit or add snippets in RStudio go to Tools \u0026gt; Global Option \u0026gt; Code \u0026gt; Editing, now enable Snippets and click edit snippets.\nFigure 6: R Snippets for quick code chunks.\rFrom the edit snippets window you can manage snippets of R. Mostly I use R and R markdown snippets in my daily use. Few useful snippets are:\nsnippet cls\rgraphics.off(); rm(list = ls()); cat(\u0026quot;\\014\u0026quot;)\rsnippet rqr\rif(!require(${1:packageName})){install.packages(\u0026quot;${1:packageName}\u0026quot;);\rlibrary(${1:packageName})}\rsnippet fmt\r# Title :: ${1:File Title}------------------------------------------------\r# Author :: Ankit Deshmukh\r# DOC :: `r eval(Sys.Date())`\r# DOLE :: `r eval(Sys.time())`\r# Description :: ${2:File Description}\r# setup ------------------------------------------------------------------------\rgraphics.off(); cat(\u0026quot;\\014\u0026quot;)\rsetwd(\u0026quot;`r eval(getwd())`\u0026quot;)\r# libraries --------------------------------------------------------------------\rlibrary(tidyverse)\r# code -------------------------------------------------------------------------\rsnippet pp\r\u0026quot;`r gsub(\u0026quot;\\\\\\\\\u0026quot;, \u0026quot;/\u0026quot;, readClipboard())`\u0026quot;\rsnippet clear\rrm(list = ls()); graphics.off();cat(\u0026quot;\\014\u0026quot;)\rTo use the snippet use the keywords such as clear and press Shift + Tab to auto complete the snippet text.\n","permalink":"https://ankitdeshmukh.com/post/2021-09-20-getting-started-with-r/","summary":"R is a very versatile statistical tool and programming language. It’s my all-time good-to-go data analysis tool. It is fast, reliable and nifty. It provides great flexibility for my daily work and analysis tasks. If one uses R, they could consider RStudio as a more sophisticated GUI than the Base R once.\n1. Installing R\rStep 01: Install R-Binaries from R-Project\nStep 02: Install RStudio IDE from RStudio\n2. Customizing RStudio\rA lot of ways you can make RStudio more useful for your personal use.","title":"Getting Started with R Programming"}]