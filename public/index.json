[{"content":"\rSBI Submits all the data to ECI and ECI publish it on Thursday 21 Mar 2024, 6:32 PM Links of 2 files are provided here:\nDisclosure of Electoral Bonds\nDetails of Electoral Bonds submitted by SBI on 21st March 2024 (EB_Redemption_Details)\nDetails of Electoral Bonds submitted by SBI on 21st March 2024 (EB_Purchase_Details)\nTo know the background of the case read this: SBI submits all electoral bond details, including unique numbers, to ECI\nA short note on regexregex\rregex, short for Regular Expressions, are essentially sequences of characters that form a search pattern, which is then used to match strings or parts of strings in a text. Think of them as sophisticated search queries that allow you to find specific patterns within a larger body of text. regex can be used for simple patterns like a specific word or phrase, or you can create complex patterns to match more intricate structures such as email addresses, URLs, dates, and more.\nFor example, if you wanted to find all email addresses in a text document, you could use a regular expression pattern like [a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\\\\\.[a-zA-Z0-9-.]+. This pattern breaks down as follows:\n# Sample emails\rem1 \u0026lt;- \u0026quot;example-1_Random@gmail.com\u0026quot; # this is a valid email\rem2 \u0026lt;- \u0026quot;example*1_Random$gmail.com\u0026quot; # this is an invalid email. # Detect email addresses\rstringr::str_detect(c(em1, em2), \u0026quot;[a-zA-Z0-9_.+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z0-9-.]+\u0026quot;)\r## [1] TRUE FALSE\ra-zA-Z0-9_.+-]+ matches one or more alphanumeric characters, dots, underscores, percent signs, plus signs, or hyphens, representing the username portion of an email address.\n\\(\\textbf{@}\\) matches the “@” symbol, separating the username from the domain.\n[A-Za-z0-9.-]+ matches one or more alphanumeric characters, dots, or hyphens, representing the domain name.\n\\. matches a literal dot, separating the domain from the top-level domain.\n[a-zA-Z0-9-.]+ matches two or more alphabetic characters, representing the top-level domain (e.g., com, org, net).\nRegular expressions can be used in various programming languages, here we are using it with R programming with stringr package.\nRequired R libraries for this analysis\r# To process the PDF document if(!require(lubridate)){install.packages(\u0026quot;lubridate\u0026quot;);library(lubridate)}\rif(!require(pdftools)){install.packages(\u0026quot;pdftools\u0026quot;);library(pdftools)}\rif(!require(stringr)){install.packages(\u0026quot;stringr\u0026quot;);library(stringr)}\rif(!require(readxl)){install.packages(\u0026quot;readxl\u0026quot;);library(readxl)}\rif(!require(DT)){install.packages(\u0026quot;DT\u0026quot;);library(DT)}\rFigure 1: Structure of the files in working directory\rCleaning the Party data\rWe starting with reading the file Parties.pdf from ./Data/Raw Data/ directory. The regular expression [\\r\\n]+ is used to match one or more occurrences of either a carriage return ( or a newline character (). After that str_trip remove space either from starting or end of the each string. str_split_fixed split string with \"\\\\s{2,}\" means 2 consiqutive space till it reaches 9 element. We choose 9 because there are total 9 column in Parties.pdf.\nParty \u0026lt;- pdf_text(\u0026quot;./Data/Raw Data/Parties.pdf\u0026quot;)\rParty \u0026lt;- unlist(str_split(Party, \u0026quot;[\\\\r\\\\n]+\u0026quot;))\rParty \u0026lt;- as.data.frame(str_split_fixed(str_trim(Party), \u0026quot;\\\\s{2,}\u0026quot;, n = 9))\rhead(Party,5)\r## V1 V2 V3\r## 1 Date of Account no. of Bond\r## 2 Sr No. Name of the Political Party Prefix\r## 3 Encashment Political Party Number\r## 4 1 12/Apr/2019 ALL INDIA ANNA DRAVIDA MUNNETRA KAZHAGAM *******5199\r## 5 2 12/Apr/2019 ALL INDIA ANNA DRAVIDA MUNNETRA KAZHAGAM *******5199\r## V4 V5 V6 V7 V8 V9\r## 1 Pay Branch ## 2 Denominations Pay Teller ## 3 Code ## 4 OC 775 1,00,00,000 00800 2770121 ## 5 OC 3975 1,00,00,000 00800 2770121\rUpon inspection we find all the pages of pdf as the table header and now we are removing the headers, I remove row containing “Date of”, “Sr No.”, “Encashment” using V1 column.\nAfter that we have Page.and it’s numbers for each pages read by pdftools. We remove using str_extract, here Page{dash} represent anything after Page must be selected. We also remove rows that have all the columns emptly.\nIndex = Party$V1 %in% c(\u0026quot;Date of\u0026quot;, \u0026quot;Sr No.\u0026quot;, \u0026quot;Encashment\u0026quot;)\rParty \u0026lt;- Party[!Index,]\rIndex2 = is.na(str_extract(Party$V1, \u0026quot;Page.\u0026quot;))\rParty \u0026lt;- Party[Index2,];\rParty \u0026lt;- Party[!apply(Party == \u0026quot;\u0026quot;, 1, all),]\rrm(Index, Index2)\rNow, all the data is stored in the Party variable the structure of it.\nclass(Party)\r## [1] \u0026quot;data.frame\u0026quot;\rdim(Party)\r## [1] 20421 9\rWe found at some row total number of columns are greater then 9. So before constructing the final data frame we choose checking each row with for loop. We pre-poulated several columns such as SN, Date of Enchashment\nn = rep(NA,dim(Party)[1])\rIndex_m \u0026lt;- !nchar(Party$V9)\u0026gt;0\rCol1 = as.data.frame(str_split_fixed(Party$V2, pattern = \u0026quot; \u0026quot;, n = 2))\rParty_Data \u0026lt;- data.frame(\u0026quot;SN\u0026quot; = as.numeric(Party$V1),\r\u0026quot;Date of Encashment\u0026quot; = lubridate::dmy(Col1$V1),\r\u0026quot;Party Name\u0026quot; = n,\r\u0026quot;Account No\u0026quot; = n,\r\u0026quot;Prefix\u0026quot; = n,\r\u0026quot;Bond No\u0026quot; = n,\r\u0026quot;Denominations\u0026quot; = n,\r\u0026quot;Pay Branch Code\u0026quot; = n,\r\u0026quot;Pay Teller\u0026quot; = n)\rcol7_t \u0026lt;- as.numeric(gsub(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;, Party$V6))\rcol7_f \u0026lt;- as.numeric(gsub(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;, Party$V7))\rfor(ii in seq_along(n)){\rif (Index_m[ii]) {\rParty_Data[ii,3] \u0026lt;- Col1$V2[ii]\rParty_Data[ii,4] \u0026lt;- Party$V3[ii]\rParty_Data[ii,5] \u0026lt;- Party$V4[ii]\rParty_Data[ii,6] \u0026lt;- as.numeric(Party$V5)[ii]\rParty_Data[ii,7] \u0026lt;- col7_t[ii]\rParty_Data[ii,8] \u0026lt;- Party$V7[ii]\rParty_Data[ii,9] \u0026lt;- as.numeric(Party$V8)[ii]\r} else {\rParty_Data[ii,3] \u0026lt;- Party$V3[ii]\rParty_Data[ii,4] \u0026lt;- Party$V4[ii]\rParty_Data[ii,5] \u0026lt;- Party$V5[ii]\rParty_Data[ii,6] \u0026lt;- as.numeric(Party$V6)[ii]\rParty_Data[ii,7] \u0026lt;- col7_f[ii]\rParty_Data[ii,8] \u0026lt;- Party$V8[ii]\rParty_Data[ii,9] \u0026lt;- as.numeric(Party$V9)[ii]\r}\r# print(ii) # running this might take some time. # IMP: don\u0026#39;t run white building the site.\r}\rNow, we have table similar to pdf, but I want to create unique id un_id by merging Perfiex with Bond No., so that I can match Parties to Donner, after that I save the data into Party_Cleaned.xlsx file.\nun_id= paste0(Party_Data$Prefix,\rstr_pad(Party_Data$Bond.No, width = 5,pad = \u0026quot;0\u0026quot;))\rParty_Data$unique_ID \u0026lt;- un_id\rwrite.xlsx(Party_Data, file=\u0026quot;./Data/Processed xlsx/Party_Cleaned.xlsx\u0026quot;, row.names = FALSE)\rNow to time to generate some table data, I am aggregation Party and Denominations data for table. To aggregate the data with Party name aggregate function from base R is used. After that DT::datatable is used for interactive table.\nParty_Data \u0026lt;- read_xlsx(path = \u0026quot;./Data/Processed xlsx/Party_Cleaned.xlsx\u0026quot;)\rfx_name \u0026lt;- function(x) {c(length(x), median(x), round(mean(x),0), sum(x))}\ragg_data = aggregate(Party_Data, Denominations ~ Party.Name, FUN = fx_name)\ragg_data_df \u0026lt;- data.frame(\u0026quot;Purchaser\u0026quot; = agg_data$Party.Name,\r\u0026quot;Count\u0026quot; = agg_data$Denominations[,1],\r\u0026quot;Median\u0026quot; = agg_data$Denominations[,2],\r\u0026quot;Mean\u0026quot; = agg_data$Denominations[,3],\r\u0026quot;Sum\u0026quot; = agg_data$Denominations[,4])\rdatatable(agg_data_df, options = list(\rinitComplete = JS(\r\u0026quot;function(settings, json) {\u0026quot;,\r\u0026quot;$(this.api().table().header()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().container()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().body()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;}\u0026quot;)))%\u0026gt;% formatCurrency(c(\u0026#39;Median\u0026#39;, \u0026#39;Mean\u0026#39;, \u0026#39;Sum\u0026#39;),\rcurrency = \u0026quot;₹\u0026quot;,\rinterval = 3,\rmark = \u0026quot;,\u0026quot;,\rdigits = 0)\rCleaning the Donner data\rSimilar to the Party data, explore the Donner data from the pdf “Company.pdf” in the folder Data/Raw Data. Similar processing methodology is use as Party.pdf with one change this time total number of columns are 12.\nComp \u0026lt;- pdf_text(\u0026quot;./Data/Raw Data/Company.pdf\u0026quot;)\rComp \u0026lt;- unlist(str_split(Comp, \u0026quot;[\\\\r\\\\n]+\u0026quot;))\rComp \u0026lt;- str_trim(gsub(\u0026quot;,\u0026quot;,\u0026quot;\u0026quot;,Comp))\rBased on the data format, I found breaking it from as digit followed by space (?\u0026lt;=\\\\d ) is more useful.\nComp = as.data.frame(str_split_fixed(Comp, \u0026quot;(?\u0026lt;=\\\\d )\u0026quot;, n = 12))\rThis will break the string the string in 2 columns, “SN”, and “Reference No URN”\na \u0026lt;- gsub(\u0026quot;\\\\s+\u0026quot;, \u0026quot; \u0026quot;, Comp$V1, perl = TRUE)\rIndex = a %in% c(a[1:3])\rComp \u0026lt;- Comp[!Index,]\rIndex2 = is.na(str_extract(Comp$V1, \u0026quot;Page.\u0026quot;))\rComp \u0026lt;- Comp[Index2,]\rComp \u0026lt;- Comp[!apply(Comp == \u0026quot;\u0026quot;, 1, all),]\rrm(Index, Index2, a)\rNow we will break down column V6 that has structure of\n“A B C INDIA LIMITED TL 11448” First we will crop Bond numbers using str_sub with 1 to -6 (6th character from the last) save in c. After that Names are extracted using c removing last to charcter that are Prefix of the Unitq bond number. At last Prefix are extraced using str_sub(c, -2, -1).\nBondN \u0026lt;- as.numeric(str_sub(str_trim(Comp$V6), -6,-1))\r# you cans simply choose last 4 digit of the V6\rc = str_trim(str_sub(str_trim(Comp$V6),1,-6))\rName \u0026lt;- str_trim(str_sub(c,1,-3))\rPrefix \u0026lt;- str_sub(c, -2, -1)\rNow we will assemble the data in the total 12 column.\nComp_Data \u0026lt;- data.frame(\u0026quot;SN\u0026quot; = as.numeric(Comp$V1),\r\u0026quot;Reference No URN\u0026quot; = str_trim(Comp$V2),\r\u0026quot;Journal Date\u0026quot; = dmy(Comp$V3),\r\u0026quot;Date of Purchase\u0026quot; = dmy(Comp$V4),\r\u0026quot;Date of Expiry\u0026quot; = dmy(Comp$V5),\r\u0026quot;Purchaser\u0026quot; = Name,\r\u0026quot;Prefix\u0026quot; = Prefix,\r\u0026quot;Bond No\u0026quot; = BondN,\r\u0026quot;Denominations\u0026quot; = as.numeric(str_trim(Comp$V7)),\r\u0026quot;Issue Branch Code\u0026quot; = str_trim(Comp$V8),\r\u0026quot;Issue Teller\u0026quot; = as.numeric(str_trim(Comp$V9)),\r\u0026quot;Status\u0026quot; = str_trim(Comp$V10))\r# write.xlsx(Comp_Data, file=\u0026quot;Comp_Cleaned.xlsx\u0026quot;, row.names = FALSE)\rWe temporarily saved the files with “Comp_Cleaned.xlsx” and found there are some error in the columns, modify them here for one Donner.\nIndex = Comp_Data$Reference.No.URN %in% c(\u0026quot;00300202310100000003344\u0026quot;,\r\u0026quot;00300202310120000003422\u0026quot;,\r\u0026quot;00300202310130000003470\u0026quot;)\rComp_Data$Purchaser[Index] \u0026lt;- \u0026quot;L7 HITECH PRIVATE LIMITED\u0026quot;\rComp_Data$Denominations[Index] \u0026lt;- 10000000\rComp_Data$Issue.Branch.Code[Index] \u0026lt;- \u0026quot;00300\u0026quot;\rComp_Data$Prefix[Index] \u0026lt;- \u0026quot;OC\u0026quot;\rComp_Data$Status[Index] \u0026lt;- \u0026quot;Paid\u0026quot;\rComp_Data$Issue.Teller[Index] \u0026lt;- 1022034 # same value is used as it\u0026#39;s not much useful information\r## Adding Bond no. for all 3 cases\rComp_Data$Bond.No[Index] \u0026lt;- c(16524, 16531, 16521, 16535, 16529, 16527, 16523, 16519, 16533, 16569, 16577, 16565, 16567,16573, 16563, 16571, 16575, 16636,16638, 16644, 16642, 16640)\rwrite.xlsx(Comp_Data, file=\u0026quot;Comp_Cleaned.xlsx\u0026quot;, row.names = FALSE)\rprint(paste0(\u0026quot;No of NAs: \u0026quot;, sum(is.na(Comp_Data))))\rUpon inspection around 16 row have incorrect information, it’s because the parser function you used and they cleaning manually.\nNOTE: NA values are cleaned manually and renamed it to “Comp_Cleaned_Man.xlsx”\nNow we will load the cleaned data for tabular representation\nComp_Data \u0026lt;- read_xlsx(path = \u0026quot;./Data/Processed xlsx/Comp_Cleaned_man.xlsx\u0026quot;, sheet = \u0026quot;Sheet1\u0026quot;)\rComp_Data$unique_ID \u0026lt;- paste0(Comp_Data$Prefix,\rstr_pad(Comp_Data$Bond.No, width = 5,pad = \u0026quot;0\u0026quot;))\rfx_name \u0026lt;- function(x) {c(length(x), median(x), round(mean(x),0), sum(x))}\ragg_data = aggregate(Comp_Data, Denominations ~ Purchaser, FUN = fx_name)\ragg_data_df \u0026lt;- data.frame(\u0026quot;Purchaser\u0026quot; = agg_data$Purchaser,\r\u0026quot;Count\u0026quot; = agg_data$Denominations[,1],\r\u0026quot;Median\u0026quot; = agg_data$Denominations[,2],\r\u0026quot;Mean\u0026quot; = agg_data$Denominations[,3],\r\u0026quot;Sum\u0026quot; = agg_data$Denominations[,4])\rdatatable(agg_data_df, options = list(\rinitComplete = JS(\r\u0026quot;function(settings, json) {\u0026quot;,\r\u0026quot;$(this.api().table().header()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().container()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().body()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;}\u0026quot;)))%\u0026gt;% formatCurrency(c(\u0026#39;Median\u0026#39;, \u0026#39;Mean\u0026#39;, \u0026#39;Sum\u0026#39;),\rcurrency = \u0026quot;₹\u0026quot;,\rinterval = 3,\rmark = \u0026quot;,\u0026quot;,\rdigits = 0)\rProcessing the pdf in an easy way:\rUsing tabulizer package that can extract pdf tables, I find it’s slow but works well for table data. Similar to PDFtools it also required rJava.\n# remotes::install_github(c(\u0026quot;ropensci/tabulizerjars\u0026quot;, \u0026quot;ropensci/tabulizer\u0026quot;))\rlibrary(tabulizer)\rout_tables \u0026lt;- extract_tables(\u0026quot;./2024-03-25-Electoral-Bond-Analysis/Data/Raw Data/Company.pdf\u0026quot;)\rOut \u0026lt;- out_tables[[1]]\rfor (ii in 2:length(out_tables)) {\rtemp \u0026lt;- out_tables[[ii]]\rtemp1 \u0026lt;- temp[2:dim(temp)[1],] Out \u0026lt;- rbind(Out,temp1)\r}\rCombining Party and Donner data using UN_ID\rp_uid \u0026lt;- Party_Data$unique_ID # Unique ID for Party\rc_uid \u0026lt;- Comp_Data$unique_ID # Unique ID for Donner\r## Finding how many UID don\u0026#39;t match in both the dataset\rmissing_record_p_name = Party_Data$Party.Name[!p_uid %in% c_uid]\rtable(missing_record_p_name)\r## missing_record_p_name\r## AAM AADMI PARTY ## 2 ## ADYAKSHA SAMAJVADI PARTY ## 30 ## ALL INDIA ANNA DRAVIDA MUNNETRA KAZHAGAM ## 38 ## ALL INDIA TRINAMOOL CONGRESS ## 36 ## BHARAT RASHTRA SAMITHI ## 33 ## BHARATIYA JANATA PARTY ## 1145 ## BIHAR PRADESH JANTA DAL(UNITED) ## 2 ## DRAVIDA MUNNETRA KAZHAGAM (DMK) ## 7 ## JANATA DAL ( SECULAR ) ## 25 ## JHARKHAND MUKTI MORCHA ## 10 ## NATIONALIST CONGRESS PARTY MAHARASHTRA PRADESH ## 25 ## PRESIDENT, ALL INDIA CONGRESS COMMITTEE ## 238 ## RASHTRIYA JANTA DAL ## 10 ## SHIVSENA ## 36 ## TELUGU DESAM PARTY ## 19 ## YSR CONGRESS PARTY (YUVAJANA SRAMIKA RYTHU CONGRESS PARTY) ## 24\rNow, the total mission record by comparing Unique ID from Party and Company data/Donner data: 1680\nMismatch in our analysis 130. Remaining 1550 are not provided by SBI list.\nNow computing number of bonds are given to each party.\rBased on our analysis there are total 24 parties and 1318 Donner in the list.\nif(!require(janitor)){install.packages(\u0026quot;janiter\u0026quot;);library(janiter)}\rp_name \u0026lt;- unique(Party_Data$Party.Name)\rc_name \u0026lt;- unique(Comp_Data$Purchaser)\rComp_table \u0026lt;- matrix(NA, nrow = length(c_name), ncol = length(p_name))\rrownames(Comp_table) \u0026lt;- c_name colnames(Comp_table) \u0026lt;- p_name\rfor(ii in seq_along(p_name)){\rp_Ind \u0026lt;- Party_Data$unique_ID[Party_Data$Party.Name %in% p_name[ii]]\rc_Ind \u0026lt;- which(Comp_Data$unique_ID %in% p_Ind)\rdonner \u0026lt;- table(Comp_Data$Purchaser[c_Ind])\rfor (jj in seq_along(donner)) {\rinx \u0026lt;- which(c_name %in% names(donner)[jj])\rComp_table[inx,ii] \u0026lt;- as.numeric(donner[jj])\r}\r}\rNow the table:\nDT::datatable(as.data.frame(Comp_table), options = list(\rpageLength=25, scrollX=\u0026#39;400px\u0026#39;), filter = \u0026#39;top\u0026#39;)\rdatatable(as.data.frame(Comp_table), class = \u0026#39;cell-border stripe\u0026#39;, options = list(pageLength=5, scrollX=\u0026#39;400px\u0026#39;,\rinitComplete = JS(\r\u0026quot;function(settings, json) {\u0026quot;,\r\u0026quot;$(this.api().table().header()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().container()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().body()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;}\u0026quot;))) #write.csv(x = Comp_table, file = \u0026quot;Agg_Data.csv\u0026quot;)\r","permalink":"https://ankitdeshmukh.com/post/2024-03-25-electoral-bond-analysis/","summary":"SBI Submits all the data to ECI and ECI publish it on Thursday 21 Mar 2024, 6:32 PM Links of 2 files are provided here:\nDisclosure of Electoral Bonds\nDetails of Electoral Bonds submitted by SBI on 21st March 2024 (EB_Redemption_Details)\nDetails of Electoral Bonds submitted by SBI on 21st March 2024 (EB_Purchase_Details)\nTo know the background of the case read this: SBI submits all electoral bond details, including unique numbers, to ECI","title":"Data Cleaning with R using 'PDFtools' and 'stringr'"},{"content":"\rWhat is PostGIS and why PostGIS is benificial over tredtional analysis approaches.\rPostGIS is a spatial database extension for PostgreSQL that allows users to store and manipulate geospatial data. PostGIS has several advantages over the traditional approach of geospatial analysis, such as:\nPostGIS supports a wide range of spatial data types, functions, and operators, enabling complex spatial queries and operations.\rPostGIS integrates well with other GIS tools and frameworks, such as QGIS, GeoServer, and Leaflet, allowing users to visualize and analyze their data in different ways.\rPostGIS leverages the power and scalability of PostgreSQL, which is a robust, open-source, and widely used relational database management system.\rPostGIS enables spatial data analysis on large datasets, as it can handle millions of features and perform spatial joins and aggregations efficiently.\rPostGIS facilitates data sharing and collaboration, as it allows multiple users to access and modify the same spatial data concurrently.\rPostGIS for Professionals\rThe PostGIS roadmap is crucial for professionals in the field of GIS, Water resources, and Hydrology. To grow in the field of GIS expert you must to the certain taks on regular basis:\nMaster Core PostGIS Functions:\rEnsure a solid understanding of fundamental PostGIS functions for spatial data handling.\rProficiency in spatial queries, geometric operations, and indexing techniques is essential.\rDeepen Geospatial Database Skills:\rExpand your expertise in geospatial database management, including performance tuning and optimization strategies.\rFamiliarize yourself with advanced database concepts relevant to spatial data storage.\rIntegration with GIS Software:\rExplore integration possibilities with popular GIS software like QGIS and ArcGIS to enhance your interoperability skills.\rStay informed about evolving standards and best practices in the GIS industry.\rAdvanced Spatial Analytics:\rFocus on advanced spatial analytics using PostGIS, such as spatial regression analysis, network analysis, and 3D spatial operations.\rIncorporate machine learning algorithms into spatial analysis to address contemporary challenges in water resources and hydrology.\rOpen Source Contributions:\rConsider contributing to the PostGIS project or related open-source GIS projects. This enhances your visibility in the community and deepens your understanding of the system.\rNetworking and Professional Development:\rAttend conferences, workshops, and webinars focused on geospatial technologies and PostGIS.\rEngage with professionals in your field, both online and offline, to build a strong professional network.\rCertifications and Academic Collaborations:\rPursue relevant certifications in GIS and spatial databases to validate your expertise.\rCollaborate with academic institutions on research projects to stay at the forefront of advancements in hydrology, remote sensing, and GIS.\rBy systematically progressing through these steps, you’ll be a highly skilled professional in PostGIS, well-equipped to tackle complex challenges in water resources and hydrology, and ultimately increase your attractiveness for high-paying job opportunities.\nNow we will understand how to setup PostGIS with QGIS in a Windows machine.\nQuick installation\rInstall the PostgreSQL from the link 🔗\nInstall with default setting. Default port is 5432, change to something else if you have another version of PostgreSQL installed.\rYouTube link https://www.youtube.com/watch?v=IYHx0ovvxPs\rConnect a database with QGIS\rBasic of Structured Query Language (SQL)\rSQL stands for Structured Query Language and is a domain-specific language for managing data in relational databases. SQL was originally developed by IBM in the 1970s and later standardized by ANSI and ISO. SQL allows users to query, manipulate, and control data using keywords, clauses, expressions, and statements that resemble natural language.\nLearning the basics of SQL\rUse tutorial and data from: https://www.sqltutorial.org/sql-sample-database/\nCreating a SQL Sample Database\nThe following database diagram illustrates the HR sample database\rThis database has 7 tables, row numbers are shown in the table below\n| Table | Rows |\r----------------------\r| employees | 40 |\r| dependents | 30 |\r| departments | 11 |\r| jobs | 11 |\r| locations | 07 |\r| countries | 25 |\r| regions | 04 |\rThe following script creates the HR sample Database Structure in PostgreSQL\rCreating a Database:\rCREATE DATABASE test_db\rWITH\rOWNER = postgres\rENCODING = \u0026#39;UTF8\u0026#39;\rLOCALE_PROVIDER = \u0026#39;libc\u0026#39;\rCONNECTION LIMIT = -1\rIS_TEMPLATE = False;\rList all the database and select the one\rpostgres=# \\l\rpostgres=# \\c DATABASE_NAME\rRun the following to add PostGIS extension to Postgres\rCREATE EXTENSION postgis;\rCREATE EXTENSION postgis_raster;\rImport shapefile in PostGIS\rUse DB manager form QGIS\nImport raster in a database\rraster2pgsql -s [SRID] -I -M [raster data source] -F [schema.table_name] | psql -U [username] -d [database name] -p [port] -h [host]\rExample:\nraster2pgsql -s 4326 -I -M Your_file.tif -F | psql -U postgres -d post_gis_v1 -p 5432 -h localhost\rSELECT ST_X(ST_Centroid(geom)) AS long, ST_Y(ST_Centroid(geom)) AS lat FROM \u0026quot;ST_India-Dist\u0026quot;;\rSELECT ST_Centroid(geom) AS geom, gid, st_nm FROM \u0026quot;ST_India-Dist\u0026quot;;\r","permalink":"https://ankitdeshmukh.com/post/2024-01-08-postgis-with-qgis/","summary":"What is PostGIS and why PostGIS is benificial over tredtional analysis approaches.\rPostGIS is a spatial database extension for PostgreSQL that allows users to store and manipulate geospatial data. PostGIS has several advantages over the traditional approach of geospatial analysis, such as:\nPostGIS supports a wide range of spatial data types, functions, and operators, enabling complex spatial queries and operations.\rPostGIS integrates well with other GIS tools and frameworks, such as QGIS, GeoServer, and Leaflet, allowing users to visualize and analyze their data in different ways.","title":"PostGIS with QGIS"},{"content":"\rGetting started with gdal\rGDAL (Geospatial Data Abstraction Library) is a free and open source translator library for raster and vector geospatial data formats. It also comes with a variety of useful command line utilities for data translation and processing.\nGDAL is used by many GIS software packages, such as QGIS, ArcGIS, and GRASS GIS and R. It is also used in many scientific applications that require geospatial data processing, such as remote sensing, hydrology, and geology.\nIn R you can use GDAL with rgdal package. Several Packages are retiring for R See (https://r-spatial.org/r/2022/04/12/evolution.html)\ninstall.packages(\u0026quot;rgdal\u0026quot;, dependencies = TRUE)\rGDAL in QGIS\rThe GDAL library consists of a set of command line programs, each with a large list of options. Users comfortable with running commands from a terminal may prefer the command line, with access to the full set of options. The GDAL Tools plugin offers an easy interface to the tools, exposing only the most popular options.\nA summary of GDAL library capabilities\nExtensive Format Support: GDAL supports an extensive list of geospatial data formats, including GeoTIFF, NetCDF, shapefiles, and more. This diversity simplifies data access and manipulation.\nData Extraction: GDAL enables the extraction of specific data layers or subsets from geospatial datasets, which is particularly useful for researchers focused on specific regions or parameters within a dataset.\nReprojection and Transformation: Researchers often work with data in different projections. GDAL can reproject and transform data, ensuring it aligns spatially, which is vital in geospatial analysis.\nCommand-Line and GUI: GDAL offers both command-line and graphical user interfaces (such as QGIS), making it suitable for researchers with various preferences for interacting with geospatial data.\nPython and R Integration: GDAL provides bindings for popular programming languages like Python and R. This allows geospatial analysts to incorporate GDAL’s capabilities into their data analysis and modeling workflows, aligning with your preference for using R.\nGeospatial Image Processing: Beyond data conversion, GDAL provides capabilities for basic image processing, such as resampling, cropping, and filtering, which are essential for preparing data for further analysis.\nGDAL algorithm provider has major categories of operation in vector and raster analysis:\r1. Raster analysis\r2. Raster conversion\r3. Raster extraction\r4. Raster miscellaneous (merge, raster calculator, etc.)\r5. Raster projections\r6. Vector conversion\r7. Vector geoprocessing\r8. Vector miscellaneous (build virtual vector, SQL, etc.)\nWe will try to understand how we can use GDAL in R for most commonly used operation in geospatial analysis\nData Format Conversion: GDAL can convert data between various geospatial file formats. You can use the gdal_translate command to convert data from one format to another. For example, converting from GeoTIFF to Shapefile or vice versa.\nData Subset and Clipping: You can extract a specific region of interest from a larger dataset using the gdalwarp command. This is particularly useful for working with large raster datasets.\nResampling: GDAL allows you to change the resolution of raster data using the gdalwarp command. This can be helpful when merging or aligning datasets with different resolutions.\nMosaicking: You can merge multiple raster datasets into a single file using the gdal_merge.py utility. This is useful when you have data spread across multiple tiles or images.\nReprojection: Changing the coordinate system of a dataset is a common operation in geospatial analysis. GDAL’s gdalwarp allows you to reproject data to a different coordinate system.\nCreation of Virtual Raster (VRT): VRT files allow you to work with large datasets without actually copying or resampling the data. The gdalbuildvrt command can create VRT files, which are essentially virtual catalogs of your data.\nMetadata Extraction: You can extract metadata information from a geospatial dataset using the gdalinfo command. This includes information about the dataset’s spatial reference, geotransform, and more.\nRaster Calculator: The gdal_calc.py utility allows you to perform mathematical operations on raster datasets, enabling you to create derived products like difference maps or vegetation indices.\nData Warping and Reprojection: The gdalwarp utility can also be used to reproject and warp datasets, which is essential for ensuring that data aligns properly when conducting geospatial analyses.\nHistogram Analysis: GDAL provides tools for generating histograms of raster data, which can be valuable for understanding the distribution of values within a dataset. The gdal_hist utility can help with this.\nThese operations are fundamental to geospatial analysis and can be used in your research in the field of hydrology, remote sensing, and GIS. GDAL’s versatility and extensive command-line tools make it a valuable resource for working with geospatial data, especially when integrated with modern machine learning algorithms for drought analysis and geospatial data analysis using R.\n","permalink":"https://ankitdeshmukh.com/post/2023-10-24-gdal-an-introduction-rmd/","summary":"Getting started with gdal\rGDAL (Geospatial Data Abstraction Library) is a free and open source translator library for raster and vector geospatial data formats. It also comes with a variety of useful command line utilities for data translation and processing.\nGDAL is used by many GIS software packages, such as QGIS, ArcGIS, and GRASS GIS and R. It is also used in many scientific applications that require geospatial data processing, such as remote sensing, hydrology, and geology.","title":"Introduction of GDAL with R programming"},{"content":"\rHTML slides with Xaringan.\rXaringan is an R package for creating slideshows with remark.js through R Markdown.\n…from https://github.com/yihui/xaringan\nThe package name xaringan comes from Sharingan, a dōjutsu in Naruto with two abilities: the “Eye of Insight” and the “Eye of Hypnotism”. A presentation ninja should have these basic abilities, and I think remark.js may help you acquire these abilities, even if you are not a member of the Uchiha clan.\nInstalling Xaringan\rInstall the xaringan with CRAN or Github\ninstall.packages(\u0026#39;xaringan\u0026#39;)\r# or for latest version\rremotes::install_github(\u0026#39;yihui/xaringan\u0026#39;)\rSetting up the xaringan slides\r---\rtitle: \u0026quot;Title of my presentation\u0026quot;\rsubtitle: \u0026quot;Your subtitle\u0026quot;\rauthor: \u0026quot;**Dr. Ankit Deshmukh**\u0026quot;\rinstitute: \u0026quot;Affiliation\u0026quot;\rdate: \u0026quot;Week #: `r format(Sys.time(), \u0026#39;%d %B %Y\u0026#39;)`\u0026quot;\routput:\rxaringan::moon_reader:\rcss: [\u0026quot;css/default.css\u0026quot;, \u0026quot;css/metropolis.css\u0026quot;, \u0026quot;css/tachyons.min.css\u0026quot;]\rself_contained: false lib_dir: libs\rnature:\rhighlightStyle: solarized-light\rhighlightLines: true\rcountIncrementalSlides: false\rratio: 16:9\r---\rGlobal Setting for figures and code chunk\n```{r setup, include=FALSE}\roptions(htmltools.dir.version = FALSE)\rknitr::opts_chunk$set(\r#out.width = \u0026quot;100%\u0026quot;,\rcache = FALSE,\recho = TRUE,\rmessage = FALSE, warning = FALSE,\rfig.show = TRUE,\rhiline = TRUE,\rresults= \u0026quot;asis\u0026quot; # Useful to show bibliography as normal text.\r)\r```\rSetting up for Bibliography and Citation\r```{r setup, include=FALSE}\rlibrary(RefManageR)\rlibrary(bibtex)\rBibOptions(check.entries = FALSE, bib.style = \u0026quot;authoryear\u0026quot;, style = \u0026quot;text\u0026quot;, first.inits = FALSE)\rbib \u0026lt;- ReadBib(\u0026quot;~/adx/Bibliography.bib\u0026quot;) # A bibtex bibliography file. Use zotoro for this. ```\rUse xaringanExtra to enhance slide feature\ruse_logo for adding the logo in your slide.\ruse_progress_bar for progress bar.\ruse_extra_styles for code hover effect.\ruse_xaringan_extra(\"tile_view\") for see the preview of slides a once, it helps to jump on slides.\ruse_xaringan_extra(\"tachyons\") for an awesome miniature css with your slides. Find more here https://tachyons.io/#features\r```{r, echo=FALSE, include=TRUE}\rlibrary(xaringanExtra)\ruse_logo(image_url = \u0026quot;./css/Anix-Logo.png\u0026quot;, link_url = \u0026quot;https://www.ankitdeshmukh.com/\u0026quot;, width = \u0026quot;60px\u0026quot;, height = \u0026quot;60px\u0026quot;)\ruse_progress_bar(color = \u0026quot;#28282888\u0026quot;,location = \u0026quot;top\u0026quot;, height = \u0026quot;0.25em\u0026quot;)\ruse_extra_styles(hover_code_line = TRUE, mute_unhighlighted_code = FALSE)\ruse_xaringan_extra(c(\u0026quot;tile_view\u0026quot;, \u0026quot;tachyons\u0026quot;, \u0026quot;use_logo\u0026quot;, \u0026quot;use_progress_bar\u0026quot;))\r```\rOrganize all the image in images folder\ruse knitr of html+tachyons to add image\n```{r, include=TRUE, echo=FALSE, fig.align=\u0026#39;center\u0026#39;, out.width=\u0026quot;60%\u0026quot;}\rknitr::include_graphics(\u0026quot;images/YourImage.xyz\u0026quot;, error = FALSE)\r```\ror use html tags\r\u0026lt;img src=\u0026quot;images/YourImage.xyz\u0026quot; class=\u0026quot;w-60 br4 dib center\u0026quot;\u0026gt;\rCheckout the slides with Xaringan\rClick here to view in full screen\n","permalink":"https://ankitdeshmukh.com/post/2022-08-16-create-awesome-slides-with-xaringan/","summary":"HTML slides with Xaringan.\rXaringan is an R package for creating slideshows with remark.js through R Markdown.\n…from https://github.com/yihui/xaringan\nThe package name xaringan comes from Sharingan, a dōjutsu in Naruto with two abilities: the “Eye of Insight” and the “Eye of Hypnotism”. A presentation ninja should have these basic abilities, and I think remark.js may help you acquire these abilities, even if you are not a member of the Uchiha clan.","title":"Create awesome slides with Xaringan"},{"content":"\rThe main aim of this blog to show, how you can configure R, Python, and SQL in a single R-markdown file. Most of time we have to use data from databases and python code along with R functions, and having a setup that bring goodness of all the tool in one place comes really handy.\nSetup Python in Rstudio\rTo set up R Python And SQL in the Rstudio you have to first install miniconda. Miniconda helps to create python virtual environments and let you organize it.\rSuch as for, data cleaning you might use a data cleaning environment for web scraping you will use a web scraping environment and so on.\nCommands to manage conda environment inside RStudio:\rOnce you install miniconda You have to use R-reticulate in Rstudio, it is REPL (Read-eval-print loop) for Python in R, and helps to run Python code inside Rstudio. You can add miniconda into your windows terminal, but to use in Rstudio it is recommended to create and select environment from Rstudio itself.\nReticulate package provides all the necessary functionalities to run and manage python commands, such as\nload reticulate package with install.packages(reticulate) command in R console.\rcommand conda_ create() will create a new python environment.\ruse_condaenv(condaenv = \"ENVNAME\") to select a newly created environment or existing python environment\rto know the existing environments that you have created earlier, use conda_list() function.\ronce you select a environment now you are ready to install python packages with py_install(\"PackageName\").\rUse conda prompt\rI use conda prompt to create environment but some how those environment doesn’t work with my R-reticulate. But if you are using python only then you can configure conda with the following commands (run in system terminal):\nconda help for help.\rconda create --name your_environment to create a conda environment.\rconda env list to list all the environment in conda.\rconda activate your_environment to activate an specific conda environment.\rUse conda config to add other channels for package providers, but I tried pip and it worked just fine.\rconda config --add channels conda-forge (conda-forge is one of many channel)\rconda config --set channel_priority strict (set preference of channel)\rInstall a few packages: conda install pandas scikit-learn matplotlib\rHow to use SQL with R markdown\rUse dplyr, dbplyr, and RSQLite packages\nUse SQL block to view the query output in ln-line mode.\rA blogpot to learn R and SQL in Rstudio https://irene.rbind.io/post/using-sql-in-rstudio/\rExample of R, Python, and SQL with Rmarkdonw (*.Rmd)\rif(!require(dplyr)){install.packages(\u0026quot;dplyr\u0026quot;);library(dplyr)}\rif(!require(dbplyr)){install.packages(\u0026quot;dbplyr\u0026quot;);library(dbplyr)}\rif(!require(RSQLite)){install.packages(\u0026quot;RSQLite\u0026quot;);library(RSQLite)}\rconn \u0026lt;- src_memdb() # create a SQLite database in memory\r# Similar way you can add other database connections see `?dbplyr`\rcon_iso \u0026lt;- conn$con\rcopy_to(conn, storms, # this is a dataset built into dplyr\roverwrite = TRUE)\rtbl(conn, sql(\u0026quot;SELECT * FROM storms LIMIT 5\u0026quot;))\r## # Source: SQL [5 x 13]\r## # Database: sqlite 3.38.5 [:memory:]\r## name year month day hour lat long status category wind pressure\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 Amy 1975 6 27 0 27.5 -79 tropical de… -1 25 1013\r## 2 Amy 1975 6 27 6 28.5 -79 tropical de… -1 25 1013\r## 3 Amy 1975 6 27 12 29.5 -79 tropical de… -1 25 1013\r## 4 Amy 1975 6 27 18 30.5 -79 tropical de… -1 25 1013\r## 5 Amy 1975 6 28 0 31.5 -78.8 tropical de… -1 25 1012\r## # … with 2 more variables: tropicalstorm_force_diameter \u0026lt;int\u0026gt;,\r## # hurricane_force_diameter \u0026lt;int\u0026gt;\rNow create SLQ code chunk to directly run SQL queries from R.\n```{sql connection=con_iso}\rSELECT * FROM storms LIMIT 5;\r```\rThis will print 5 entire form database table storms\nTable 1: 5 records\rname\ryear\rmonth\rday\rhour\rlat\rlong\rstatus\rcategory\rwind\rpressure\rtropicalstorm_force_diameter\rhurricane_force_diameter\rAmy\r1975\r6\r27\r0\r27.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r27\r6\r28.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r27\r12\r29.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r27\r18\r30.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r28\r0\r31.5\r-78.8\rtropical depression\r-1\r25\r1012\rNA\rNA\rSQL code chunk can ouput the data as r-variable i.e. storm_preview\n```{sql connection=con_iso, output.var=\u0026quot;storm_preview\u0026quot;}\rSELECT * FROM storms LIMIT 5;\r```\r```{r}\rclass(storm_preview)\rstorm_preview ```\r## [1] \u0026quot;data.frame\u0026quot;\r## name year month day hour lat long status category wind\r## 1 Amy 1975 6 27 0 27.5 -79.0 tropical depression -1 25\r## 2 Amy 1975 6 27 6 28.5 -79.0 tropical depression -1 25\r## 3 Amy 1975 6 27 12 29.5 -79.0 tropical depression -1 25\r## 4 Amy 1975 6 27 18 30.5 -79.0 tropical depression -1 25\r## 5 Amy 1975 6 28 0 31.5 -78.8 tropical depression -1 25\r## pressure tropicalstorm_force_diameter hurricane_force_diameter\r## 1 1013 NA NA\r## 2 1013 NA NA\r## 3 1013 NA NA\r## 4 1013 NA NA\r## 5 1012 NA NA\rNow, load reticulate to run python codes\n```{r}\r# Below code will check if `reticulate` is installed or not, if not then it will install and load in the R-session. if(!require(reticulate)){install.packages(\u0026quot;reticulate\u0026quot;);library(reticulate)}\r```\rA example of Python code inside R markdown.\n```{python}\rfrom matplotlib import pyplot as plt # Importing Numpy Library import numpy as np plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) mu = 50 sigma = 7 x = np.random.normal(mu, sigma, size=200) fig, ax = plt.subplots() ax.hist(x, 20) ax.set_title(\u0026#39;Historgram\u0026#39;) ax.set_xlabel(\u0026#39;bin range\u0026#39;) ax.set_ylabel(\u0026#39;frequency\u0026#39;) fig.tight_layout() plt.show() # Comment out if you are using blogdown-sites to render the site. ```\r## (array([ 2., 1., 2., 8., 9., 12., 22., 25., 31., 24., 15., 15., 13.,\r## 9., 7., 2., 1., 0., 1., 1.]), array([31.4398407 , 33.56751485, 35.695189 , 37.82286315, 39.95053731,\r## 42.07821146, 44.20588561, 46.33355976, 48.46123392, 50.58890807,\r## 52.71658222, 54.84425638, 56.97193053, 59.09960468, 61.22727883,\r## 63.35495299, 65.48262714, 67.61030129, 69.73797544, 71.8656496 ,\r## 73.99332375]), \u0026lt;BarContainer object of 20 artists\u0026gt;)\rFigure 1: A figure python output of Histogram plot.\r","permalink":"https://ankitdeshmukh.com/post/2022-07-04-r-python-sql-in-rstudio/","summary":"The main aim of this blog to show, how you can configure R, Python, and SQL in a single R-markdown file. Most of time we have to use data from databases and python code along with R functions, and having a setup that bring goodness of all the tool in one place comes really handy.\nSetup Python in Rstudio\rTo set up R Python And SQL in the Rstudio you have to first install miniconda.","title":"Setting up R, Python, and SQL in RStudio"},{"content":"\rThe following packages are required for the random forest\nif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)}\rif(!require(janitor)){install.packages(\u0026quot;janitor\u0026quot;);library(janitor)} # for rename\rif(!require(randomForest)){install.packages(\u0026quot;randomForest\u0026quot;);library(randomForest)}\rif(!require(caret)){install.packages(\u0026quot;caret\u0026quot;);library(caret)} # for `confustionMatrix`\rA Random forest is made of Random Trees\rData \u0026lt;- read_csv(file = here::here(\u0026quot;content/post/2022-06-26-random-forest\u0026quot;, \u0026quot;german_credit.csv\u0026quot;))\rExploring the dataset\rData \u0026lt;- clean_names(Data)\rData$creditability \u0026lt;- as.factor(Data$creditability)\rglimpse(Data)\r## Rows: 1,000\r## Columns: 21\r## $ creditability \u0026lt;fct\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ account_balance \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 4, 2, 1, 1, …\r## $ duration_of_credit_month \u0026lt;dbl\u0026gt; 18, 9, 12, 12, 12, 10, 8, 6, 18, 24,…\r## $ payment_status_of_previous_credit \u0026lt;dbl\u0026gt; 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, …\r## $ purpose \u0026lt;dbl\u0026gt; 2, 0, 9, 0, 0, 0, 0, 0, 3, 3, 0, 1, …\r## $ credit_amount \u0026lt;dbl\u0026gt; 1049, 2799, 841, 2122, 2171, 2241, 3…\r## $ value_savings_stocks \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 2, …\r## $ length_of_current_employment \u0026lt;dbl\u0026gt; 2, 3, 4, 3, 3, 2, 4, 2, 1, 1, 3, 4, …\r## $ instalment_per_cent \u0026lt;dbl\u0026gt; 4, 2, 2, 3, 4, 1, 1, 2, 4, 1, 2, 1, …\r## $ sex_marital_status \u0026lt;dbl\u0026gt; 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 4, …\r## $ guarantors \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ duration_in_current_address \u0026lt;dbl\u0026gt; 4, 2, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, …\r## $ most_valuable_available_asset \u0026lt;dbl\u0026gt; 2, 1, 1, 1, 2, 1, 1, 1, 3, 4, 1, 3, …\r## $ age_years \u0026lt;dbl\u0026gt; 21, 36, 23, 39, 38, 48, 39, 40, 65, …\r## $ concurrent_credits \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, …\r## $ type_of_apartment \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, …\r## $ no_of_credits_at_this_bank \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, …\r## $ occupation \u0026lt;dbl\u0026gt; 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, …\r## $ no_of_dependents \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, …\r## $ telephone \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ foreign_worker \u0026lt;dbl\u0026gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, …\rAssess the creditabiliy with the help of other variables\r# code -------------------------------------------------------------------------\rggplot(data = Data, aes(x = age_years, color = creditability, fill = creditability)) +\rgeom_histogram(binwidth = 5, position = \u0026quot;identity\u0026quot;, alpha = 0.4) +\rscale_x_continuous(breaks = scales::pretty_breaks(n = 6)) +\rscale_y_continuous(breaks = scales::pretty_breaks(n = 6)) + theme_minimal()\r# Create a training and testing data\rset.seed(7791)\rpartitioning \u0026lt;- sample(2, nrow(Data), replace = TRUE, prob = c(0.8, 0.2))\rtable(partitioning)\r## partitioning\r## 1 2 ## 797 203\rtrain \u0026lt;- Data[partitioning == 1, ]\rtable(train$creditability)\r## ## 0 1 ## 244 553\rtest \u0026lt;- Data[partitioning == 2, ]\rtable(test$creditability)\r## ## 0 1 ## 56 147\rTrain the Random forest model\r# Generate random forest with train data\rrf_model \u0026lt;- randomForest(formula = creditability ~ ., data = train)\rpredict_train \u0026lt;- predict(rf_model, train)\rconfusionMatrix(predict_train, train$creditability)\r## Confusion Matrix and Statistics\r## ## Reference\r## Prediction 0 1\r## 0 244 0\r## 1 0 553\r## ## Accuracy : 1 ## 95% CI : (0.9954, 1)\r## No Information Rate : 0.6939 ## P-Value [Acc \u0026gt; NIR] : \u0026lt; 2.2e-16 ## ## Kappa : 1 ## ## Mcnemar\u0026#39;s Test P-Value : NA ## ## Sensitivity : 1.0000 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 1.0000 ## Prevalence : 0.3061 ## Detection Rate : 0.3061 ## Detection Prevalence : 0.3061 ## Balanced Accuracy : 1.0000 ## ## \u0026#39;Positive\u0026#39; Class : 0 ## Testing the model rf_model on test data\rpredict_test \u0026lt;- predict(rf_model, test)\rconfusionMatrix(predict_test, test$creditability)\r## Confusion Matrix and Statistics\r## ## Reference\r## Prediction 0 1\r## 0 28 12\r## 1 28 135\r## ## Accuracy : 0.803 ## 95% CI : (0.7415, 0.8553)\r## No Information Rate : 0.7241 ## P-Value [Acc \u0026gt; NIR] : 0.006157 ## ## Kappa : 0.459 ## ## Mcnemar\u0026#39;s Test P-Value : 0.017706 ## ## Sensitivity : 0.5000 ## Specificity : 0.9184 ## Pos Pred Value : 0.7000 ## Neg Pred Value : 0.8282 ## Prevalence : 0.2759 ## Detection Rate : 0.1379 ## Detection Prevalence : 0.1970 ## Balanced Accuracy : 0.7092 ## ## \u0026#39;Positive\u0026#39; Class : 0 ## # Reference\r# Prediction 0 1\r# 0 29 14\r# 1 27 133\rvarImpPlot(rf_model)\rOptimize the performance of randomforest.\rplot(rf_model) # black line is out of bag error.\roob_error \u0026lt;- double(20)\rfor (mtry in 1:20) {\rrf \u0026lt;- randomForest(formula = creditability ~ ., data = train, mtry = mtry, ntree = 166)\roob_error[mtry] \u0026lt;- rf$err.rate[166]\r}\rplot(1:20, oob_error, type = \u0026quot;b\u0026quot;, xlab = \u0026quot;Number of variable considered\u0026quot;, ylab = \u0026quot;Out of bag erro [-]\u0026quot;, xaxt = \u0026quot;n\u0026quot;)\raxis(1, at = 1:20, labels = 1:20, cex = 0.8)\ropti_num_var = which.min(oob_error)\rRe running the random forest with optimum number of variables\rrf_optim \u0026lt;- randomForest(formula = creditability ~ ., data = train, mtry = opti_num_var, ntree = 166)\r# Testing the model `rf_model` on test data\rconfusionMatrix(predict(rf_optim, test), test$creditability)\r## Confusion Matrix and Statistics\r## ## Reference\r## Prediction 0 1\r## 0 33 20\r## 1 23 127\r## ## Accuracy : 0.7882 ## 95% CI : (0.7255, 0.8423)\r## No Information Rate : 0.7241 ## P-Value [Acc \u0026gt; NIR] : 0.02266 ## ## Kappa : 0.4609 ## ## Mcnemar\u0026#39;s Test P-Value : 0.76037 ## ## Sensitivity : 0.5893 ## Specificity : 0.8639 ## Pos Pred Value : 0.6226 ## Neg Pred Value : 0.8467 ## Prevalence : 0.2759 ## Detection Rate : 0.1626 ## Detection Prevalence : 0.2611 ## Balanced Accuracy : 0.7266 ## ## \u0026#39;Positive\u0026#39; Class : 0 ## Exploring useful variables\rtrain \u0026lt;- as.data.frame(train)\rvarImpPlot(rf_model)\rimportance(rf_model)\r## MeanDecreaseGini\r## account_balance 36.543019\r## duration_of_credit_month 34.072986\r## payment_status_of_previous_credit 18.303982\r## purpose 20.837156\r## credit_amount 46.208519\r## value_savings_stocks 18.564843\r## length_of_current_employment 18.296281\r## instalment_per_cent 13.122638\r## sex_marital_status 12.781397\r## guarantors 7.055415\r## duration_in_current_address 13.666627\r## most_valuable_available_asset 14.340330\r## age_years 33.451374\r## concurrent_credits 8.064289\r## type_of_apartment 8.821601\r## no_of_credits_at_this_bank 7.306201\r## occupation 10.567355\r## no_of_dependents 4.325819\r## telephone 6.359517\r## foreign_worker 1.666973\r# How variable affect the chance of getting loan.\rpartialPlot(rf_model, train, account_balance, \u0026quot;1\u0026quot;)\rpartialPlot(rf_model, train, age_years, \u0026quot;1\u0026quot;)\rThis analysis for discrete variable of creditability.\n","permalink":"https://ankitdeshmukh.com/post/2022-06-26-random-forest/","summary":"The following packages are required for the random forest\nif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)}\rif(!require(janitor)){install.packages(\u0026quot;janitor\u0026quot;);library(janitor)} # for rename\rif(!require(randomForest)){install.packages(\u0026quot;randomForest\u0026quot;);library(randomForest)}\rif(!require(caret)){install.packages(\u0026quot;caret\u0026quot;);library(caret)} # for `confustionMatrix`\rA Random forest is made of Random Trees\rData \u0026lt;- read_csv(file = here::here(\u0026quot;content/post/2022-06-26-random-forest\u0026quot;, \u0026quot;german_credit.csv\u0026quot;))\rExploring the dataset\rData \u0026lt;- clean_names(Data)\rData$creditability \u0026lt;- as.factor(Data$creditability)\rglimpse(Data)\r## Rows: 1,000\r## Columns: 21\r## $ creditability \u0026lt;fct\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ account_balance \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 4, 2, 1, 1, …\r## $ duration_of_credit_month \u0026lt;dbl\u0026gt; 18, 9, 12, 12, 12, 10, 8, 6, 18, 24,…\r## $ payment_status_of_previous_credit \u0026lt;dbl\u0026gt; 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, …\r## $ purpose \u0026lt;dbl\u0026gt; 2, 0, 9, 0, 0, 0, 0, 0, 3, 3, 0, 1, …\r## $ credit_amount \u0026lt;dbl\u0026gt; 1049, 2799, 841, 2122, 2171, 2241, 3…\r## $ value_savings_stocks \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 2, …\r## $ length_of_current_employment \u0026lt;dbl\u0026gt; 2, 3, 4, 3, 3, 2, 4, 2, 1, 1, 3, 4, …\r## $ instalment_per_cent \u0026lt;dbl\u0026gt; 4, 2, 2, 3, 4, 1, 1, 2, 4, 1, 2, 1, …\r## $ sex_marital_status \u0026lt;dbl\u0026gt; 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 4, …\r## $ guarantors \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ duration_in_current_address \u0026lt;dbl\u0026gt; 4, 2, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, …\r## $ most_valuable_available_asset \u0026lt;dbl\u0026gt; 2, 1, 1, 1, 2, 1, 1, 1, 3, 4, 1, 3, …\r## $ age_years \u0026lt;dbl\u0026gt; 21, 36, 23, 39, 38, 48, 39, 40, 65, …\r## $ concurrent_credits \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, …\r## $ type_of_apartment \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, …\r## $ no_of_credits_at_this_bank \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, …\r## $ occupation \u0026lt;dbl\u0026gt; 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, …\r## $ no_of_dependents \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, …\r## $ telephone \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ foreign_worker \u0026lt;dbl\u0026gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, …\rAssess the creditabiliy with the help of other variables\r# code -------------------------------------------------------------------------\rggplot(data = Data, aes(x = age_years, color = creditability, fill = creditability)) +\rgeom_histogram(binwidth = 5, position = \u0026quot;identity\u0026quot;, alpha = 0.","title":"Random Forest with R-Programming"},{"content":"\rHow to make a better boxplot with custom fonts, let’s explore this in this post, it can be used for the standard template for boxplot with facet and user defined fonts.\nRequired R libraries:\rif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)} # for ggplot2 function\rif(!require(gapminder)){install.packages(\u0026quot;gapminder\u0026quot;);library(gapminder)} # for sample data\rif(!require(showtext)){install.packages(\u0026quot;showtext\u0026quot;);library(showtext)} # to import fonts\rAdd fonts in R session\rfont_add_google(\u0026quot;Karla\u0026quot;, \u0026quot;Karla\u0026quot;) # adding local font\rfont_add(family = \u0026quot;Helvetica\u0026quot;, regular = \u0026quot;C:/Windows/Fonts/Helvetica 400.ttf\u0026quot;)\r# Adding from Google fonts\rfont_add_google(\u0026quot;Roboto Slab\u0026quot;, \u0026quot;Roboto Slab\u0026quot;) # adding font from the web/google font\rfont_families()\r## [1] \u0026quot;sans\u0026quot; \u0026quot;serif\u0026quot; \u0026quot;mono\u0026quot; \u0026quot;wqy-microhei\u0026quot; \u0026quot;Karla\u0026quot; ## [6] \u0026quot;Helvetica\u0026quot; \u0026quot;Roboto Slab\u0026quot;\rDefine theme for Boxplot and fonts\rthemeBox \u0026lt;- function(base_family = \u0026quot;sans\u0026quot;, exFont, ...){\rtheme_bw(base_family = base_family, ...) +\rtheme(\rpanel.grid = element_blank(),\rplot.title = element_text(size = 8),\raxis.ticks.length = unit(-0.05, \u0026quot;in\u0026quot;),\raxis.text.y = element_text(margin=unit(c(0.3,0.3,0.3,0.3), \u0026quot;cm\u0026quot;)),\raxis.text.x = element_text(margin=unit(c(0.3,0.3,0.3,0.3), \u0026quot;cm\u0026quot;)),\raxis.ticks.x = element_blank(),\raspect.ratio = 1,\rlegend.background = element_rect(color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;),\rtext = element_text(family=exFont)\r)\r}\rPlot the boxplots of Average Life Expectancy\rggplot(gapminder, aes(x = continent, y = lifeExp, fill = continent)) +\rfacet_wrap(~year) +\rgeom_boxplot(linetype = \u0026quot;dashed\u0026quot;) +\rstat_boxplot(aes(ymin = ..lower.., ymax = ..upper..), outlier.shape = 1) +\rstat_boxplot(geom = \u0026quot;errorbar\u0026quot;, aes(ymin = ..ymax..)) +\rstat_boxplot(geom = \u0026quot;errorbar\u0026quot;, aes(ymax = ..ymin..)) +\rscale_y_continuous(name = \u0026quot;Average Life Expectancy\u0026quot;) +\rscale_x_discrete(labels = abbreviate, name = \u0026quot;Continent\u0026quot;) + themeBox(exFont = \u0026quot;Karla\u0026quot;)\rLinks:\rBoxplot customization\r","permalink":"https://ankitdeshmukh.com/post/2022-01-21-pretty-boxplot-with-focet/","summary":"How to make a better boxplot with custom fonts, let’s explore this in this post, it can be used for the standard template for boxplot with facet and user defined fonts.\nRequired R libraries:\rif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)} # for ggplot2 function\rif(!require(gapminder)){install.packages(\u0026quot;gapminder\u0026quot;);library(gapminder)} # for sample data\rif(!require(showtext)){install.packages(\u0026quot;showtext\u0026quot;);library(showtext)} # to import fonts\rAdd fonts in R session\rfont_add_google(\u0026quot;Karla\u0026quot;, \u0026quot;Karla\u0026quot;) # adding local font\rfont_add(family = \u0026quot;Helvetica\u0026quot;, regular = \u0026quot;C:/Windows/Fonts/Helvetica 400.ttf\u0026quot;)\r# Adding from Google fonts\rfont_add_google(\u0026quot;Roboto Slab\u0026quot;, \u0026quot;Roboto Slab\u0026quot;) # adding font from the web/google font\rfont_families()\r## [1] \u0026quot;sans\u0026quot; \u0026quot;serif\u0026quot; \u0026quot;mono\u0026quot; \u0026quot;wqy-microhei\u0026quot; \u0026quot;Karla\u0026quot; ## [6] \u0026quot;Helvetica\u0026quot; \u0026quot;Roboto Slab\u0026quot;\rDefine theme for Boxplot and fonts\rthemeBox \u0026lt;- function(base_family = \u0026quot;sans\u0026quot;, exFont, .","title":"How to create a pretty facet-boxplot with custom fonts"},{"content":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nA few useful syntax are shown below in the post:\n1. You can embed an R code chunk like this:\rsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r2. Including Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\rFigure 1: A fancy pie chart.\rReferencing a image/figure.\r# Define the tag after lagnuage notation such as `pie`\r{r pie, fig.cap=\u0026#39;A fancy pie chart.\u0026#39;, tidy=FALSE}\rUse \\@ref(fig:pie) to refrence an image. 3. Inline R Code\rThere were `r nrow(cars)` cars studied\rThere were 50 cars studied.\n4. Use links in Rmarkdown\rUse a plain http address or add a link to a phrase:\nhttp://example.com\rlinked phrase\ryou can also use use a snippet predefined for Rmd file link (Shfit + Tab) to create a link.\n5. Use local images or image URLs.\r![Caption](http://example.com/logo.png)\r![optional caption text](figures/img.png)\r6. Tables in R markdown\rInsert table use knitr::kable() function. Applicable for any 2D rectangular data(Data Frame, Matrix, etc.).\rknitr::kable(head(iris[, 1:3]), \u0026quot;pipe\u0026quot;)\rSepal.Length\rSepal.Width\rPetal.Length\r5.1\r3.5\r1.4\r4.9\r3.0\r1.4\r4.7\r3.2\r1.3\r4.6\r3.1\r1.5\r5.0\r3.6\r1.4\r5.4\r3.9\r1.7\rOr create table with traditional markdown way.\rmpg\rcyl\rdisp\rhp\rMazda RX4\r21.0\r6\r160\r110\rMazda RX4 Wag\r21.0\r6\r160\r110\rDatsun 710\r22.8\r4\r108\r93\rHornet 4 Drive\r21.4\r6\r258\r110\rHornet Sportabout\r18.7\r8\r360\r175\rValiant\r18.1\r6\r225\r105\r7. LaTeX Equations\rInline equation:\r$equation$\r\\(\\int\\limits_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx = \\sqrt{\\pi}\\)\nDisplay equation:\r$$equation$$\r\\[\\int\\limits_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx = \\sqrt{\\pi}\\]\n8. Sub/Super scripts and others\rsuperscript^2^ subscript~2~ ~~strikethrough~~\r\u0026lt;mark\u0026gt;This is a highlighted text\u0026lt;/mark\u0026gt;\rsuperscript2\nsubscript2\nstrikethrough\nThis is a highlighted text\n9. Use Bibliography\rCitation in sentence @R-base or after the sentence [@casella2002statistical] and one more [-@king1974nonoperative]\rCitation in sentence Brutsaert (2005) or after the sentence (Brath and Jonker 2015) and one more (2022)\nThis will automatically add the bibliography at the end of the documents. For more information see this\n10. Links and footnotes\rA sample phrase^[This is a footnote; bottom of the page]\rA sample phrase1\nReference\rBrath, Richard, and David Jonker. 2015. Graph Analysis and Visualization: Discovering Business Opportunity in Linked Data. Indianapolis, Ind: Wiley.\rBrutsaert, Wilfried. 2005. Hydrology: An Introduction. Cambridge ; New York: Cambridge University Press.\rChen, Chen, Jiange Jiang, Zhan Liao, Yang Zhou, Hao Wang, and Qingqi Pei. 2022. “A Short-Term Flood Prediction Based on Spatial Deep Learning Network: A Case Study for Xi County, China.” Journal of Hydrology 607 (April): 127535. https://doi.org/10.1016/j.jhydrol.2022.127535.\rThis is a footnote text; see in the bottom of the page.↩︎\n","permalink":"https://ankitdeshmukh.com/post/2021-12-01-r-markdown/","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nA few useful syntax are shown below in the post:\n1. You can embed an R code chunk like this:\rsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.","title":"Hello R Markdown!"},{"content":"\rR is a very versatile statistical tool and programming language. It’s my all-time good-to-go data analysis tool. It is fast, reliable and nifty. It provides great flexibility for my daily work and analysis tasks. If one uses R, they could consider RStudio as a more sophisticated GUI than the Base R once.\n1. Installing R\rStep 01: Install R-Binaries from R-Project\nStep 02: Install RStudio IDE from RStudio\n2. Customizing RStudio\rA lot of ways you can make RStudio more useful for your personal use. Like updating your ‘.RProfile’, adding an awesome theme, and fonts that support ligatures.\nUsing Rstudio as R-IDE: R studio has multiple windows but the most important are Code editor, console, Environment variable pane, and plot output pane.\rFigure 1: R-Studio IDE have many pans.\rCode editor: You will write your code in this window. R used # as the comment character. To assign a variable to a value we use ← (lowercase followed by a dash).\r2.1 Customize R startup with “.Rprofile”\rIf you are using a Windows machine you can find the location of your ‘.Rprofile’ at ‘C:/Users/UserName/Documents’. The code block is shown below. This is what my ‘.Rprofile’ looks like. Several costume functions can be added here and they will load with R startup every time.\ncat(\u0026quot;\\014\u0026quot;)\rcat(\u0026quot;Hi Ankit! What are we doing today?\\n\u0026quot;)\rFigure 2: A RStudio window\r2.2 Adding a theme\rI personally like the dark theme for my R studio. I particularly like to use the ‘Gruvbox’ theme that is not available in RStudio but can be downloaded from here. Download the file and paste it to 'C:\\Users\\UserName\\AppData\\Roaming\\RStudio\\themes'. To apply the theme in R studio go to Tools \u0026gt; Global Option \u0026gt; Appearance and select editor theme as gruvbox.\nFigure 3: RSudio configuration windows\r2.3 Adding fonts that support ligature\rThis helps to read and understand code faster and efficiently, mostly the merged common occurring 2 characters to one for easy reading but this is just a font rendering feature it means the underlying code remains ASCII-compatible [Source].\nFigure 4: Font Ligature makes code more asthetic pleasing and readable\rFira Code is a free monospaced font containing ligatures for common programming multi-character combinations.\nFigure 5: Fira Font with different themes\rDownload and FiraCode font from Here, and Install on your machine for all users. To apply the theme in R studio go to Tools \u0026gt; Global Option \u0026gt; Appearance and select editor font as Fira Code.\n3. Customize R with code snippets\rThe snippet is a re-usable piece of code or text. Ordinarily, these are formally defined operative units to incorporate into larger programming modules. To repeat a few operations and formats you can use snippets in R. To edit or add snippets in RStudio go to Tools \u0026gt; Global Option \u0026gt; Code \u0026gt; Editing, now enable Snippets and click edit snippets.\nFigure 6: R Snippets for quick code chunks.\rFrom the edit snippets window you can manage snippets of R. Mostly I use R and R markdown snippets in my daily use. Few useful snippets are:\nsnippet cls\rgraphics.off(); rm(list = ls()); cat(\u0026quot;\\014\u0026quot;)\rsnippet rqr\rif(!require(${1:packageName})){install.packages(\u0026quot;${1:packageName}\u0026quot;);\rlibrary(${1:packageName})}\rsnippet fmt\r# Title :: ${1:File Title}------------------------------------------------\r# Author :: Ankit Deshmukh\r# DOC :: `r eval(Sys.Date())`\r# DOLE :: `r eval(Sys.time())`\r# Description :: ${2:File Description}\r# setup ------------------------------------------------------------------------\rgraphics.off(); cat(\u0026quot;\\014\u0026quot;)\rsetwd(\u0026quot;`r eval(getwd())`\u0026quot;)\r# libraries --------------------------------------------------------------------\rlibrary(tidyverse)\r# code -------------------------------------------------------------------------\rsnippet pp\r\u0026quot;`r gsub(\u0026quot;\\\\\\\\\u0026quot;, \u0026quot;/\u0026quot;, readClipboard())`\u0026quot;\rsnippet clear\rrm(list = ls()); graphics.off();cat(\u0026quot;\\014\u0026quot;)\rTo use the snippet use the keywords such as clear and press Shift + Tab to auto complete the snippet text.\n","permalink":"https://ankitdeshmukh.com/post/2021-09-20-getting-started-with-r/","summary":"R is a very versatile statistical tool and programming language. It’s my all-time good-to-go data analysis tool. It is fast, reliable and nifty. It provides great flexibility for my daily work and analysis tasks. If one uses R, they could consider RStudio as a more sophisticated GUI than the Base R once.\n1. Installing R\rStep 01: Install R-Binaries from R-Project\nStep 02: Install RStudio IDE from RStudio\n2. Customizing RStudio\rA lot of ways you can make RStudio more useful for your personal use.","title":"Getting Started with R Programming"}]