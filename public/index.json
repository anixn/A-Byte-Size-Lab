[{"content":"What is Hugo\u0026rsquo;s shortcode? A Hugo shortcode is a reusable and customizable template-like feature Shortcodes are written using Hugo\u0026rsquo;s templating language (based on Go templates). They are placed in the layouts/shortcodes directory of your Hugo project. Create a Shortcode File Save this as layouts/shortcodes/your_shortcode.html your-site/ ├── archetypes/ │ └── default.md ├── assets/ ├── content/ ├── data/ ├── i18n/ ├── layouts/ ├── └── shortcodes/ \u0026lt;-- your_shortcode.html ├── static/ ├── themes/ └── hugo.yaml A sample shortcode is here: \u0026lt;details style=\u0026#34;border: 1px solid #666; border-radius: 4px; padding: 10px; margin: 10px 0; box-shadow: 3px 3px 5px #8989e420;\u0026#34;\u0026gt; \u0026lt;summary style=\u0026#34;font-weight: normal; cursor: pointer; margin-bottom: 5px;\u0026#34;\u0026gt; {{ .Get \u0026#34;title\u0026#34; | default \u0026#34;Click to expand\u0026#34; | markdownify }} \u0026lt;/summary\u0026gt; \u0026lt;div style=\u0026#34;margin-top: 10px;\u0026#34;\u0026gt; {{ .Inner | markdownify }} \u0026lt;/div\u0026gt; \u0026lt;/details\u0026gt; Shortcode for collapse section Now let\u0026rsquo;s use the shortcode details\n{{\u0026lt; details title=\u0026#34;Sample Text *Learn More...*\u0026#34; \u0026gt;}} \u0026gt; Collapsed text {{\u0026lt; /details \u0026gt;}} will produce the collapsable field\nSample Text Learn More\u0026hellip;\rCollapsed text\nCallouts on my site I also, found out and edited the osbidian style callout. Currently it supports only important, tip, warning, and info.\n{{\u0026lt; box important \u0026gt;}} **This is a bold line** Lorem ipsum dolor sit amet consectetur adipisicing elit. Nam, omnis aliquam... {{\u0026lt; /box \u0026gt;}} This is the output you will get: This is a bold line\nLorem ipsum dolor sit amet consectetur adipisicing elit. Nam, omnis aliquam\u0026hellip;\nSimilarly, you can create for others.\nThis is a bold line\nLorem ipsum dolor sit amet consectetur adipisicing elit. Nam, omnis aliquam\u0026hellip;\nThis is a bold line\nLorem ipsum dolor sit amet consectetur adipisicing elit. Nam, omnis aliquam\u0026hellip;\nThis is a bold line\nLorem ipsum dolor sit amet consectetur adipisicing elit. Nam, omnis aliquam\u0026hellip;\nYouTube shortcode We can embed YouTube videos with the inbuilt YouTube shortcode\n{{\u0026lt; youtube video_id \u0026gt;}}\nThe YouTube Video ID is the string of characters that appears after v= in the URL. See the video ID from the following URL\nhttps://www.youtube.com/watch?v=pp6NX5lyx54\r{{\u0026lt; youtube pp6NX5lyx54 \u0026gt;}} Add quotes with Hugo \u0026lt;blockquote class=\u0026#34;custom-quote\u0026#34;\u0026gt; \u0026lt;p\u0026gt;{{ .Inner }}\u0026lt;/p\u0026gt; {{ if .Get \u0026#34;author\u0026#34; }} \u0026lt;footer\u0026gt;— {{ .Get \u0026#34;author\u0026#34; }}\u0026lt;/footer\u0026gt; {{ end }} \u0026lt;/blockquote\u0026gt; \u0026lt;style\u0026gt; .custom-quote { font-style: italic; border-left: 4px solid #ccc; padding-left: 1em; margin: 1.5em 0; color: #555; } .custom-quote footer { margin-top: 0.5em; font-size: 0.9em; color: #888; } \u0026lt;/style\u0026gt; To use the quote shortcode do the following.\n{{\u0026lt; quote author=\u0026#34;Albert Einstein\u0026#34; \u0026gt;}} Imagination is more important than knowledge. {{\u0026lt; /quote \u0026gt;}} Imagination is more important than knowledge.\r— Albert Einstein\rI will keep updating new shortcodes and tips \u0026hellip;\n","permalink":"https://ankitdeshmukh.com/post/2025-01-04-use-hugo-shortcodes/","summary":"What is Hugo\u0026rsquo;s shortcode? A Hugo shortcode is a reusable and customizable template-like feature Shortcodes are written using Hugo\u0026rsquo;s templating language (based on Go templates). They are placed in the layouts/shortcodes directory of your Hugo project. Create a Shortcode File Save this as layouts/shortcodes/your_shortcode.html your-site/ ├── archetypes/ │ └── default.md ├── assets/ ├── content/ ├── data/ ├── i18n/ ├── layouts/ ├── └── shortcodes/ \u0026lt;-- your_shortcode.html ├── static/ ├── themes/ └── hugo.","title":"Customizing 'Hugo' site with shortcodes - callouts, collapsable, and more"},{"content":"\rClimatic Data in Hydrological Analysis\rClimatic data such as temperature and precipitation are essential for any hydrological analysis. Usually, several years of data are required to do any type of hydrological analysis. The climatic data is obtained from either satellite-based observation or in situ observation.\nFor hydrological analysis historical precipitation and temperature data is essential. Usually these data can be obtained from Indian Meterolgocal Department (भारतीय मौसम विज्ञान विभाग:IMD) free of cost.\nIMD pune provide the gridded precipitation and Temperature data since 1901. Most of time I find many students are struggling to process the data that this either in NetCDF (ext: “.nc”), or Binary data (ext: “.grd”). The processign of both the data is a little different.\nData source, specifications, and metadata.\rTo process any data the most important step is metadata. Metadata for temperature and precipitation is shown:\nRainfall Data:\rThe rainfall data product is the resolution of daily gridded rainfall data (0.25 x 0.25 degrees). The unit of rainfall is in millimeters (mm). Data available for 122 years, 1901 to 2023. Data is arranged in 135x129 grid points. The first data in the record is at 6.5N \u0026amp; 66.5E, the second is at 6.5N \u0026amp; 66.75E, and so on. The last data record corresponds to 38.5N \u0026amp; 100.0E. The yearly data file consists of 365/366 records corresponding to nonleap/ leap years. You can download it from https://imdpune.gov.in/cmpg/Griddata/Rainfall_25_NetCDF.html\nTemperature data (min/max)\rIMD High resolution \\(1\\times1\\) degree gridded daily temperature data (1951-2018). This data is arranged in 31x31 grid points. Lat 7.5N, 8.5N … 36.5, 37.5 (31 Values). Long 67.5E, 68.5E … 96.5, 97.5 (31 Values). For leap years, data for 366 days are included. The unit of temperature is in Celsius.\rGridded data for the years 2008 and onwards are based on a relatively small number of stations (around 180) for which data were received operationally on real time basis.\nReference paper for more detail\rPai et al. (2014). Pai D.S., Latha Sridhar, Rajeevan M., Sreejith O.P., Satbhai N.S. and Mukhopadhyay B., 2014:\rDevelopment of a new high spatial resolution (0.25° X 0.25°)Long period (1901-2010) daily gridded rainfall data set over.\rIndia and its comparison with existing data sets over the region; MAUSAM, 65, 1(January 2014), pp1-18.\rProcessing ‘NetCDF’\rNetCDF (Network Common Data Form) is a widely used data format designed for storing and sharing scientific data in a structured, self-describing, and platform-independent manner. Developed by Unidata, NetCDF is primarily used in geosciences, such as atmospheric science, hydrology, oceanography, and climate modeling, but can be applied to various other fields requiring efficient storage of multidimensional data.\nStructure of a NetCDF File:\rAs an example, I am showing the metadata of the IMD precipitation file.\nFile ./RF25_ind2023_rfp25.nc (NC_FORMAT_CLASSIC):\r1 variables (excluding dimension variables):\rdouble RAINFALL[LONGITUDE,LATITUDE,TIME]\rmissing_value: -999\r_FillValue: -999\rlong_name: Rainfall\runits: mm\rhistory: From ind2023_rfp25.grd\r2 dimensions:\rLONGITUDE Size:135\runits: degrees_east\rpoint_spacing: even\raxis: X\rmodulo: 360\rstandard_name: longitude\rLATITUDE Size:129\runits: degrees_north\rpoint_spacing: even\raxis: Y\rstandard_name: latitude\rTIME Size:365 *** is unlimited ***\runits: days since 1900-12-31\raxis: T\rcalendar: GREGORIAN\rtime_origin: 31-DEC-1900\rstandard_name: time\r3 global attributes:\rhistory: FERRET V7.5 (optimized) 6-Feb-24\rConventions: CF-1.6\rBased on the aforementioned NetCDF metadata, it consists of three main components:\nDimensions: These define the shape of the data, such as time, latitude, longitude, or vertical levels. A dimension can be used to describe how many values a variable contains in a specific direction or category.\rExample: If you have daily temperature data at different locations, the dimensions could be time (number of days), latitude, and longitude.\rtime: Unlimited, indicating that more time steps can be added without rewriting the file.\rlatitude: 180 values, representing geographic locations from the southern to the northern hemisphere.\rlongitude: 360 values, representing locations from west to east around the globe.\rVariables: These store the actual data. Variables can be scalar or multi-dimensional arrays (e.g., temperature, rainfall, or elevation data). Each variable is defined along one or more dimensions and can have associated metadata (attributes).\rExample: A variable could represent temperature (temp), with dimensions time, latitude, and longitude.\rrainfall: A 3D variable that depends on time, latitude, and longitude. It stores daily rainfall values and includes metadata (units = mm, long_name = Daily Rainfall) to describe the data.\rlatitude and longitude: 1D variables with values representing geographic coordinates in degrees.\rtime: A 1D variable containing time values since a reference date (2000-01-01). The calendar attribute specifies the type of calendar system.\rAttributes: These provide additional information about the file or variables, such as units, missing data values, or descriptive text. They are used to describe global properties (applied to the whole dataset) or specific variables.\rExample: An attribute might define the units of temperature as “degrees Celsius” or mark missing values with a specific value like -9999.\rMetadata about the dataset, such as the title, institution, and data source, which provide context and provenance information.\rProcessing NetCDF file\rNetCDF file has 3 dimension lat, lon, and time (day)\nLoading ncdf4 library to process the NetCDF file. Geospatial analysis is performed by terra package.\nif (!require(ncdf4)) { install.packages(\u0026#39;ncdf4\u0026#39;); library(ncdf4)}\rif (!require(terra)) { install.packages(\u0026#39;terra\u0026#39;); library(terra)}\rif (!require(tidyverse)) { install.packages(\u0026#39;tidyverse\u0026#39;); library(tidyverse)}\rRead shapefile with multiple polygons in it.\nshp \u0026lt;- vect(\u0026quot;./Shapefile/Krishna_subbasins.shp\u0026quot;)\rshp \u0026lt;- project(x = shp, y = \u0026quot;+proj=longlat +datum=WGS84 +no_defs\u0026quot;)\rplot(shp, bg = \u0026quot;gray\u0026quot;)\rsbar(d = 1000, type = \u0026quot;bar\u0026quot;, divs = 4, below = \u0026quot;km\u0026quot;) # Scale\rnorth() # for north arrow.\rrotate_clockwise \u0026lt;- function(x) {t(apply(x, 2, rev))}\rrotate_counter_clockwise \u0026lt;- function(x) {apply(t(x),2, rev)}\rSelect the year line\ryr \u0026lt;- 2015:2023\rrainfall_mat \u0026lt;- timestamp \u0026lt;- c()\rfor (yr_i in yr) {\rnc_file \u0026lt;- nc_open(paste0(\u0026quot;./RF25_ind\u0026quot;,yr_i,\u0026quot;_rfp25.nc\u0026quot;))\r# print(nc_file) # get info about the file\rnames(nc_file$var) # Variable i.e. Rainfall\rnames(nc_file$dim) # Dimenstions (Lat, Lon, time)\rlatitude \u0026lt;- nc_file$dim[[1]]$vals\rlongitude \u0026lt;- nc_file$dim[[2]]$vals\rtime \u0026lt;- nc_file$dim[[3]]$vals\rtime_as_date \u0026lt;- as.Date(time, origin = \u0026quot;1900-12-31\u0026quot;)\rrainfall \u0026lt;- ncvar_get(nc_file, varid = \u0026quot;RAINFALL\u0026quot;)\rr_mat \u0026lt;- matrix(NA, nrow = length(time), ncol = length(shp$OBJECTID))\rfor(d_i in seq_along(time)){\rprecip_day \u0026lt;- rainfall[, , d_i] %\u0026gt;% rotate_counter_clockwise() %\u0026gt;% rast()\rext(precip_day) \u0026lt;- c(min(latitude), max(latitude), min(longitude), max(longitude))\rcrs(precip_day) \u0026lt;- \u0026quot;+proj=longlat +datum=WGS84 +no_defs\u0026quot;\rr_day \u0026lt;- terra::extract(x = precip_day, y = shp, fun = mean)\rr_mat[d_i, ] \u0026lt;- r_day$lyr.1\rprint(paste(d_i, yr_i))\r}\rrainfall_mat \u0026lt;- rbind(rainfall_mat, r_mat)\rtimestamp \u0026lt;- append(timestamp, time_as_date)\r}\rProcess the output\rrainfall_df \u0026lt;- as.data.frame(rainfall_mat)\rrainfall_df \u0026lt;- cbind(timestamp, rainfall_df)\rwrite_csv(rainfall_df, file = \u0026quot;Rainfall.csv\u0026quot;)\rProcessing Binary Data ‘GRD’ files\rThe grd file are provided by IMD and reading it a little bit different. The following code demostrate the working with .grd.\nReading files and metadata\r# Working with R blog.\rif (!require(terra)) { install.packages(\u0026#39;terra\u0026#39;); library(terra)}\rf \u0026lt;- \u0026quot;Maxtemp_MaxT_2022.GRD\u0026quot;\r# -------------------------------------------------\r# Latitude: 7.5:37.5\r# Longitude:67.5:97.5\r# Resolution : 1 degree\r# Temperature units : Celcius\r# Missing data : 99.9\r# --------------------------------------------------\r# https://imdpune.gov.in/cmpg/Griddata/Max_1_Bin.html#\r# Get the metadata of the file the previous link\rlat \u0026lt;- 31\rlon \u0026lt;- 31\rnspatial \u0026lt;- lat * lon\rSetting up for loop through the daily data\rleap_year \u0026lt;- function(year) {return(ifelse((year %%4 == 0 \u0026amp; year %%100 != 0) | year %%400 == 0, 366, 365)) }\rdays = leap_year(as.numeric(substr(f, 14, 17)))\r# read all the data\rm_data \u0026lt;- readBin(f, what = \u0026quot;numeric\u0026quot;, size = 4, n = lat * lon * days)\rtemp_list \u0026lt;- list()\rfor(day in 1:days){\rnday = m_data[1+(day*nspatial):(nspatial*(day+1) - 1)]\rm = matrix(nday, lat, lon, byrow=TRUE)\rm[m \u0026gt; 99]=NA\rm = m[nrow(m):1,] # Invert the matrix\rr = rast(m)\rext(r) = c(67.5, 97.5, 7.5, 37.5)\rcrs(r) = \u0026quot;+init=epsg:4326\u0026quot;\rtemp_list[[day]] \u0026lt;- r\rrm(r, m)\rprint(day)\r}\rtemp_raster \u0026lt;- rast(temp_list)\rplot(temp_raster[[122]])\rwriteRaster(x = temp_raster, filename = paste0(\u0026quot;IMD_Temp_\u0026quot;, substr(f, 14, 17), \u0026quot;.tif\u0026quot;))\rTest \u0026lt;- rast(\u0026quot;./IMD_Temp_2022.tif\u0026quot;)\rExtracting the daily raster stack data with extract\rWe use terra r-package to do geoprocessing.\nterra::extract(x, y, fun=NULL, method=\u0026quot;simple\u0026quot;, fun = FUN)\rwhere:\nx SpatRaster or SpatVector of polygons\ny SpatVector (points, lines, or polygons).\nYou can use any function to aggrigate the value, such as mean, count, sum, or custom.\nConcluding Remarks\rProcessing climatic data is essential for hydrological analysis. Many R packages and software are available for this. We find terra has all the functionality and is fast to process the .GRD and ‘.nc’ files. Some important tips:\r- Always plot to check the CRS of Raster and Vector data.\r- Plot a raster file to check the extent.\r- Check the null value carefully from the metadata (-99.9, -9999, etc.)\n","permalink":"https://ankitdeshmukh.com/post/2024-10-03-working-with-netcdf/","summary":"Climatic Data in Hydrological Analysis\rClimatic data such as temperature and precipitation are essential for any hydrological analysis. Usually, several years of data are required to do any type of hydrological analysis. The climatic data is obtained from either satellite-based observation or in situ observation.\nFor hydrological analysis historical precipitation and temperature data is essential. Usually these data can be obtained from Indian Meterolgocal Department (भारतीय मौसम विज्ञान विभाग:IMD) free of cost.","title":"Working with NetCDF and Binary climatic data in R"},{"content":"\rSBI Submits all the data to ECI and ECI publish it on Thursday 21 Mar 2024, 6:32 PM Links of 2 files are provided here:\nDisclosure of Electoral Bonds\nDetails of Electoral Bonds submitted by SBI on 21st March 2024 (EB_Redemption_Details)\nDetails of Electoral Bonds submitted by SBI on 21st March 2024 (EB_Purchase_Details)\nTo know the background of the case read this: SBI submits all electoral bond details, including unique numbers, to ECI\nA short note on regexregex\rregex, short for Regular Expressions, are essentially sequences of characters that form a search pattern, which is then used to match strings or parts of strings in a text. Think of them as sophisticated search queries that allow you to find specific patterns within a larger body of text. regex can be used for simple patterns like a specific word or phrase, or you can create complex patterns to match more intricate structures such as email addresses, URLs, dates, and more.\nFor example, if you wanted to find all email addresses in a text document, you could use a regular expression pattern like [a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\\\\\.[a-zA-Z0-9-.]+. This pattern breaks down as follows:\n# Sample emails\rem1 \u0026lt;- \u0026quot;example-1_Random@gmail.com\u0026quot; # this is a valid email\rem2 \u0026lt;- \u0026quot;example*1_Random$gmail.com\u0026quot; # this is an invalid email.\r# Detect email addresses\rstringr::str_detect(c(em1, em2), \u0026quot;[a-zA-Z0-9_.+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z0-9-.]+\u0026quot;)\r## [1] TRUE FALSE\ra-zA-Z0-9_.+-]+ matches one or more alphanumeric characters, dots, underscores, percent signs, plus signs, or hyphens, representing the username portion of an email address.\n\\(\\textbf{@}\\) matches the “@” symbol, separating the username from the domain.\n[A-Za-z0-9.-]+ matches one or more alphanumeric characters, dots, or hyphens, representing the domain name.\n\\. matches a literal dot, separating the domain from the top-level domain.\n[a-zA-Z0-9-.]+ matches two or more alphabetic characters, representing the top-level domain (e.g., com, org, net).\nRegular expressions can be used in various programming languages, here we are using it with R programming with stringr package.\nRequired R libraries for this analysis\r# To process the PDF document\rif(!require(lubridate)){install.packages(\u0026quot;lubridate\u0026quot;);library(lubridate)}\rif(!require(pdftools)){install.packages(\u0026quot;pdftools\u0026quot;);library(pdftools)}\rif(!require(stringr)){install.packages(\u0026quot;stringr\u0026quot;);library(stringr)}\rif(!require(readxl)){install.packages(\u0026quot;readxl\u0026quot;);library(readxl)}\rif(!require(DT)){install.packages(\u0026quot;DT\u0026quot;);library(DT)}\rFigure 1: Structure of the files in working directory\rCleaning the Party data\rWe starting with reading the file Parties.pdf from ./Data/Raw Data/ directory. The regular expression [\\r\\n]+ is used to match one or more occurrences of either a carriage return ( or a newline character (). After that str_trip remove space either from starting or end of the each string. str_split_fixed split string with \"\\\\s{2,}\" means 2 consiqutive space till it reaches 9 element. We choose 9 because there are total 9 column in Parties.pdf.\nParty \u0026lt;- pdf_text(\u0026quot;./Data/Raw Data/Parties.pdf\u0026quot;)\rParty \u0026lt;- unlist(str_split(Party, \u0026quot;[\\\\r\\\\n]+\u0026quot;))\rParty \u0026lt;- as.data.frame(str_split_fixed(str_trim(Party), \u0026quot;\\\\s{2,}\u0026quot;, n = 9))\rhead(Party,5)\r## V1 V2 V3\r## 1 Date of Account no. of Bond\r## 2 Sr No. Name of the Political Party Prefix\r## 3 Encashment Political Party Number\r## 4 1 12/Apr/2019 ALL INDIA ANNA DRAVIDA MUNNETRA KAZHAGAM *******5199\r## 5 2 12/Apr/2019 ALL INDIA ANNA DRAVIDA MUNNETRA KAZHAGAM *******5199\r## V4 V5 V6 V7 V8 V9\r## 1 Pay Branch ## 2 Denominations Pay Teller ## 3 Code ## 4 OC 775 1,00,00,000 00800 2770121 ## 5 OC 3975 1,00,00,000 00800 2770121\rUpon inspection we find all the pages of pdf as the table header and now we are removing the headers, I remove row containing “Date of”, “Sr No.”, “Encashment” using V1 column.\nAfter that we have Page.and it’s numbers for each pages read by pdftools. We remove using str_extract, here Page{dash} represent anything after Page must be selected. We also remove rows that have all the columns emptly.\nIndex = Party$V1 %in% c(\u0026quot;Date of\u0026quot;, \u0026quot;Sr No.\u0026quot;, \u0026quot;Encashment\u0026quot;)\rParty \u0026lt;- Party[!Index,]\rIndex2 = is.na(str_extract(Party$V1, \u0026quot;Page.\u0026quot;))\rParty \u0026lt;- Party[Index2,];\rParty \u0026lt;- Party[!apply(Party == \u0026quot;\u0026quot;, 1, all),]\rrm(Index, Index2)\rNow, all the data is stored in the Party variable the structure of it.\nclass(Party)\r## [1] \u0026quot;data.frame\u0026quot;\rdim(Party)\r## [1] 20421 9\rWe found at some row total number of columns are greater then 9. So before constructing the final data frame we choose checking each row with for loop. We pre-poulated several columns such as SN, Date of Enchashment\nn = rep(NA,dim(Party)[1])\rIndex_m \u0026lt;- !nchar(Party$V9)\u0026gt;0\rCol1 = as.data.frame(str_split_fixed(Party$V2, pattern = \u0026quot; \u0026quot;, n = 2))\rParty_Data \u0026lt;- data.frame(\u0026quot;SN\u0026quot; = as.numeric(Party$V1),\r\u0026quot;Date of Encashment\u0026quot; = lubridate::dmy(Col1$V1),\r\u0026quot;Party Name\u0026quot; = n,\r\u0026quot;Account No\u0026quot; = n,\r\u0026quot;Prefix\u0026quot; = n,\r\u0026quot;Bond No\u0026quot; = n,\r\u0026quot;Denominations\u0026quot; = n,\r\u0026quot;Pay Branch Code\u0026quot; = n,\r\u0026quot;Pay Teller\u0026quot; = n)\rcol7_t \u0026lt;- as.numeric(gsub(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;, Party$V6))\rcol7_f \u0026lt;- as.numeric(gsub(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;, Party$V7))\rfor(ii in seq_along(n)){\rif (Index_m[ii]) {\rParty_Data[ii,3] \u0026lt;- Col1$V2[ii]\rParty_Data[ii,4] \u0026lt;- Party$V3[ii]\rParty_Data[ii,5] \u0026lt;- Party$V4[ii]\rParty_Data[ii,6] \u0026lt;- as.numeric(Party$V5)[ii]\rParty_Data[ii,7] \u0026lt;- col7_t[ii]\rParty_Data[ii,8] \u0026lt;- Party$V7[ii]\rParty_Data[ii,9] \u0026lt;- as.numeric(Party$V8)[ii]\r} else {\rParty_Data[ii,3] \u0026lt;- Party$V3[ii]\rParty_Data[ii,4] \u0026lt;- Party$V4[ii]\rParty_Data[ii,5] \u0026lt;- Party$V5[ii]\rParty_Data[ii,6] \u0026lt;- as.numeric(Party$V6)[ii]\rParty_Data[ii,7] \u0026lt;- col7_f[ii]\rParty_Data[ii,8] \u0026lt;- Party$V8[ii]\rParty_Data[ii,9] \u0026lt;- as.numeric(Party$V9)[ii]\r}\r# print(ii) # running this might take some time.\r# IMP: don\u0026#39;t run white building the site.\r}\rNow, we have table similar to pdf, but I want to create unique id un_id by merging Perfiex with Bond No., so that I can match Parties to Donner, after that I save the data into Party_Cleaned.xlsx file.\nun_id \u0026lt;- paste0(Party_Data$Prefix,\rstr_pad(Party_Data$Bond.No, width = 5,pad = \u0026quot;0\u0026quot;))\rParty_Data$unique_ID \u0026lt;- un_id\rwrite.xlsx(Party_Data, file=\u0026quot;./Data/Processed xlsx/Party_Cleaned.xlsx\u0026quot;, row.names = FALSE)\rNow to time to generate some table data, I am aggregation Party and Denominations data for table. To aggregate the data with Party name aggregate function from base R is used. After that DT::datatable is used for interactive table.\nParty_Data \u0026lt;- read_xlsx(path = \u0026quot;./Data/Processed xlsx/Party_Cleaned.xlsx\u0026quot;)\rfx_name \u0026lt;- function(x) {c(length(x), median(x), round(mean(x),0), sum(x))}\ragg_data = aggregate(Party_Data, Denominations ~ Party.Name, FUN = fx_name)\ragg_data_df \u0026lt;- data.frame(\u0026quot;Purchaser\u0026quot; = agg_data$Party.Name,\r\u0026quot;Count\u0026quot; = agg_data$Denominations[,1],\r\u0026quot;Median\u0026quot; = agg_data$Denominations[,2],\r\u0026quot;Mean\u0026quot; = agg_data$Denominations[,3],\r\u0026quot;Sum\u0026quot; = agg_data$Denominations[,4])\rdatatable(agg_data_df, options = list(\rinitComplete = JS(\r\u0026quot;function(settings, json) {\u0026quot;,\r\u0026quot;$(this.api().table().header()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().container()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().body()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;}\u0026quot;)))%\u0026gt;% formatCurrency(c(\u0026#39;Median\u0026#39;, \u0026#39;Mean\u0026#39;, \u0026#39;Sum\u0026#39;),\rcurrency = \u0026quot;₹\u0026quot;,\rinterval = 3,\rmark = \u0026quot;,\u0026quot;,\rdigits = 0)\rCleaning the Donner data\rSimilar to the Party data, explore the Donner data from the pdf “Company.pdf” in the folder Data/Raw Data. Similar processing methodology is use as Party.pdf with one change this time total number of columns are 12.\nComp \u0026lt;- pdf_text(\u0026quot;./Data/Raw Data/Company.pdf\u0026quot;)\rComp \u0026lt;- unlist(str_split(Comp, \u0026quot;[\\\\r\\\\n]+\u0026quot;))\rComp \u0026lt;- str_trim(gsub(\u0026quot;,\u0026quot;,\u0026quot;\u0026quot;,Comp))\rBased on the data format, I found breaking it from as digit followed by space (?\u0026lt;=\\\\d ) is more useful.\nComp = as.data.frame(str_split_fixed(Comp, \u0026quot;(?\u0026lt;=\\\\d )\u0026quot;, n = 12))\rThis will break the string the string in 2 columns, “SN”, and “Reference No URN”\na \u0026lt;- gsub(\u0026quot;\\\\s+\u0026quot;, \u0026quot; \u0026quot;, Comp$V1, perl = TRUE)\rIndex = a %in% c(a[1:3])\rComp \u0026lt;- Comp[!Index,]\rIndex2 = is.na(str_extract(Comp$V1, \u0026quot;Page.\u0026quot;))\rComp \u0026lt;- Comp[Index2,]\rComp \u0026lt;- Comp[!apply(Comp == \u0026quot;\u0026quot;, 1, all),]\rrm(Index, Index2, a)\rNow we will break down column V6 that has structure of\n“A B C INDIA LIMITED TL 11448” First we will crop Bond numbers using str_sub with 1 to -6 (6th character from the last) save in c. After that Names are extracted using c removing last to charcter that are Prefix of the Unitq bond number. At last Prefix are extraced using str_sub(c, -2, -1).\nBondN \u0026lt;- as.numeric(str_sub(str_trim(Comp$V6), -6,-1))\r# you cans simply choose last 4 digit of the V6\rc = str_trim(str_sub(str_trim(Comp$V6),1,-6))\rName \u0026lt;- str_trim(str_sub(c,1,-3))\rPrefix \u0026lt;- str_sub(c, -2, -1)\rNow we will assemble the data in the total 12 column.\nComp_Data \u0026lt;- data.frame(\u0026quot;SN\u0026quot; = as.numeric(Comp$V1),\r\u0026quot;Reference No URN\u0026quot; = str_trim(Comp$V2),\r\u0026quot;Journal Date\u0026quot; = dmy(Comp$V3),\r\u0026quot;Date of Purchase\u0026quot; = dmy(Comp$V4),\r\u0026quot;Date of Expiry\u0026quot; = dmy(Comp$V5),\r\u0026quot;Purchaser\u0026quot; = Name,\r\u0026quot;Prefix\u0026quot; = Prefix,\r\u0026quot;Bond No\u0026quot; = BondN,\r\u0026quot;Denominations\u0026quot; = as.numeric(str_trim(Comp$V7)),\r\u0026quot;Issue Branch Code\u0026quot; = str_trim(Comp$V8),\r\u0026quot;Issue Teller\u0026quot; = as.numeric(str_trim(Comp$V9)),\r\u0026quot;Status\u0026quot; = str_trim(Comp$V10))\r# write.xlsx(Comp_Data, file=\u0026quot;Comp_Cleaned.xlsx\u0026quot;, row.names = FALSE)\rWe temporarily saved the files with “Comp_Cleaned.xlsx” and found there are some error in the columns, modify them here for one Donner.\nIndex = Comp_Data$Reference.No.URN %in% c(\u0026quot;00300202310100000003344\u0026quot;,\r\u0026quot;00300202310120000003422\u0026quot;,\r\u0026quot;00300202310130000003470\u0026quot;)\rComp_Data$Purchaser[Index] \u0026lt;- \u0026quot;L7 HITECH PRIVATE LIMITED\u0026quot;\rComp_Data$Denominations[Index] \u0026lt;- 10000000\rComp_Data$Issue.Branch.Code[Index] \u0026lt;- \u0026quot;00300\u0026quot;\rComp_Data$Prefix[Index] \u0026lt;- \u0026quot;OC\u0026quot;\rComp_Data$Status[Index] \u0026lt;- \u0026quot;Paid\u0026quot;\rComp_Data$Issue.Teller[Index] \u0026lt;- 1022034 # same value is used as it\u0026#39;s not much useful information\r## Adding Bond no. for all 3 cases\rComp_Data$Bond.No[Index] \u0026lt;- c(16524, 16531, 16521, 16535, 16529, 16527, 16523, 16519, 16533, 16569, 16577, 16565, 16567,16573, 16563, 16571, 16575, 16636,16638, 16644, 16642, 16640)\rwrite.xlsx(Comp_Data, file=\u0026quot;Comp_Cleaned.xlsx\u0026quot;, row.names = FALSE)\rprint(paste0(\u0026quot;No of NAs: \u0026quot;, sum(is.na(Comp_Data))))\rUpon inspection around 16 row have incorrect information, it’s because the parser function you used and they cleaning manually.\nNOTE: NA values are cleaned manually and renamed it to “Comp_Cleaned_Man.xlsx”\nNow we will load the cleaned data for tabular representation\nComp_Data \u0026lt;- read_xlsx(path = \u0026quot;./Data/Processed xlsx/Comp_Cleaned_man.xlsx\u0026quot;, sheet = \u0026quot;Sheet1\u0026quot;)\rComp_Data$unique_ID \u0026lt;- paste0(Comp_Data$Prefix,\rstr_pad(Comp_Data$Bond.No, width = 5,pad = \u0026quot;0\u0026quot;))\rfx_name \u0026lt;- function(x) {c(length(x), median(x), round(mean(x),0), sum(x))}\ragg_data = aggregate(Comp_Data, Denominations ~ Purchaser, FUN = fx_name)\ragg_data_df \u0026lt;- data.frame(\u0026quot;Purchaser\u0026quot; = agg_data$Purchaser,\r\u0026quot;Count\u0026quot; = agg_data$Denominations[,1],\r\u0026quot;Median\u0026quot; = agg_data$Denominations[,2],\r\u0026quot;Mean\u0026quot; = agg_data$Denominations[,3],\r\u0026quot;Sum\u0026quot; = agg_data$Denominations[,4])\rdatatable(agg_data_df, options = list(\rinitComplete = JS(\r\u0026quot;function(settings, json) {\u0026quot;,\r\u0026quot;$(this.api().table().header()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().container()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().body()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;}\u0026quot;)))%\u0026gt;% formatCurrency(c(\u0026#39;Median\u0026#39;, \u0026#39;Mean\u0026#39;, \u0026#39;Sum\u0026#39;),\rcurrency = \u0026quot;₹\u0026quot;,\rinterval = 3,\rmark = \u0026quot;,\u0026quot;,\rdigits = 0)\rProcessing the pdf in an easy way:\rUsing tabulizer package that can extract pdf tables, I find it’s slow but works well for table data. Similar to PDFtools it also required rJava.\n# remotes::install_github(c(\u0026quot;ropensci/tabulizerjars\u0026quot;, \u0026quot;ropensci/tabulizer\u0026quot;))\rlibrary(tabulizer)\rout_tables \u0026lt;- extract_tables(\u0026quot;./2024-03-25-Electoral-Bond-Analysis/Data/Raw Data/Company.pdf\u0026quot;)\rOut \u0026lt;- out_tables[[1]]\rfor (ii in 2:length(out_tables)) {\rtemp \u0026lt;- out_tables[[ii]]\rtemp1 \u0026lt;- temp[2:dim(temp)[1],]\rOut \u0026lt;- rbind(Out,temp1)\r}\rCombining Party and Donner data using UN_ID\rp_uid \u0026lt;- Party_Data$unique_ID # Unique ID for Party\rc_uid \u0026lt;- Comp_Data$unique_ID # Unique ID for Donner\r## Finding how many UID don\u0026#39;t match in both the dataset\rmissing_record_p_name = Party_Data$Party.Name[!p_uid %in% c_uid]\rtable(missing_record_p_name)\r## missing_record_p_name\r## AAM AADMI PARTY ## 2 ## ADYAKSHA SAMAJVADI PARTY ## 30 ## ALL INDIA ANNA DRAVIDA MUNNETRA KAZHAGAM ## 38 ## ALL INDIA TRINAMOOL CONGRESS ## 36 ## BHARAT RASHTRA SAMITHI ## 33 ## BHARATIYA JANATA PARTY ## 1145 ## BIHAR PRADESH JANTA DAL(UNITED) ## 2 ## DRAVIDA MUNNETRA KAZHAGAM (DMK) ## 7 ## JANATA DAL ( SECULAR ) ## 25 ## JHARKHAND MUKTI MORCHA ## 10 ## NATIONALIST CONGRESS PARTY MAHARASHTRA PRADESH ## 25 ## PRESIDENT, ALL INDIA CONGRESS COMMITTEE ## 238 ## RASHTRIYA JANTA DAL ## 10 ## SHIVSENA ## 36 ## TELUGU DESAM PARTY ## 19 ## YSR CONGRESS PARTY (YUVAJANA SRAMIKA RYTHU CONGRESS PARTY) ## 24\rNow, the total mission record by comparing Unique ID from Party and Company data/Donner data: 1680\nMismatch in our analysis 130. Remaining 1550 are not provided by SBI list.\nNow computing number of bonds are given to each party.\rBased on our analysis there are total 24 parties and 1318 Donner in the list.\np_name \u0026lt;- unique(Party_Data$Party.Name)\rc_name \u0026lt;- unique(Comp_Data$Purchaser)\rComp_table \u0026lt;- matrix(NA, nrow = length(c_name), ncol = length(p_name))\rrownames(Comp_table) \u0026lt;- c_name\rcolnames(Comp_table) \u0026lt;- p_name\rfor(ii in seq_along(p_name)){\rp_Ind \u0026lt;- Party_Data$unique_ID[Party_Data$Party.Name %in% p_name[ii]]\rc_Ind \u0026lt;- which(Comp_Data$unique_ID %in% p_Ind)\rdonner \u0026lt;- table(Comp_Data$Purchaser[c_Ind])\rfor (jj in seq_along(donner)) {\rinx \u0026lt;- which(c_name %in% names(donner)[jj])\rComp_table[inx,ii] \u0026lt;- as.numeric(donner[jj])\r}\r}\rNow the table:\ndatatable(as.data.frame(Comp_table), class = \u0026#39;cell-border stripe\u0026#39;, options = list(pageLength=5, scrollX=\u0026#39;400px\u0026#39;,\rinitComplete = JS(\r\u0026quot;function(settings, json) {\u0026quot;,\r\u0026quot;$(this.api().table().header()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().container()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().body()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;}\u0026quot;)))\rAlso to see all the party data use this link🔗\nData \u0026lt;- read.csv(file = \u0026quot;Data/Agg_Data_Sorted.csv\u0026quot;)\rdatatable(Data, class = \u0026#39;cell-border stripe\u0026#39;, options = list(pageLength=5, scrollX=\u0026#39;400px\u0026#39;,\rinitComplete = JS(\r\u0026quot;function(settings, json) {\u0026quot;,\r\u0026quot;$(this.api().table().header()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().container()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;$(this.api().table().body()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#fff\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#111\u0026#39;});\u0026quot;,\r\u0026quot;}\u0026quot;)))\r","permalink":"https://ankitdeshmukh.com/post/2024-03-25-electoral-bond-analysis/","summary":"SBI Submits all the data to ECI and ECI publish it on Thursday 21 Mar 2024, 6:32 PM Links of 2 files are provided here:\nDisclosure of Electoral Bonds\nDetails of Electoral Bonds submitted by SBI on 21st March 2024 (EB_Redemption_Details)\nDetails of Electoral Bonds submitted by SBI on 21st March 2024 (EB_Purchase_Details)\nTo know the background of the case read this: SBI submits all electoral bond details, including unique numbers, to ECI","title":"Data Cleaning with R using 'PDFtools' and 'stringr'"},{"content":"\rWhat is PostGIS and why PostGIS is benificial over tredtional analysis approaches.\rPostGIS is a spatial database extension for PostgreSQL that allows users to store and manipulate geospatial data. PostGIS has several advantages over the traditional approach of geospatial analysis, such as:\nPostGIS supports a wide range of spatial data types, functions, and operators, enabling complex spatial queries and operations.\rPostGIS integrates well with other GIS tools and frameworks, such as QGIS, GeoServer, and Leaflet, allowing users to visualize and analyze their data in different ways.\rPostGIS leverages the power and scalability of PostgreSQL, which is a robust, open-source, and widely used relational database management system.\rPostGIS enables spatial data analysis on large datasets, as it can handle millions of features and perform spatial joins and aggregations efficiently.\rPostGIS facilitates data sharing and collaboration, as it allows multiple users to access and modify the same spatial data concurrently.\rPostGIS for Professionals\rThe PostGIS roadmap is crucial for professionals in the field of GIS, Water resources, and Hydrology. To grow in the field of GIS expert you must to the certain taks on regular basis:\nMaster Core PostGIS Functions:\rEnsure a solid understanding of fundamental PostGIS functions for spatial data handling.\rProficiency in spatial queries, geometric operations, and indexing techniques is essential.\rDeepen Geospatial Database Skills:\rExpand your expertise in geospatial database management, including performance tuning and optimization strategies.\rFamiliarize yourself with advanced database concepts relevant to spatial data storage.\rIntegration with GIS Software:\rExplore integration possibilities with popular GIS software like QGIS and ArcGIS to enhance your interoperability skills.\rStay informed about evolving standards and best practices in the GIS industry.\rAdvanced Spatial Analytics:\rFocus on advanced spatial analytics using PostGIS, such as spatial regression analysis, network analysis, and 3D spatial operations.\rIncorporate machine learning algorithms into spatial analysis to address contemporary challenges in water resources and hydrology.\rOpen Source Contributions:\rConsider contributing to the PostGIS project or related open-source GIS projects. This enhances your visibility in the community and deepens your understanding of the system.\rNetworking and Professional Development:\rAttend conferences, workshops, and webinars focused on geospatial technologies and PostGIS.\rEngage with professionals in your field, both online and offline, to build a strong professional network.\rCertifications and Academic Collaborations:\rPursue relevant certifications in GIS and spatial databases to validate your expertise.\rCollaborate with academic institutions on research projects to stay at the forefront of advancements in hydrology, remote sensing, and GIS.\rBy systematically progressing through these steps, you’ll be a highly skilled professional in PostGIS, well-equipped to tackle complex challenges in water resources and hydrology, and ultimately increase your attractiveness for high-paying job opportunities.\nNow we will understand how to setup PostGIS with QGIS in a Windows machine.\nQuick installation\rInstall the PostgreSQL from the link 🔗\nInstall with default setting. Default port is 5432, change to something else if you have another version of PostgreSQL installed.\rYouTube link https://www.youtube.com/watch?v=IYHx0ovvxPs\rConnect a database with QGIS\rBasic of Structured Query Language (SQL)\rSQL stands for Structured Query Language and is a domain-specific language for managing data in relational databases. SQL was originally developed by IBM in the 1970s and later standardized by ANSI and ISO. SQL allows users to query, manipulate, and control data using keywords, clauses, expressions, and statements that resemble natural language.\nLearning the basics of SQL\rUse tutorial and data from: https://www.sqltutorial.org/sql-sample-database/\nCreating a SQL Sample Database\nThe following database diagram illustrates the HR sample database\rThis database has 7 tables, row numbers are shown in the table below\n| Table | Rows |\r----------------------\r| employees | 40 |\r| dependents | 30 |\r| departments | 11 |\r| jobs | 11 |\r| locations | 07 |\r| countries | 25 |\r| regions | 04 |\rThe following script creates the HR sample Database Structure in PostgreSQL\rCreating a Database:\rCREATE DATABASE test_db\rWITH\rOWNER = postgres\rENCODING = \u0026#39;UTF8\u0026#39;\rLOCALE_PROVIDER = \u0026#39;libc\u0026#39;\rCONNECTION LIMIT = -1\rIS_TEMPLATE = False;\rList all the database and select the one\rpostgres=# \\l\rpostgres=# \\c DATABASE_NAME\rRun the following to add PostGIS extension to Postgres\rCREATE EXTENSION postgis;\rCREATE EXTENSION postgis_raster;\rImport shapefile in PostGIS\rUse DB manager form QGIS\nImport raster in a database\rraster2pgsql -s [SRID] -I -M [raster data source] -F [schema.table_name] | psql -U [username] -d [database name] -p [port] -h [host]\rExample:\nraster2pgsql -s 4326 -I -M Your_file.tif -F | psql -U postgres -d post_gis_v1 -p 5432 -h localhost\rSELECT ST_X(ST_Centroid(geom)) AS long, ST_Y(ST_Centroid(geom)) AS lat FROM \u0026quot;ST_India-Dist\u0026quot;;\rSELECT ST_Centroid(geom) AS geom, gid, st_nm FROM \u0026quot;ST_India-Dist\u0026quot;;\r","permalink":"https://ankitdeshmukh.com/post/2024-01-08-postgis-with-qgis/","summary":"What is PostGIS and why PostGIS is benificial over tredtional analysis approaches.\rPostGIS is a spatial database extension for PostgreSQL that allows users to store and manipulate geospatial data. PostGIS has several advantages over the traditional approach of geospatial analysis, such as:\nPostGIS supports a wide range of spatial data types, functions, and operators, enabling complex spatial queries and operations.\rPostGIS integrates well with other GIS tools and frameworks, such as QGIS, GeoServer, and Leaflet, allowing users to visualize and analyze their data in different ways.","title":"PostGIS with QGIS"},{"content":"\rGetting started with gdal\rGDAL (Geospatial Data Abstraction Library) is a free and open source translator library for raster and vector geospatial data formats. It also comes with a variety of useful command line utilities for data translation and processing. It is used by many GIS software i.e. QGIS, ArcGIS, and GRASS GIS and R.\nAs a core R use, working in climate change and hydrology, I need to process spatia-tempral data daily on daily basis. GDAL provide various functions for make life easy for daily GIS users, at the same time I wish, I never have to leave the R enviornment and rgdal was a good alternative but it is retired and removed from CRAN\r(See https://r-spatial.org/r/2022/04/12/evolution.html)\nYou can use GDAL in R with\nUse sf for vector data: It’s modern, efficient, and integrates seamlessly with GDAL.\rUse terra for raster data: It supports larger datasets and provides better performance than raster.\rUse system() for niche GDAL commands: When you need advanced features not directly exposed by R packages.\rNow we’ll explore the gdal command line utility to started with basic geospatial operations and later in the blog discuss how just usng terra and sf sufficient for most of regular geospatial needs.\nGDAL library capabilities\rGDAL offers both command-line and graphical user interfaces (such as QGIS), making it suitable for researchers with various preferences for interacting with geospatial data.\rGDAL provides bindings for popular programming languages like Python and R. This allows geospatial analysts to incorporate GDAL’s capabilities into their data analysis and modeling workflows, aligning with your preference for using R.\rBeyond data conversion, GDAL provides capabilities for basic image processing, such as resampling, cropping, and filtering, which are essential for preparing data for further analysis.\rGDAL algorithm provider has major categories of operation in vector and raster analysis:\nRaster analysis\rRaster conversion\rRaster extraction\rRaster miscellaneous (merge, raster calculator, etc.)\rRaster projections\rVector conversion\rVector geoprocessing\rVector miscellaneous (build virtual vector, SQL, etc.)\r","permalink":"https://ankitdeshmukh.com/post/2023-10-24-gdal-an-introduction-rmd/","summary":"Getting started with gdal\rGDAL (Geospatial Data Abstraction Library) is a free and open source translator library for raster and vector geospatial data formats. It also comes with a variety of useful command line utilities for data translation and processing. It is used by many GIS software i.e. QGIS, ArcGIS, and GRASS GIS and R.\nAs a core R use, working in climate change and hydrology, I need to process spatia-tempral data daily on daily basis.","title":"Introduction of GDAL, Terra and SF for geospatial analysis with R programming"},{"content":"\rHTML slides with Xaringan.\rXaringan is an R package for creating slideshows with remark.js through R Markdown.\n…from https://github.com/yihui/xaringan\nThe package name xaringan comes from Sharingan, a dōjutsu in Naruto with two abilities: the “Eye of Insight” and the “Eye of Hypnotism”. A presentation ninja should have these basic abilities, and I think remark.js may help you acquire these abilities, even if you are not a member of the Uchiha clan.\nInstalling Xaringan\rInstall the xaringan with CRAN or Github\ninstall.packages(\u0026#39;xaringan\u0026#39;)\r# or for latest version\rremotes::install_github(\u0026#39;yihui/xaringan\u0026#39;)\rSetting up the xaringan slides\r---\rtitle: \u0026quot;Title of my presentation\u0026quot;\rsubtitle: \u0026quot;Your subtitle\u0026quot;\rauthor: \u0026quot;**Dr. Ankit Deshmukh**\u0026quot;\rinstitute: \u0026quot;Affiliation\u0026quot;\rdate: \u0026quot;Week #: `r format(Sys.time(), \u0026#39;%d %B %Y\u0026#39;)`\u0026quot;\routput:\rxaringan::moon_reader:\rcss: [\u0026quot;css/default.css\u0026quot;, \u0026quot;css/metropolis.css\u0026quot;, \u0026quot;css/tachyons.min.css\u0026quot;]\rself_contained: false lib_dir: libs\rnature:\rhighlightStyle: solarized-light\rhighlightLines: true\rcountIncrementalSlides: false\rratio: 16:9\r---\rGlobal Setting for figures and code chunk\n```{r setup, include=FALSE}\roptions(htmltools.dir.version = FALSE)\rknitr::opts_chunk$set(\r#out.width = \u0026quot;100%\u0026quot;,\rcache = FALSE,\recho = TRUE,\rmessage = FALSE, warning = FALSE,\rfig.show = TRUE,\rhiline = TRUE,\rresults= \u0026quot;asis\u0026quot; # Useful to show bibliography as normal text.\r)\r```\rSetting up for Bibliography and Citation\r```{r setup, include=FALSE}\rlibrary(RefManageR)\rlibrary(bibtex)\rBibOptions(check.entries = FALSE, bib.style = \u0026quot;authoryear\u0026quot;, style = \u0026quot;text\u0026quot;, first.inits = FALSE)\rbib \u0026lt;- ReadBib(\u0026quot;~/adx/Bibliography.bib\u0026quot;) # A bibtex bibliography file. Use zotoro for this. ```\rUse xaringanExtra to enhance slide feature\ruse_logo for adding the logo in your slide.\ruse_progress_bar for progress bar.\ruse_extra_styles for code hover effect.\ruse_xaringan_extra(\"tile_view\") for see the preview of slides a once, it helps to jump on slides.\ruse_xaringan_extra(\"tachyons\") for an awesome miniature css with your slides. Find more here https://tachyons.io/#features\r```{r, echo=FALSE, include=TRUE}\rlibrary(xaringanExtra)\ruse_logo(image_url = \u0026quot;./css/Anix-Logo.png\u0026quot;, link_url = \u0026quot;https://www.ankitdeshmukh.com/\u0026quot;, width = \u0026quot;60px\u0026quot;, height = \u0026quot;60px\u0026quot;)\ruse_progress_bar(color = \u0026quot;#28282888\u0026quot;,location = \u0026quot;top\u0026quot;, height = \u0026quot;0.25em\u0026quot;)\ruse_extra_styles(hover_code_line = TRUE, mute_unhighlighted_code = FALSE)\ruse_xaringan_extra(c(\u0026quot;tile_view\u0026quot;, \u0026quot;tachyons\u0026quot;, \u0026quot;use_logo\u0026quot;, \u0026quot;use_progress_bar\u0026quot;))\r```\rOrganize all the image in images folder\ruse knitr of html+tachyons to add image\n```{r, include=TRUE, echo=FALSE, fig.align=\u0026#39;center\u0026#39;, out.width=\u0026quot;60%\u0026quot;}\rknitr::include_graphics(\u0026quot;images/YourImage.xyz\u0026quot;, error = FALSE)\r```\ror use html tags\r\u0026lt;img src=\u0026quot;images/YourImage.xyz\u0026quot; class=\u0026quot;w-60 br4 dib center\u0026quot;\u0026gt;\rCheckout the slides with Xaringan\rClick here to view in full screen\n","permalink":"https://ankitdeshmukh.com/post/2022-08-16-create-awesome-slides-with-xaringan/","summary":"HTML slides with Xaringan.\rXaringan is an R package for creating slideshows with remark.js through R Markdown.\n…from https://github.com/yihui/xaringan\nThe package name xaringan comes from Sharingan, a dōjutsu in Naruto with two abilities: the “Eye of Insight” and the “Eye of Hypnotism”. A presentation ninja should have these basic abilities, and I think remark.js may help you acquire these abilities, even if you are not a member of the Uchiha clan.","title":"Create awesome slides with Xaringan"},{"content":"\rThe main aim of this blog to show, how you can configure R, Python, and SQL in a single R-markdown file. Most of time we have to use data from databases and python code along with R functions, and having a setup that bring goodness of all the tool in one place comes really handy.\nSetup Python in Rstudio\rTo set up R Python And SQL in the Rstudio you have to first install miniconda. Miniconda helps to create python virtual environments and let you organize it.\rSuch as for, data cleaning you might use a data cleaning environment for web scraping you will use a web scraping environment and so on.\nCommands to manage conda environment inside RStudio:\rOnce you install miniconda You have to use R-reticulate in Rstudio, it is REPL (Read-eval-print loop) for Python in R, and helps to run Python code inside Rstudio. You can add miniconda into your windows terminal, but to use in Rstudio it is recommended to create and select environment from Rstudio itself.\nReticulate package provides all the necessary functionalities to run and manage python commands, such as\nload reticulate package with install.packages(reticulate) command in R console.\rcommand conda_ create() will create a new python environment.\ruse_condaenv(condaenv = \"ENVNAME\") to select a newly created environment or existing python environment\rto know the existing environments that you have created earlier, use conda_list() function.\ronce you select a environment now you are ready to install python packages with py_install(\"PackageName\").\rUse conda prompt\rI use conda prompt to create environment but some how those environment doesn’t work with my R-reticulate. But if you are using python only then you can configure conda with the following commands (run in system terminal):\nconda help for help.\rconda create --name your_environment to create a conda environment.\rconda env list to list all the environment in conda.\rconda activate your_environment to activate an specific conda environment.\rUse conda config to add other channels for package providers, but I tried pip and it worked just fine.\rconda config --add channels conda-forge (conda-forge is one of many channel)\rconda config --set channel_priority strict (set preference of channel)\rInstall a few packages: conda install pandas scikit-learn matplotlib\rHow to use SQL with R markdown\rUse dplyr, dbplyr, and RSQLite packages\nUse SQL block to view the query output in ln-line mode.\rA blogpot to learn R and SQL in Rstudio https://irene.rbind.io/post/using-sql-in-rstudio/\rExample of R, Python, and SQL with Rmarkdonw (*.Rmd)\rif(!require(dplyr)){install.packages(\u0026quot;dplyr\u0026quot;);library(dplyr)}\rif(!require(dbplyr)){install.packages(\u0026quot;dbplyr\u0026quot;);library(dbplyr)}\rif(!require(RSQLite)){install.packages(\u0026quot;RSQLite\u0026quot;);library(RSQLite)}\rconn \u0026lt;- src_memdb() # create a SQLite database in memory\r# Similar way you can add other database connections see `?dbplyr`\rcon_iso \u0026lt;- conn$con\rcopy_to(conn, storms, # this is a dataset built into dplyr\roverwrite = TRUE)\rtbl(conn, sql(\u0026quot;SELECT * FROM storms LIMIT 5\u0026quot;))\r## # Source: SQL [5 x 13]\r## # Database: sqlite 3.38.5 [:memory:]\r## name year month day hour lat long status category wind pressure\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 Amy 1975 6 27 0 27.5 -79 tropical de… -1 25 1013\r## 2 Amy 1975 6 27 6 28.5 -79 tropical de… -1 25 1013\r## 3 Amy 1975 6 27 12 29.5 -79 tropical de… -1 25 1013\r## 4 Amy 1975 6 27 18 30.5 -79 tropical de… -1 25 1013\r## 5 Amy 1975 6 28 0 31.5 -78.8 tropical de… -1 25 1012\r## # … with 2 more variables: tropicalstorm_force_diameter \u0026lt;int\u0026gt;,\r## # hurricane_force_diameter \u0026lt;int\u0026gt;\rNow create SLQ code chunk to directly run SQL queries from R.\n```{sql connection=con_iso}\rSELECT * FROM storms LIMIT 5;\r```\rThis will print 5 entire form database table storms\nTable 1: 5 records\rname\ryear\rmonth\rday\rhour\rlat\rlong\rstatus\rcategory\rwind\rpressure\rtropicalstorm_force_diameter\rhurricane_force_diameter\rAmy\r1975\r6\r27\r0\r27.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r27\r6\r28.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r27\r12\r29.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r27\r18\r30.5\r-79.0\rtropical depression\r-1\r25\r1013\rNA\rNA\rAmy\r1975\r6\r28\r0\r31.5\r-78.8\rtropical depression\r-1\r25\r1012\rNA\rNA\rSQL code chunk can ouput the data as r-variable i.e. storm_preview\n```{sql connection=con_iso, output.var=\u0026quot;storm_preview\u0026quot;}\rSELECT * FROM storms LIMIT 5;\r```\r```{r}\rclass(storm_preview)\rstorm_preview ```\r## [1] \u0026quot;data.frame\u0026quot;\r## name year month day hour lat long status category wind\r## 1 Amy 1975 6 27 0 27.5 -79.0 tropical depression -1 25\r## 2 Amy 1975 6 27 6 28.5 -79.0 tropical depression -1 25\r## 3 Amy 1975 6 27 12 29.5 -79.0 tropical depression -1 25\r## 4 Amy 1975 6 27 18 30.5 -79.0 tropical depression -1 25\r## 5 Amy 1975 6 28 0 31.5 -78.8 tropical depression -1 25\r## pressure tropicalstorm_force_diameter hurricane_force_diameter\r## 1 1013 NA NA\r## 2 1013 NA NA\r## 3 1013 NA NA\r## 4 1013 NA NA\r## 5 1012 NA NA\rNow, load reticulate to run python codes\n```{r}\r# Below code will check if `reticulate` is installed or not, if not then it will install and load in the R-session. if(!require(reticulate)){install.packages(\u0026quot;reticulate\u0026quot;);library(reticulate)}\r```\rA example of Python code inside R markdown.\n```{python}\rfrom matplotlib import pyplot as plt # Importing Numpy Library import numpy as np plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) mu = 50 sigma = 7 x = np.random.normal(mu, sigma, size=200) fig, ax = plt.subplots() ax.hist(x, 20) ax.set_title(\u0026#39;Historgram\u0026#39;) ax.set_xlabel(\u0026#39;bin range\u0026#39;) ax.set_ylabel(\u0026#39;frequency\u0026#39;) fig.tight_layout() plt.show() # Comment out if you are using blogdown-sites to render the site. ```\r## (array([ 2., 1., 2., 8., 9., 12., 22., 25., 31., 24., 15., 15., 13.,\r## 9., 7., 2., 1., 0., 1., 1.]), array([31.4398407 , 33.56751485, 35.695189 , 37.82286315, 39.95053731,\r## 42.07821146, 44.20588561, 46.33355976, 48.46123392, 50.58890807,\r## 52.71658222, 54.84425638, 56.97193053, 59.09960468, 61.22727883,\r## 63.35495299, 65.48262714, 67.61030129, 69.73797544, 71.8656496 ,\r## 73.99332375]), \u0026lt;BarContainer object of 20 artists\u0026gt;)\rFigure 1: A figure python output of Histogram plot.\r","permalink":"https://ankitdeshmukh.com/post/2022-07-04-r-python-sql-in-rstudio/","summary":"The main aim of this blog to show, how you can configure R, Python, and SQL in a single R-markdown file. Most of time we have to use data from databases and python code along with R functions, and having a setup that bring goodness of all the tool in one place comes really handy.\nSetup Python in Rstudio\rTo set up R Python And SQL in the Rstudio you have to first install miniconda.","title":"Setting up R, Python, and SQL in RStudio"},{"content":"\rCourse I teach for graduate student and Ph.D.\nR For Hydrology and Water Resources\rtools available for Hydrology\rTauDEM\rQGIS\rGrass GIS\rR (Geo-spatial analysis)\rLand use/ Land cover classification\rSWAT\rbasic hydrologic concept and methods\rguidelines for the stream network analysis\rComputational Hydrology\rHydrology is the study of water across the earth system. I will tell you some interesting phenomenon of hydrology in this blog post(s). The main goal of writing post is to summarize the knowledge of Water resource and Hydrology.\nMostly I will refer two books of hydrology Applied Hydrology (Chow, Maidment, and Mays 1988) and Rainfall-Runoff Modelling: The Primer (Beven 2012)\nReference\rBeven, K. J. 2012. Rainfall-Runoff Modelling: The Primer. 2nd ed. Chichester, West Sussex ; Hoboken, NJ: Wiley-Blackwell.\rChow, Ven Te, David R. Maidment, and Larry W. Mays. 1988. Applied Hydrology. McGraw-Hill Series in Water Resources and Environmental Engineering. New York: McGraw-Hill.\r","permalink":"https://ankitdeshmukh.com/post/2022-07-02-computational-hydrology/","summary":"Course I teach for graduate student and Ph.D.\nR For Hydrology and Water Resources\rtools available for Hydrology\rTauDEM\rQGIS\rGrass GIS\rR (Geo-spatial analysis)\rLand use/ Land cover classification\rSWAT\rbasic hydrologic concept and methods\rguidelines for the stream network analysis\rComputational Hydrology\rHydrology is the study of water across the earth system. I will tell you some interesting phenomenon of hydrology in this blog post(s). The main goal of writing post is to summarize the knowledge of Water resource and Hydrology.","title":"Computational Hydrology"},{"content":"\rThe following packages are required for the random forest\nif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)}\rif(!require(janitor)){install.packages(\u0026quot;janitor\u0026quot;);library(janitor)} # for rename\rif(!require(randomForest)){install.packages(\u0026quot;randomForest\u0026quot;);library(randomForest)}\rif(!require(caret)){install.packages(\u0026quot;caret\u0026quot;);library(caret)} # for `confustionMatrix`\rA Random forest is made of Random Trees\rData \u0026lt;- read_csv(file = here::here(\u0026quot;content/post/2022-06-26-random-forest\u0026quot;, \u0026quot;german_credit.csv\u0026quot;))\rExploring the dataset\rData \u0026lt;- clean_names(Data)\rData$creditability \u0026lt;- as.factor(Data$creditability)\rglimpse(Data)\r## Rows: 1,000\r## Columns: 21\r## $ creditability \u0026lt;fct\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ account_balance \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 4, 2, 1, 1, …\r## $ duration_of_credit_month \u0026lt;dbl\u0026gt; 18, 9, 12, 12, 12, 10, 8, 6, 18, 24,…\r## $ payment_status_of_previous_credit \u0026lt;dbl\u0026gt; 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, …\r## $ purpose \u0026lt;dbl\u0026gt; 2, 0, 9, 0, 0, 0, 0, 0, 3, 3, 0, 1, …\r## $ credit_amount \u0026lt;dbl\u0026gt; 1049, 2799, 841, 2122, 2171, 2241, 3…\r## $ value_savings_stocks \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 2, …\r## $ length_of_current_employment \u0026lt;dbl\u0026gt; 2, 3, 4, 3, 3, 2, 4, 2, 1, 1, 3, 4, …\r## $ instalment_per_cent \u0026lt;dbl\u0026gt; 4, 2, 2, 3, 4, 1, 1, 2, 4, 1, 2, 1, …\r## $ sex_marital_status \u0026lt;dbl\u0026gt; 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 4, …\r## $ guarantors \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ duration_in_current_address \u0026lt;dbl\u0026gt; 4, 2, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, …\r## $ most_valuable_available_asset \u0026lt;dbl\u0026gt; 2, 1, 1, 1, 2, 1, 1, 1, 3, 4, 1, 3, …\r## $ age_years \u0026lt;dbl\u0026gt; 21, 36, 23, 39, 38, 48, 39, 40, 65, …\r## $ concurrent_credits \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, …\r## $ type_of_apartment \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, …\r## $ no_of_credits_at_this_bank \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, …\r## $ occupation \u0026lt;dbl\u0026gt; 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, …\r## $ no_of_dependents \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, …\r## $ telephone \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ foreign_worker \u0026lt;dbl\u0026gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, …\rAssess the creditabiliy with the help of other variables\r# code -------------------------------------------------------------------------\rggplot(data = Data, aes(x = age_years, color = creditability, fill = creditability)) +\rgeom_histogram(binwidth = 5, position = \u0026quot;identity\u0026quot;, alpha = 0.4) +\rscale_x_continuous(breaks = scales::pretty_breaks(n = 6)) +\rscale_y_continuous(breaks = scales::pretty_breaks(n = 6)) + theme_minimal()\r# Create a training and testing data\rset.seed(7791)\rpartitioning \u0026lt;- sample(2, nrow(Data), replace = TRUE, prob = c(0.8, 0.2))\rtable(partitioning)\r## partitioning\r## 1 2 ## 797 203\rtrain \u0026lt;- Data[partitioning == 1, ]\rtable(train$creditability)\r## ## 0 1 ## 244 553\rtest \u0026lt;- Data[partitioning == 2, ]\rtable(test$creditability)\r## ## 0 1 ## 56 147\rTrain the Random forest model\r# Generate random forest with train data\rrf_model \u0026lt;- randomForest(formula = creditability ~ ., data = train)\rpredict_train \u0026lt;- predict(rf_model, train)\rconfusionMatrix(predict_train, train$creditability)\r## Confusion Matrix and Statistics\r## ## Reference\r## Prediction 0 1\r## 0 244 0\r## 1 0 553\r## ## Accuracy : 1 ## 95% CI : (0.9954, 1)\r## No Information Rate : 0.6939 ## P-Value [Acc \u0026gt; NIR] : \u0026lt; 2.2e-16 ## ## Kappa : 1 ## ## Mcnemar\u0026#39;s Test P-Value : NA ## ## Sensitivity : 1.0000 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 1.0000 ## Prevalence : 0.3061 ## Detection Rate : 0.3061 ## Detection Prevalence : 0.3061 ## Balanced Accuracy : 1.0000 ## ## \u0026#39;Positive\u0026#39; Class : 0 ## Testing the model rf_model on test data\rpredict_test \u0026lt;- predict(rf_model, test)\rconfusionMatrix(predict_test, test$creditability)\r## Confusion Matrix and Statistics\r## ## Reference\r## Prediction 0 1\r## 0 28 12\r## 1 28 135\r## ## Accuracy : 0.803 ## 95% CI : (0.7415, 0.8553)\r## No Information Rate : 0.7241 ## P-Value [Acc \u0026gt; NIR] : 0.006157 ## ## Kappa : 0.459 ## ## Mcnemar\u0026#39;s Test P-Value : 0.017706 ## ## Sensitivity : 0.5000 ## Specificity : 0.9184 ## Pos Pred Value : 0.7000 ## Neg Pred Value : 0.8282 ## Prevalence : 0.2759 ## Detection Rate : 0.1379 ## Detection Prevalence : 0.1970 ## Balanced Accuracy : 0.7092 ## ## \u0026#39;Positive\u0026#39; Class : 0 ## # Reference\r# Prediction 0 1\r# 0 29 14\r# 1 27 133\rvarImpPlot(rf_model)\rOptimize the performance of randomforest.\rplot(rf_model) # black line is out of bag error.\roob_error \u0026lt;- double(20)\rfor (mtry in 1:20) {\rrf \u0026lt;- randomForest(formula = creditability ~ ., data = train, mtry = mtry, ntree = 166)\roob_error[mtry] \u0026lt;- rf$err.rate[166]\r}\rplot(1:20, oob_error, type = \u0026quot;b\u0026quot;, xlab = \u0026quot;Number of variable considered\u0026quot;, ylab = \u0026quot;Out of bag erro [-]\u0026quot;, xaxt = \u0026quot;n\u0026quot;)\raxis(1, at = 1:20, labels = 1:20, cex = 0.8)\ropti_num_var = which.min(oob_error)\rRe running the random forest with optimum number of variables\rrf_optim \u0026lt;- randomForest(formula = creditability ~ ., data = train, mtry = opti_num_var, ntree = 166)\r# Testing the model `rf_model` on test data\rconfusionMatrix(predict(rf_optim, test), test$creditability)\r## Confusion Matrix and Statistics\r## ## Reference\r## Prediction 0 1\r## 0 33 20\r## 1 23 127\r## ## Accuracy : 0.7882 ## 95% CI : (0.7255, 0.8423)\r## No Information Rate : 0.7241 ## P-Value [Acc \u0026gt; NIR] : 0.02266 ## ## Kappa : 0.4609 ## ## Mcnemar\u0026#39;s Test P-Value : 0.76037 ## ## Sensitivity : 0.5893 ## Specificity : 0.8639 ## Pos Pred Value : 0.6226 ## Neg Pred Value : 0.8467 ## Prevalence : 0.2759 ## Detection Rate : 0.1626 ## Detection Prevalence : 0.2611 ## Balanced Accuracy : 0.7266 ## ## \u0026#39;Positive\u0026#39; Class : 0 ## Exploring useful variables\rtrain \u0026lt;- as.data.frame(train)\rvarImpPlot(rf_model)\rimportance(rf_model)\r## MeanDecreaseGini\r## account_balance 36.543019\r## duration_of_credit_month 34.072986\r## payment_status_of_previous_credit 18.303982\r## purpose 20.837156\r## credit_amount 46.208519\r## value_savings_stocks 18.564843\r## length_of_current_employment 18.296281\r## instalment_per_cent 13.122638\r## sex_marital_status 12.781397\r## guarantors 7.055415\r## duration_in_current_address 13.666627\r## most_valuable_available_asset 14.340330\r## age_years 33.451374\r## concurrent_credits 8.064289\r## type_of_apartment 8.821601\r## no_of_credits_at_this_bank 7.306201\r## occupation 10.567355\r## no_of_dependents 4.325819\r## telephone 6.359517\r## foreign_worker 1.666973\r# How variable affect the chance of getting loan.\rpartialPlot(rf_model, train, account_balance, \u0026quot;1\u0026quot;)\rpartialPlot(rf_model, train, age_years, \u0026quot;1\u0026quot;)\rThis analysis for discrete variable of creditability.\n","permalink":"https://ankitdeshmukh.com/post/2022-06-26-random-forest/","summary":"The following packages are required for the random forest\nif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)}\rif(!require(janitor)){install.packages(\u0026quot;janitor\u0026quot;);library(janitor)} # for rename\rif(!require(randomForest)){install.packages(\u0026quot;randomForest\u0026quot;);library(randomForest)}\rif(!require(caret)){install.packages(\u0026quot;caret\u0026quot;);library(caret)} # for `confustionMatrix`\rA Random forest is made of Random Trees\rData \u0026lt;- read_csv(file = here::here(\u0026quot;content/post/2022-06-26-random-forest\u0026quot;, \u0026quot;german_credit.csv\u0026quot;))\rExploring the dataset\rData \u0026lt;- clean_names(Data)\rData$creditability \u0026lt;- as.factor(Data$creditability)\rglimpse(Data)\r## Rows: 1,000\r## Columns: 21\r## $ creditability \u0026lt;fct\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ account_balance \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 4, 2, 1, 1, …\r## $ duration_of_credit_month \u0026lt;dbl\u0026gt; 18, 9, 12, 12, 12, 10, 8, 6, 18, 24,…\r## $ payment_status_of_previous_credit \u0026lt;dbl\u0026gt; 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, …\r## $ purpose \u0026lt;dbl\u0026gt; 2, 0, 9, 0, 0, 0, 0, 0, 3, 3, 0, 1, …\r## $ credit_amount \u0026lt;dbl\u0026gt; 1049, 2799, 841, 2122, 2171, 2241, 3…\r## $ value_savings_stocks \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 2, …\r## $ length_of_current_employment \u0026lt;dbl\u0026gt; 2, 3, 4, 3, 3, 2, 4, 2, 1, 1, 3, 4, …\r## $ instalment_per_cent \u0026lt;dbl\u0026gt; 4, 2, 2, 3, 4, 1, 1, 2, 4, 1, 2, 1, …\r## $ sex_marital_status \u0026lt;dbl\u0026gt; 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 4, …\r## $ guarantors \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ duration_in_current_address \u0026lt;dbl\u0026gt; 4, 2, 4, 2, 4, 3, 4, 4, 4, 4, 2, 4, …\r## $ most_valuable_available_asset \u0026lt;dbl\u0026gt; 2, 1, 1, 1, 2, 1, 1, 1, 3, 4, 1, 3, …\r## $ age_years \u0026lt;dbl\u0026gt; 21, 36, 23, 39, 38, 48, 39, 40, 65, …\r## $ concurrent_credits \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, …\r## $ type_of_apartment \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, …\r## $ no_of_credits_at_this_bank \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, …\r## $ occupation \u0026lt;dbl\u0026gt; 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, …\r## $ no_of_dependents \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, …\r## $ telephone \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\r## $ foreign_worker \u0026lt;dbl\u0026gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, …\rAssess the creditabiliy with the help of other variables\r# code -------------------------------------------------------------------------\rggplot(data = Data, aes(x = age_years, color = creditability, fill = creditability)) +\rgeom_histogram(binwidth = 5, position = \u0026quot;identity\u0026quot;, alpha = 0.","title":"Random Forest with R-Programming"},{"content":"\rHow to make a better boxplot with custom fonts, let’s explore this in this post, it can be used for the standard template for boxplot with facet and user defined fonts.\nRequired R libraries:\rif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)} # for ggplot2 function\rif(!require(gapminder)){install.packages(\u0026quot;gapminder\u0026quot;);library(gapminder)} # for sample data\rif(!require(showtext)){install.packages(\u0026quot;showtext\u0026quot;);library(showtext)} # to import fonts\rAdd fonts in R session\rfont_add_google(\u0026quot;Karla\u0026quot;, \u0026quot;Karla\u0026quot;) # adding local font\rfont_add(family = \u0026quot;Helvetica\u0026quot;, regular = \u0026quot;C:/Windows/Fonts/Helvetica 400.ttf\u0026quot;)\r# Adding from Google fonts\rfont_add_google(\u0026quot;Roboto Slab\u0026quot;, \u0026quot;Roboto Slab\u0026quot;) # adding font from the web/google font\rfont_families()\r## [1] \u0026quot;sans\u0026quot; \u0026quot;serif\u0026quot; \u0026quot;mono\u0026quot; \u0026quot;wqy-microhei\u0026quot; \u0026quot;Karla\u0026quot; ## [6] \u0026quot;Helvetica\u0026quot; \u0026quot;Roboto Slab\u0026quot;\rDefine theme for Boxplot and fonts\rthemeBox \u0026lt;- function(base_family = \u0026quot;sans\u0026quot;, exFont, ...){\rtheme_bw(base_family = base_family, ...) +\rtheme(\rpanel.grid = element_blank(),\rplot.title = element_text(size = 8),\raxis.ticks.length = unit(-0.05, \u0026quot;in\u0026quot;),\raxis.text.y = element_text(margin=unit(c(0.3,0.3,0.3,0.3), \u0026quot;cm\u0026quot;)),\raxis.text.x = element_text(margin=unit(c(0.3,0.3,0.3,0.3), \u0026quot;cm\u0026quot;)),\raxis.ticks.x = element_blank(),\raspect.ratio = 1,\rlegend.background = element_rect(color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;),\rtext = element_text(family=exFont)\r)\r}\rPlot the boxplots of Average Life Expectancy\rggplot(gapminder, aes(x = continent, y = lifeExp, fill = continent)) +\rfacet_wrap(~year) +\rgeom_boxplot(linetype = \u0026quot;dashed\u0026quot;) +\rstat_boxplot(aes(ymin = ..lower.., ymax = ..upper..), outlier.shape = 1) +\rstat_boxplot(geom = \u0026quot;errorbar\u0026quot;, aes(ymin = ..ymax..)) +\rstat_boxplot(geom = \u0026quot;errorbar\u0026quot;, aes(ymax = ..ymin..)) +\rscale_y_continuous(name = \u0026quot;Average Life Expectancy\u0026quot;) +\rscale_x_discrete(labels = abbreviate, name = \u0026quot;Continent\u0026quot;) + themeBox(exFont = \u0026quot;Karla\u0026quot;)\rLinks:\rBoxplot customization\r","permalink":"https://ankitdeshmukh.com/post/2022-01-21-pretty-boxplot-with-focet/","summary":"How to make a better boxplot with custom fonts, let’s explore this in this post, it can be used for the standard template for boxplot with facet and user defined fonts.\nRequired R libraries:\rif(!require(tidyverse)){install.packages(\u0026quot;tidyverse\u0026quot;);library(tidyverse)} # for ggplot2 function\rif(!require(gapminder)){install.packages(\u0026quot;gapminder\u0026quot;);library(gapminder)} # for sample data\rif(!require(showtext)){install.packages(\u0026quot;showtext\u0026quot;);library(showtext)} # to import fonts\rAdd fonts in R session\rfont_add_google(\u0026quot;Karla\u0026quot;, \u0026quot;Karla\u0026quot;) # adding local font\rfont_add(family = \u0026quot;Helvetica\u0026quot;, regular = \u0026quot;C:/Windows/Fonts/Helvetica 400.ttf\u0026quot;)\r# Adding from Google fonts\rfont_add_google(\u0026quot;Roboto Slab\u0026quot;, \u0026quot;Roboto Slab\u0026quot;) # adding font from the web/google font\rfont_families()\r## [1] \u0026quot;sans\u0026quot; \u0026quot;serif\u0026quot; \u0026quot;mono\u0026quot; \u0026quot;wqy-microhei\u0026quot; \u0026quot;Karla\u0026quot; ## [6] \u0026quot;Helvetica\u0026quot; \u0026quot;Roboto Slab\u0026quot;\rDefine theme for Boxplot and fonts\rthemeBox \u0026lt;- function(base_family = \u0026quot;sans\u0026quot;, exFont, .","title":"How to create a pretty facet-boxplot with custom fonts"},{"content":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nA few useful syntax are shown below in the post:\n1. You can embed an R code chunk like this:\rsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r2. Including Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\rFigure 1: A fancy pie chart.\rReferencing a image/figure.\r# Define the tag after lagnuage notation such as `pie`\r{r pie, fig.cap=\u0026#39;A fancy pie chart.\u0026#39;, tidy=FALSE}\rUse \\@ref(fig:pie) to refrence an image. 3. Inline R Code\rThere were `r nrow(cars)` cars studied\rThere were 50 cars studied.\n4. Use links in Rmarkdown\rUse a plain http address or add a link to a phrase:\nhttp://example.com\rlinked phrase\ryou can also use use a snippet predefined for Rmd file link (Shfit + Tab) to create a link.\n5. Use local images or image URLs.\r![Caption](http://example.com/logo.png)\r![optional caption text](figures/img.png)\r6. Tables in R markdown\rInsert table use knitr::kable() function. Applicable for any 2D rectangular data(Data Frame, Matrix, etc.).\rknitr::kable(head(iris[, 1:3]), \u0026quot;pipe\u0026quot;)\rSepal.Length\rSepal.Width\rPetal.Length\r5.1\r3.5\r1.4\r4.9\r3.0\r1.4\r4.7\r3.2\r1.3\r4.6\r3.1\r1.5\r5.0\r3.6\r1.4\r5.4\r3.9\r1.7\rOr create table with traditional markdown way.\rmpg\rcyl\rdisp\rhp\rMazda RX4\r21.0\r6\r160\r110\rMazda RX4 Wag\r21.0\r6\r160\r110\rDatsun 710\r22.8\r4\r108\r93\rHornet 4 Drive\r21.4\r6\r258\r110\rHornet Sportabout\r18.7\r8\r360\r175\rValiant\r18.1\r6\r225\r105\r7. LaTeX Equations\rInline equation:\r$equation$\r\\(\\int\\limits_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx = \\sqrt{\\pi}\\)\nDisplay equation:\r$$equation$$\r\\[\\int\\limits_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx = \\sqrt{\\pi}\\]\n8. Sub/Super scripts and others\rsuperscript^2^ subscript~2~ ~~strikethrough~~\r\u0026lt;mark\u0026gt;This is a highlighted text\u0026lt;/mark\u0026gt;\rsuperscript2\nsubscript2\nstrikethrough\nThis is a highlighted text\n9. Use Bibliography\rCitation in sentence @R-base or after the sentence [@casella2002statistical] and one more [-@king1974nonoperative]\rCitation in sentence Brutsaert (2005) or after the sentence (Brath and Jonker 2015) and one more (2022)\nThis will automatically add the bibliography at the end of the documents. For more information see this\n10. Links and footnotes\rA sample phrase^[This is a footnote; bottom of the page]\rA sample phrase1\nReference\rBrath, Richard, and David Jonker. 2015. Graph Analysis and Visualization: Discovering Business Opportunity in Linked Data. Indianapolis, Ind: Wiley.\rBrutsaert, Wilfried. 2005. Hydrology: An Introduction. Cambridge ; New York: Cambridge University Press.\rChen, Chen, Jiange Jiang, Zhan Liao, Yang Zhou, Hao Wang, and Qingqi Pei. 2022. “A Short-Term Flood Prediction Based on Spatial Deep Learning Network: A Case Study for Xi County, China.” Journal of Hydrology 607 (April): 127535. https://doi.org/10.1016/j.jhydrol.2022.127535.\rThis is a footnote text; see in the bottom of the page.↩︎\n","permalink":"https://ankitdeshmukh.com/post/2021-12-01-r-markdown/","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nA few useful syntax are shown below in the post:\n1. You can embed an R code chunk like this:\rsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.","title":"Hello R Markdown!"},{"content":"\rR is a very versatile statistical tool and programming language. It’s my all-time good-to-go data analysis tool. It is fast, reliable and nifty. It provides great flexibility for my daily work and analysis tasks. If one uses R, they could consider RStudio as a more sophisticated GUI than the Base R once.\n1. Installing R\rStep 01: Install R-Binaries from R-Project\nStep 02: Install RStudio IDE from RStudio\n2. Customizing RStudio\rA lot of ways you can make RStudio more useful for your personal use. Like updating your ‘.RProfile’, adding an awesome theme, and fonts that support ligatures.\nUsing Rstudio as R-IDE: R studio has multiple windows but the most important are Code editor, console, Environment variable pane, and plot output pane.\rFigure 1: R-Studio IDE have many pans.\rCode editor: You will write your code in this window. R used # as the comment character. To assign a variable to a value we use ← (lowercase followed by a dash).\r2.1 Customize R startup with “.Rprofile”\rIf you are using a Windows machine you can find the location of your ‘.Rprofile’ at ‘C:/Users/UserName/Documents’. The code block is shown below. This is what my ‘.Rprofile’ looks like. Several costume functions can be added here and they will load with R startup every time.\ncat(\u0026quot;\\014\u0026quot;)\rcat(\u0026quot;Hi Ankit! What are we doing today?\\n\u0026quot;)\rFigure 2: A RStudio window\r2.2 Adding a theme\rI personally like the dark theme for my R studio. I particularly like to use the ‘Gruvbox’ theme that is not available in RStudio but can be downloaded from here. Download the file and paste it to 'C:\\Users\\UserName\\AppData\\Roaming\\RStudio\\themes'. To apply the theme in R studio go to Tools \u0026gt; Global Option \u0026gt; Appearance and select editor theme as gruvbox.\nFigure 3: RSudio configuration windows\r2.3 Adding fonts that support ligature\rThis helps to read and understand code faster and efficiently, mostly the merged common occurring 2 characters to one for easy reading but this is just a font rendering feature it means the underlying code remains ASCII-compatible [Source].\nFigure 4: Font Ligature makes code more asthetic pleasing and readable\rFira Code is a free monospaced font containing ligatures for common programming multi-character combinations.\nFigure 5: Fira Font with different themes\rDownload and FiraCode font from Here, and Install on your machine for all users. To apply the theme in R studio go to Tools \u0026gt; Global Option \u0026gt; Appearance and select editor font as Fira Code.\n3. Customize R with code snippets\rThe snippet is a re-usable piece of code or text. Ordinarily, these are formally defined operative units to incorporate into larger programming modules. To repeat a few operations and formats you can use snippets in R. To edit or add snippets in RStudio go to Tools \u0026gt; Global Option \u0026gt; Code \u0026gt; Editing, now enable Snippets and click edit snippets.\nFigure 6: R Snippets for quick code chunks.\rFrom the edit snippets window you can manage snippets of R. Mostly I use R and R markdown snippets in my daily use. Few useful snippets are:\nsnippet cls\rgraphics.off(); rm(list = ls()); cat(\u0026quot;\\014\u0026quot;)\rsnippet rqr\rif(!require(${1:packageName})){install.packages(\u0026quot;${1:packageName}\u0026quot;);\rlibrary(${1:packageName})}\rsnippet fmt\r# Title :: ${1:File Title}------------------------------------------------\r# Author :: Ankit Deshmukh\r# DOC :: `r eval(Sys.Date())`\r# DOLE :: `r eval(Sys.time())`\r# Description :: ${2:File Description}\r# setup ------------------------------------------------------------------------\rgraphics.off(); cat(\u0026quot;\\014\u0026quot;)\rsetwd(\u0026quot;`r eval(getwd())`\u0026quot;)\r# libraries --------------------------------------------------------------------\rlibrary(tidyverse)\r# code -------------------------------------------------------------------------\rsnippet pp\r\u0026quot;`r gsub(\u0026quot;\\\\\\\\\u0026quot;, \u0026quot;/\u0026quot;, readClipboard())`\u0026quot;\rsnippet clear\rrm(list = ls()); graphics.off();cat(\u0026quot;\\014\u0026quot;)\rTo use the snippet use the keywords such as clear and press Shift + Tab to auto complete the snippet text.\n","permalink":"https://ankitdeshmukh.com/post/2021-09-20-getting-started-with-r/","summary":"R is a very versatile statistical tool and programming language. It’s my all-time good-to-go data analysis tool. It is fast, reliable and nifty. It provides great flexibility for my daily work and analysis tasks. If one uses R, they could consider RStudio as a more sophisticated GUI than the Base R once.\n1. Installing R\rStep 01: Install R-Binaries from R-Project\nStep 02: Install RStudio IDE from RStudio\n2. Customizing RStudio\rA lot of ways you can make RStudio more useful for your personal use.","title":"Getting Started with R Programming"}]